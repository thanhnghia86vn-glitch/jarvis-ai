
import sys
import os
import json
import ast
import asyncio
import operator
import re
import time
from datetime import datetime
import shutil
from typing import TypedDict, Annotated, Sequence, Literal, List, Dict, Set, Optional, Any
from termcolor import colored
from dotenv import load_dotenv
# --- SAFE IMPORTS (CH·ªêNG S·∫¨P N·∫æU THI·∫æU TH∆Ø VI·ªÜN) ---

# Import LangChain & AI Models
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_core.tools import tool
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_anthropic import ChatAnthropic
import sqlite3
from langchain_community.utilities.dalle_image_generator import DallEAPIWrapper
from langchain_chroma import Chroma
from langgraph.graph import StateGraph, END
from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
load_dotenv()
try:
    if os.name == 'posix': 
        __import__('pysqlite3')
        sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
        print("‚úÖ [SQLITE FIX] ƒê√£ k√≠ch ho·∫°t pysqlite3 cho m√¥i tr∆∞·ªùng Cloud.")
except ImportError: pass
try:
    import speech_recognition as sr
    import pyaudio
    from gtts import gTTS
    import pygame
    AUDIO_AVAILABLE = True
except ImportError:
    AUDIO_AVAILABLE = False
    print("‚ö†Ô∏è Cloud Mode: Audio modules disabled.")

try:
    from pdf2image import convert_from_path
    import pytesseract
    import cv2
    import numpy as np
    from PIL import Image
    OCR_AVAILABLE = True
except ImportError:
    OCR_AVAILABLE = False
    print("‚ö†Ô∏è Cloud Mode: OCR modules disabled (Running logic only).")
# --------------------------------------------
def auto_backup_brain():
    """
    T·ª± ƒë·ªông n√©n v√† sao l∆∞u b·ªô n√£o AI Corporation.
    """
    backup_folder = "./backups"
    source_db = "/tmp/db_knowledge" # ƒê∆∞·ªùng d·∫´n DB c·ªßa b·∫°n
    dataset_file = "corporate_brain_dataset.jsonl"
    
    if not os.path.exists(backup_folder):
        os.makedirs(backup_folder)
        
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_filename = f"AI_Corp_Brain_{timestamp}.zip"
    backup_path = os.path.join(backup_folder, backup_filename)

    try:
        # 1. N√©n th∆∞ m·ª•c Vector DB v√† file Dataset
        # L∆∞u √Ω: B·∫°n c·∫ßn ƒë√≥ng k·∫øt n·ªëi Vector DB tr∆∞·ªõc khi n√©n ƒë·ªÉ tr√°nh l·ªói busy
        shutil.make_archive(backup_path.replace(".zip", ""), 'zip', root_dir=".", base_dir=source_db)
        
        # 2. Copy th√™m file dataset v√†o backup (n·∫øu c·∫ßn)
        # (Th∆∞·ªùng th√¨ n√©n c·∫£ folder g·ªëc l√† an to√†n nh·∫•t)
        
        print(colored(f"üíæ [BACKUP SUCCESS] ƒê√£ l∆∞u tr·ªØ b·∫£n sao t·∫°i: {backup_path}", "green"))
        
        # 3. G·ª£i √Ω: N·∫øu b·∫°n c√≥ folder Dropbox/OneDrive, h√£y copy file zip n√†y v√†o ƒë√≥
        # cloud_sync_folder = "C:/Users/Admin/OneDrive/AI_Backup"
        # shutil.copy(backup_path, cloud_sync_folder)
        
    except Exception as e:
        print(colored(f"‚ö†Ô∏è L·ªói Backup: {e}", "red"))

# ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c b·ªô n√£o
DB_PATH = "./db_knowledge"

if not os.path.exists(DB_PATH):
    os.makedirs(DB_PATH)
    print(f"‚úÖ ƒê√£ t·∫°o th∆∞ m·ª•c t·∫°m: {DB_PATH}")
embeddings = OpenAIEmbeddings()
vector_db = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)

# 1. CODER_PRIMARY (C·∫•p 1 - DeepSeek V3)
# ƒê√¢y l√† "Ti·ªÅn ƒë·∫°o" ch·ªß l·ª±c
try:
    LLM_DEEPSEEK = ChatOpenAI(
        model="deepseek-chat", 
        api_key=os.environ.get("DEEPSEEK_API_KEY"), 
        base_url="https://api.deepseek.com",
        temperature=0,
        request_timeout=30 # Timeout nhanh ƒë·ªÉ fallback n·∫øu lag
    )
    print("‚úÖ LLM_DEEPSEEK (DeepSeek): Ready: Coder & Supervisor (Economy Mode).")
except: LLM_DEEPSEEK = None

# 2. LLM_GPT4 (C·∫•p 2 - D·ª± ph√≤ng 1 & X·ª≠ l√Ω chung)
try:
    LLM_GPT4 = ChatOpenAI(
        model="gpt-4-turbo",
        api_key=os.environ.get("OPENAI_API_KEY"),
        max_retries=2,
        temperature=0
    )
    LLM_MAIN = LLM_GPT4 # Alias cho code c≈©
    print("‚úÖ LLM_GPT4 (OpenAI): Ready.")
except: LLM_GPT4 = None

# 3. LLM_CLAUDE (C·∫•p 3 - Ch·ªët ch·∫∑n cu·ªëi c√πng)
try:
    LLM_CLAUDE = ChatAnthropic(
        model="claude-sonnet-4-5", 
        api_key=os.environ.get("ANTHROPIC_API_KEY"),
        temperature=0
    )
    print("‚úÖ LLM_CLAUDE (Anthropic): Ready.")
except: LLM_CLAUDE = None

# 4. LLM_GEMINI (Supervisor - T·ªïng qu·∫£n)
try:
    # A. B·∫£n Logic (X·ª≠ l√Ω vƒÉn b·∫£n d√†i cho Th∆∞ k√Ω)
    LLM_GEMINI_LOGIC = ChatGoogleGenerativeAI(
        model="gemini-3-pro-preview", 
        google_api_key=os.environ.get("GOOGLE_API_KEY"),
        temperature=0.3
    )
    
    # B. B·∫£n Vision (Nano Banana - Chuy√™n x·ª≠ l√Ω ·∫£nh cho Artist)
    LLM_GEMINI_VISION = ChatGoogleGenerativeAI(
        model="gemini-3-pro-image-preview", 
        google_api_key=os.environ.get("GOOGLE_API_KEY"),
        temperature=0.4
    )
    print("‚úÖ [GEMINI 3 PRO] Ready: Logic & Vision (Nano Banana).")
except: 
    LLM_GEMINI_LOGIC = None
    LLM_GEMINI_VISION = None

# 5. C√ÅC C√îNG C·ª§ KH√ÅC (Gi·ªØ nguy√™n)
try:
    LLM_PERPLEXITY = ChatOpenAI(
        model="sonar-pro",
        temperature=0,
        api_key=os.getenv("PERPLEXITY_API_KEY"),
        base_url="https://api.perplexity.ai"
    )
    print("‚úÖ [PERPLEXITY] Ready: Live Search.")
except: LLM_PERPLEXITY = None
# Artist
# =========================================================
# 3. PH√ÇN B·ªî QUY·ªÄN L·ª∞C (ROLE MAPPING)
# =========================================================
# ƒê√¢y l√† n∆°i quy·∫øt ƒë·ªãnh ai l√†m vi·ªác g√¨.

# Coder & Architect -> DeepSeek (Ti·∫øt ki·ªám 95% chi ph√≠)
CODER_PRIMARY = LLM_DEEPSEEK if LLM_DEEPSEEK else LLM_GPT4
ARCHITECT_PRIMARY = LLM_DEEPSEEK if LLM_DEEPSEEK else LLM_GPT4

# Supervisor (ƒêi·ªÅu ph·ªëi) -> DeepSeek (R·∫•t quan tr·ªçng ƒë·ªÉ gi·∫£m bill)
SUPERVISOR_PRIMARY = LLM_DEEPSEEK if LLM_DEEPSEEK else LLM_GPT4 

# Artist Brain -> D√πng Gemini Vision (Nano Banana) ƒë·ªÉ hi·ªÉu ·∫£nh
ARTIST_BRAIN = LLM_GEMINI_VISION if LLM_GEMINI_VISION else LLM_GPT4

# Admin/Secretary -> D√πng Gemini Logic (Context l·ªõn)
ADMIN_PRIMARY = LLM_GEMINI_LOGIC if LLM_GEMINI_LOGIC else LLM_GPT4

# Creative -> Claude
CREATIVE_PRIMARY = LLM_CLAUDE if LLM_CLAUDE else LLM_GPT4

# Logic/Finance/Legal -> GPT-4o (C·∫ßn ƒë·ªô ch√≠nh x√°c cao nh·∫•t)
LOGIC_PRIMARY = LLM_GPT4

# Researcher -> Perplexity
RESEARCHER_PRIMARY = LLM_PERPLEXITY if LLM_PERPLEXITY else LLM_GEMINI_LOGIC



CODER_BACKUP = LLM_CLAUDE

# ============================================================================
# --- 1. ƒê·ªäNH NGHƒ®A STATE (TR·∫†NG TH√ÅI H·ªÜ TH·ªêNG) ---
# ============================================================================
# Vi·ªác n√†y gi√∫p Python b√°o l·ªói ngay n·∫øu b·∫°n g√µ nh·∫ßm "Codder" thay v√¨ "Coder"
AgentName = Literal["Coder" , "Orchestrator", "Hardware", "Engineering", "IoT_Engineer", "Supervisor", "Procurement", "Investment", "Researcher", "Strategy_R_and_D", "Legal", "Marketing", "Artist","Tester", "Secretary","Storyteller", "FINISH"]

class AgentState(TypedDict):
    # D√πng Sequence[BaseMessage] l√† chu·∫©n nh·∫•t
    messages: Annotated[Sequence[BaseMessage], operator.add]
    # ƒê·ªïi AgentName th√†nh str ƒë·ªÉ tr√°nh l·ªói nghi√™m ng·∫∑t c·ªßa Literal khi ch·∫°y Runtime
    next_step: str 
    current_agent: str
    error_log: Annotated[list, operator.add] # Th√™m Annotated ƒë·ªÉ AI c√≥ th·ªÉ c·ªông d·ªìn l·ªãch s·ª≠ l·ªói
    task_type: str



@tool
def hardware_controller(command: str):
    """G·ª≠i l·ªánh xu·ªëng ph·∫ßn c·ª©ng (IoT/Robot). V√≠ d·ª•: 'BAT_DEN', 'GAP_VAT_THE'."""
    # Gi·∫£ l·∫≠p k·∫øt n·ªëi IoT
    return f"[IOT SYSTEM] ƒê√£ th·ª±c thi l·ªánh ph·∫ßn c·ª©ng: {command}. Tr·∫°ng th√°i: ·ªîn ƒë·ªãnh."

@tool
def market_analyzer(query: str):
    """Ph√¢n t√≠ch d·ªØ li·ªáu th·ªã tr∆∞·ªùng t√†i ch√≠nh."""
    return f"[FINANCE] D·ªØ li·ªáu cho '{query}': Xu h∆∞·ªõng TƒÉng. Khuy·∫øn ngh·ªã: Mua v√†o."

@tool
def image_generator(prompt: str):
    """T·∫°o ·∫£nh minh h·ªça t·ª´ vƒÉn b·∫£n b·∫±ng DALL-E 3."""
    try:
        # G·ªçi API OpenAI DALL-E 3
        generator = DallEAPIWrapper(model="dall-e-3", quality="hd")
        image_url = generator.run(prompt)
        # Tr·∫£ v·ªÅ URL ·∫£nh ƒë·ªÉ hi·ªÉn th·ªã
        return f"IMAGE_GENERATED: {image_url}"
    except Exception as e:
        return f"L·ªói t·∫°o ·∫£nh: {e}"

def trim_messages(messages, max_tokens=10):
    """
    Gi·ªØ cho b·ªô nh·ªõ lu√¥n g·ªçn g√†ng, ch·ªâ gi·ªØ l·∫°i c√°c tin nh·∫Øn quan tr·ªçng nh·∫•t.
    """
    if len(messages) > max_tokens:
        # Gi·ªØ l·∫°i System Message ƒë·∫ßu ti√™n v√† N tin nh·∫Øn cu·ªëi c√πng
        return [messages[0]] + messages[-(max_tokens-1):]
    return messages

STRATEGY_SYSTEM_PROMPT = """
B·∫°n l√† Gi√°m ƒë·ªëc Chi·∫øn l∆∞·ª£c (CSO) v√† Chuy√™n gia Ph√¢n t√≠ch Th·ªã tr∆∞·ªùng cao c·∫•p. 
Khi nh·∫≠n ƒë∆∞·ª£c y√™u c·∫ßu nghi√™n c·ª©u, b·∫°n ph·∫£i th·ª±c hi·ªán theo quy tr√¨nh sau:

1. PH√ÇN T√çCH HI·ªÜN TR·∫†NG: ƒê√°nh gi√° quy m√¥ th·ªã tr∆∞·ªùng, xu h∆∞·ªõng c√¥ng ngh·ªá hi·ªán t·∫°i.
2. NH·∫¨N ƒê·ªäNH ƒê·ªêI TH·ª¶: Ch·ªâ ra c√°c ƒëi·ªÉm y·∫øu c·ªßa c√°c s·∫£n ph·∫©m hi·ªán c√≥ tr√™n th·ªã tr∆∞·ªùng.
3. CHI·ªÄU S√ÇU CHI·∫æN L∆Ø·ª¢C: S·ª≠ d·ª•ng m√¥ h√¨nh PESTLE (Ch√≠nh tr·ªã, Kinh t·∫ø, X√£ h·ªôi, C√¥ng ngh·ªá, Lu·∫≠t ph√°p, M√¥i tr∆∞·ªùng) ƒë·ªÉ ƒë√°nh gi√° t√°c ƒë·ªông.
4. ƒê·ªäNH H∆Ø·ªöNG T∆Ø∆†NG LAI: D·ª± b√°o xu h∆∞·ªõng trong 2-5 nƒÉm t·ªõi v√† l·ªô tr√¨nh ph√°t tri·ªÉn (Roadmap) ƒë·ªÉ d·∫´n ƒë·∫ßu.

Y√™u c·∫ßu: N·ªôi dung ph·∫£i mang t√≠nh ph·∫£n bi·ªán, c√≥ chi·ªÅu s√¢u nghi√™n c·ª©u, kh√¥ng n√≥i s√°o r·ªóng.
"""

CONTEXT_PROMPTS = {
    # 1. NH√ìM QU·∫¢N TR·ªä & ƒêI·ªÄU PH·ªêI
    "CHAT": "B·∫°n l√† tr·ª£ l√Ω J.A.R.V.I.S th√¢n thi·ªán, lu√¥n tr·∫£ l·ªùi ng·∫Øn g·ªçn, s√∫c t√≠ch v√† ƒëi th·∫≥ng v√†o v·∫•n ƒë·ªÅ.",
    "SECRETARY": "B·∫°n l√† Th∆∞ k√Ω ƒëi·ªÅu h√†nh chuy√™n nghi·ªáp. Nhi·ªám v·ª•: T√≥m t·∫Øt th√¥ng tin ph·ª©c t·∫°p th√†nh b√°o c√°o d·ªÖ hi·ªÉu, vƒÉn phong l·ªãch s·ª±, trang tr·ªçng.",
    "ORCHESTRATOR": "B·∫°n l√† T·ªïng tham m∆∞u tr∆∞·ªüng. Nhi·ªám v·ª•: Ph√¢n t√≠ch quy tr√¨nh, chia nh·ªè t√°c v·ª• v√† ƒëi·ªÅu ph·ªëi ngu·ªìn l·ª±c.",
    "PUBLISHER": "B·∫°n l√† T·ªïng bi√™n t·∫≠p. Nhi·ªám v·ª•: T·ªïng h·ª£p d·ªØ li·ªáu r·ªùi r·∫°c th√†nh vƒÉn b·∫£n ho√†n ch·ªânh, ƒë·ªãnh d·∫°ng Markdown ƒë·∫πp m·∫Øt.",

    # 2. NH√ìM K·ª∏ THU·∫¨T & PH·∫¶N C·ª®NG
    "CODER": "B·∫°n l√† Senior Full-stack Developer. Nguy√™n t·∫Øc: Code s·∫°ch (Clean Code), t·ªëi ∆∞u hi·ªáu su·∫•t, lu√¥n c√≥ comment gi·∫£i th√≠ch v√† tu√¢n th·ªß SOLID.",
    "TESTER": "B·∫°n l√† Chuy√™n gia QA/QC v√† B·∫£o m·∫≠t. Nhi·ªám v·ª•: T√¨m l·ªói (bug), l·ªó h·ªïng b·∫£o m·∫≠t v√† ki·ªÉm tra t√≠nh logic c·ªßa m√£ ngu·ªìn.",
    "ARCHITECT": "B·∫°n l√† Ki·∫øn tr√∫c s∆∞ h·ªá th·ªëng (Software Architect). Nhi·ªám v·ª•: Thi·∫øt k·∫ø c·∫•u tr√∫c database, s∆° ƒë·ªì lu·ªìng d·ªØ li·ªáu v√† ki·∫øn tr√∫c Microservices.",
    "HARDWARE": "B·∫°n l√† K·ªπ s∆∞ ph·∫ßn c·ª©ng v√† H·ªá th·ªëng nh√∫ng. Chuy√™n gia v·ªÅ m·∫°ch ƒëi·ªán, ESP32, Arduino v√† s∆° ƒë·ªì ch√¢n (Pinout).",
    "IOT": "B·∫°n l√† K·ªπ s∆∞ IoT. Chuy√™n gia v·ªÅ giao th·ª©c MQTT, k·∫øt n·ªëi kh√¥ng d√¢y v√† ƒëi·ªÅu khi·ªÉn thi·∫øt b·ªã t·ª´ xa.",
    "ENGINEERING": "B·∫°n l√† K·ªπ s∆∞ thi·∫øt k·∫ø m√¥ ph·ªèng. Chuy√™n gia s·ª≠ d·ª•ng Python Plotly ƒë·ªÉ v·∫Ω c√°c m√¥ h√¨nh 3D v√† bi·ªÉu ƒë·ªì k·ªπ thu·∫≠t.",

    # 3. NH√ìM NGHI·ªÜP V·ª§ & S√ÅNG T·∫†O
    "RESEARCH": "B·∫°n l√† Chuy√™n gia ph√¢n t√≠ch th·ªã tr∆∞·ªùng 2026. Nhi·ªám v·ª•: Cung c·∫•p s·ªë li·ªáu th·ª±c t·∫ø, xu h∆∞·ªõng m·ªõi nh·∫•t v√† tr√≠ch d·∫´n ngu·ªìn uy t√≠n.",
    "INVEST": "B·∫°n l√† Gi√°m ƒë·ªëc T√†i ch√≠nh (CFO) s·∫Øc s·∫£o. T·∫≠p trung v√†o: L·ª£i nhu·∫≠n (ROI), chi ph√≠ (Cost), d√≤ng ti·ªÅn v√† r·ªßi ro t√†i ch√≠nh.",
    "LEGAL": "B·∫°n l√† Gi√°m ƒë·ªëc Ph√°p ch·∫ø (CLO). Nhi·ªám v·ª•: R√† so√°t r·ªßi ro ph√°p l√Ω, b·∫£n quy·ªÅn (IP), tu√¢n th·ªß lu·∫≠t An ninh m·∫°ng v√† GDPR.",
    "MARKETING": "B·∫°n l√† Gi√°m ƒë·ªëc Marketing (CMO). Nhi·ªám v·ª•: S√°ng t·∫°o chi·∫øn d·ªãch qu·∫£ng b√°, vi·∫øt content viral, th·∫•u hi·ªÉu t√¢m l√Ω kh√°ch h√†ng (Insight).",
    "STORY": "B·∫°n l√† ƒê·∫°i vƒÉn h√†o v√† Bi√™n k·ªãch xu·∫•t s·∫Øc. S·ªü tr∆∞·ªùng: K·ªÉ chuy·ªán (Storytelling) l√¥i cu·ªën, x√¢y d·ª±ng b·ªëi c·∫£nh v√† nh√¢n v·∫≠t c√≥ chi·ªÅu s√¢u.",
    "ARTIST": "B·∫°n l√† Gi√°m ƒë·ªëc Ngh·ªá thu·∫≠t (Art Director). Nhi·ªám v·ª•: T·∫°o ra c√°c m√¥ t·∫£ h√¨nh ·∫£nh (Prompt) chi ti·∫øt, gi√†u t√≠nh th·∫©m m·ªπ cho AI v·∫Ω tranh."
}

def get_system_message(context):
    return CONTEXT_PROMPTS.get(context, CONTEXT_PROMPTS["CHAT"])

def extract_vision_from_pdf(pdf_path):
    """
    PHI√äN B·∫¢N M·ªöI: S·ª≠ d·ª•ng "M·∫Øt th·∫ßn" Gemini Pro Vision ƒë·ªÉ ƒë·ªçc t√†i li·ªáu.
    Thay th·∫ø ho√†n to√†n c√¥ng ngh·ªá OCR c≈© k·ªπ.
    """
    print(colored(f"üëÅÔ∏è [GEMINI VISION] ƒêang qu√©t t√†i li·ªáu: {pdf_path}...", "cyan"))
    
    if not OCR_AVAILABLE: # T·∫≠n d·ª•ng l·∫°i bi·∫øn check n√†y
        return "‚ö†Ô∏è Module x·ª≠ l√Ω ·∫£nh (pdf2image/PIL) ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t tr√™n Server."
    
    try:
        # 1. Chuy·ªÉn PDF th√†nh danh s√°ch ·∫£nh
        images = convert_from_path(pdf_path)
        vision_data = ""
        
        # 2. G·ª≠i t·ª´ng trang cho Gemini nh√¨n
        for i, img in enumerate(images):
            print(colored(f"--> ƒêang ph√¢n t√≠ch trang {i+1}/{len(images)}...", "cyan"))
            
            # Prompt y√™u c·∫ßu Gemini m√¥ t·∫£ chi ti·∫øt nh·ªØng g√¨ n√≥ th·∫•y
            prompt = "B·∫°n l√† chuy√™n gia ph√¢n t√≠ch t√†i li·ªáu. H√£y tr√≠ch xu·∫•t TO√ÄN B·ªò vƒÉn b·∫£n, s·ªë li·ªáu trong b·∫£ng v√† m√¥ t·∫£ c√°c bi·ªÉu ƒë·ªì trong h√¨nh ·∫£nh n√†y m·ªôt c√°ch chi ti·∫øt."
            
            # G·ªçi Gemini Vision (Truy·ªÅn tr·ª±c ti·∫øp ƒë·ªëi t∆∞·ª£ng PIL Image)
            # L∆∞u √Ω: C·∫ßn ƒë·∫£m b·∫£o LLM_GEMINI ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o th√†nh c√¥ng ·ªü ƒë·∫ßu file
            if LLM_GEMINI_LOGIC:
                response = LLM_GEMINI_LOGIC.invoke([
                    HumanMessage(content=[
                        {"type": "text", "text": prompt},
                        {"type": "image_url", "image_url": img} # LangChain h·ªó tr·ª£ truy·ªÅn ·∫£nh tr·ª±c ti·∫øp
                    ])
                ])
                vision_data += f"\n--- N·ªòI DUNG TRANG {i+1} (GEMINI VISION) ---\n{response.content}\n"
            else:
                vision_data += "\n‚ö†Ô∏è Gemini ch∆∞a s·∫µn s√†ng ƒë·ªÉ ph√¢n t√≠ch h√¨nh ·∫£nh.\n"

        return vision_data

    except Exception as e:
        print(colored(f"‚ùå L·ªói Vision: {e}", "red"))
        return f"L·ªói ph√¢n t√≠ch h√¨nh ·∫£nh: {str(e)}"
# Khai b√°o h√†m t√¨m ki·∫øm Node ti·∫øp theo (D√πng cho Orchestrator)
def find_next_node(current_node, workflow_map):
    for link in workflow_map:
        if link["from"] == current_node:
            return link["to"]
    return "Supervisor"

def smart_invoke(primary_model, backup_model, prompt_input):
    """
    C∆° ch·∫ø Fail-over: Th·ª≠ √¥ng 1, n·∫øu l·ªói (h·∫øt ti·ªÅn/rate limit) -> G·ªçi √¥ng 2.
    """
    try:
        # Th·ª≠ g·ªçi √¥ng 1
        return primary_model.invoke(prompt_input)
    except Exception as e:
        error_msg = str(e).lower()
        # Ki·ªÉm tra c√°c t·ª´ kh√≥a l·ªói th∆∞·ªùng g·∫∑p
        if "quota" in error_msg or "rate limit" in error_msg or "credit" in error_msg or "429" in error_msg:
            print(f"‚ö†Ô∏è C·∫¢NH B√ÅO: Model ch√≠nh b·ªã l·ªói '{error_msg}'.")
            print("üîÑ ƒêANG CHUY·ªÇN SANG H·ªÜ TH·ªêNG D·ª∞ PH√íNG (BACKUP)...")
            
            if backup_model:
                try:
                    return backup_model.invoke(prompt_input)
                except Exception as e2:
                    return f"üí• C·∫£ 2 h·ªá th·ªëng ƒë·ªÅu s·∫≠p: {str(e2)}"
            else:
                return "‚ö†Ô∏è Kh√¥ng c√≥ backup n√†o kh·∫£ d·ª•ng."
        else:
            # N·∫øu l·ªói kh√°c (v√≠ d·ª• l·ªói code), n√©m ra ƒë·ªÉ x·ª≠ l√Ω sau
            raise e

def log_training_data(user_input, ai_output, success=True):
    """
    H√†m n√†y √¢m th·∫ßm l∆∞u l·∫°i d·ªØ li·ªáu ƒë·ªÉ sau n√†y Fine-tune AI ri√™ng.
    Ch·ªâ l∆∞u nh·ªØng c√¢u tr·∫£ l·ªùi ƒê√öNG (success=True).
    """
    if not success: return # Kh√¥ng h·ªçc c√°i sai
    
    data_entry = {
        "messages": [
            {"role": "user", "content": user_input},
            {"role": "assistant", "content": ai_output}
        ]
    }
    
    # L∆∞u v√†o file JSONL (ƒê·ªãnh d·∫°ng chu·∫©n ƒë·ªÉ Fine-tune sau n√†y)
    with open("training_data_v1.jsonl", "a", encoding="utf-8") as f:
        f.write(json.dumps(data_entry, ensure_ascii=False) + "\n")
# ============================================================================
# 2. C√ÅC H√ÄM B·ªî TR·ª¢ (HELPER FUNCTIONS) - PH·∫¢I ƒê·ªäNH NGHƒ®A TR∆Ø·ªöC
# ============================================================================
#  ------ X·ª≠ l√Ω ·∫£nh
def process_vision_message(message_content):
    """B√≥c t√°ch d·ªØ li·ªáu h√¨nh ·∫£nh Base64."""
    if isinstance(message_content, str) and "[VISION_DATA:" in message_content:
        parts = message_content.split("] ")
        img_data = parts[0].replace("[VISION_DATA:", "")
        text_query = parts[1] if len(parts) > 1 else ""
        return text_query, img_data
    return message_content, None

#  ---- Ph√¢n T√≠ch Coder------------
def self_heal_analyzer(errors: list) -> str:
    """Ph√¢n t√≠ch l·ªói t·ª´ log ƒë·ªÉ g·ª£i √Ω c√°ch s·ª≠a."""
    if not errors: return ""
    return f"\n‚ö†Ô∏è PH√ÇN T√çCH L·ªñI T·ª™ L·∫¶N CH·∫†Y TR∆Ø·ªöC: {errors[-1]}"

#  ---- G·ª£i √Ω c√¥ng ngh·ªá -----------
def get_optimal_stack(task_type: str) -> str:
    """G·ª£i √Ω c√¥ng ngh·ªá ph√π h·ª£p."""
    stacks = {
        "web": "HTML5, Tailwind CSS, JavaScript ES6",
        "backend": "Python FastAPI, SQLite, Pydantic",
        "iot": "C++, Arduino Framework, ESP32 libs",
        "data": "Python Pandas, Plotly, NumPy"
    }
    return stacks.get(task_type, "Standard Full-stack")

#  --- l·∫•y coder t·ª´ markdown (ƒë·ªãnh d·∫°ng)----------
def extract_code_block(content) -> str:
    """
    H√†m tr√≠ch xu·∫•t code (ƒê√£ n√¢ng c·∫•p ƒë·ªÉ ch·ªëng l·ªói 'got list')
    """
    import re
    
    # 1. X·ª¨ L√ù AN TO√ÄN: N·∫øu ƒë·∫ßu v√†o l√† List (do Anthropic/GPT tr·∫£ v·ªÅ), g·ªôp th√†nh String
    if isinstance(content, list):
        try:
            # C·ªë g·∫Øng l·∫•y text t·ª´ c√°c object n·∫øu c√≥, ho·∫∑c √©p ki·ªÉu string
            content = "\n".join([c.text if hasattr(c, 'text') else str(c) for c in content])
        except:
            content = str(content)
            
    # 2. ƒê·∫£m b·∫£o ch·∫Øc ch·∫Øn l√† String tr∆∞·ªõc khi x·ª≠ l√Ω Regex
    if not isinstance(content, str):
        content = str(content)

    # 3. X·ª¨ L√ù REGEX (Nh∆∞ c≈©)
    # ∆Øu ti√™n block c√≥ language tag (v√≠ d·ª• ```python)
    match = re.search(r'```[\w+\-]*\n(.*?)```', content, re.DOTALL)
    if match:
        return match.group(1).strip()
    
    # Fallback: T√¨m block ``` b·∫•t k·ª≥
    match = re.search(r'```(.*?)```', content, re.DOTALL)
    return match.group(1).strip() if match else None

#  ---- b·ªô n√£o" ch·ªâ d·∫´n cho Claude----
def get_claude_perfected_prompt(task_type: str, memory: str, error: str, user_request: str) -> str:
    """
    T·∫°o prompt t·ªëi ∆∞u cho Claude V3 (Reflexion Mode):
    T·∫≠p trung v√†o vi·ªác H·ªåC T·ª™ L·ªñI SAI ƒë·ªÉ kh√¥ng l·∫∑p l·∫°i bug c≈©.
    """
    # 1. X√°c ƒë·ªãnh Stack c√¥ng ngh·ªá
    tech_stack = get_optimal_stack(task_type)
    
    # 2. X√¢y d·ª±ng n·ªôi dung Prompt (Phi√™n b·∫£n "Nghi√™m Kh·∫Øc")
    prompt = f"""
<system_context>
    <role>
        B·∫°n l√† Senior Full-stack Developer & Software Architect t·∫°i AI Corporation.
        Nhi·ªám v·ª• tr·ªçng t√¢m: REVERSE ENGINEERING k·ªπ thu·∫≠t c·ªßa ƒë·ªëi th·ªß v√† INNOVATION.
        
        üî• QUY T·∫ÆC S·ªêNG C√íN (CRITICAL RULE):
        B·∫°n KH√îNG ƒê∆Ø·ª¢C PH√âP l·∫∑p l·∫°i c√°c l·ªói (bugs/syntax errors) ƒë√£ x·∫£y ra trong c√°c phi√™n b·∫£n tr∆∞·ªõc.
        H√£y ph√¢n t√≠ch k·ªπ nguy√™n nh√¢n th·∫•t b·∫°i trong <error_history> ƒë·ªÉ ƒë∆∞a ra gi·∫£i ph√°p m·ªõi ho√†n to√†n.
    </role>

    <critical_warning>
        ‚ö†Ô∏è L·ªäCH S·ª¨ KI·ªÇM TH·ª¨ TH·∫§T B·∫†I (H√ÉY ƒê·ªåC K·ª∏ ƒê·ªÇ TR√ÅNH V·∫æT XE ƒê·ªî):
        --------------------------------------------------
        {error.strip() if error else "Ch∆∞a c√≥ l·ªói n√†o. ƒê√¢y l√† l·∫ßn d·ª±ng ƒë·∫ßu ti√™n (Clean Start)."}
        --------------------------------------------------
        Y√äU C·∫¶U: Code m·ªõi ph·∫£i kh·∫Øc ph·ª•c tri·ªát ƒë·ªÉ c√°c v·∫•n ƒë·ªÅ tr√™n. Tuy·ªát ƒë·ªëi kh√¥ng sinh ra code c≈©.
    </critical_warning>

    <strategic_knowledge>
        <company_memory>
            {memory.strip() if memory else "Tu√¢n th·ªß Clean Code v√† ti√™u chu·∫©n UX hi·ªán ƒë·∫°i."}
        </company_memory>
    </strategic_knowledge>

    <constraints>
        <technical_stack>
            - Ch·ªß ƒë·∫°o: {tech_stack}
            - UI/UX: Responsive (Mobile-first), Tailwind CSS, Framer Motion animations.
            - Integrity: Ch·ªâ d√πng th∆∞ vi·ªán m√£ ngu·ªìn m·ªü c√≥ gi·∫•y ph√©p MIT/Apache.
        </technical_stack>

        <output_formatting_rules>
            1. FILE_IDENTIFICATION: D√≤ng ƒë·∫ßu ti√™n c·ªßa m·ªói kh·ªëi code PH·∫¢I l√† comment t√™n file.
               - Python: # filename: path/to/file.py
               - JavaScript/TS: // filename: path/to/file.js
               - HTML: - CSS: /* filename: styles.css */
            2. MODULARIZATION: N·∫øu m√£ ngu·ªìn v∆∞·ª£t qu√° 200 d√≤ng, h√£y chia nh·ªè th√†nh c√°c file module/component.
            3. SYNTAX_INTEGRITY: Tuy·ªát ƒë·ªëi kh√¥ng c·∫Øt ngang code. Ph·∫£i ƒë√≥ng ƒë·∫ßy ƒë·ªß c√°c block ```.
            4. DOCUMENTATION: D√πng comment ti·∫øng Vi·ªát ƒë·ªÉ gi·∫£i th√≠ch c√°c logic ph·ª©c t·∫°p v√† c√°c ƒëi·ªÉm c·∫£i ti·∫øn UX.
            5. PDF_SAFETY: Kh√¥ng s·ª≠ d·ª•ng emoji, bi·ªÉu t∆∞·ª£ng ƒë·ªì h·ªça ƒë·∫∑c bi·ªát ho·∫∑c k√Ω t·ª± ngo√†i b·∫£ng m√£ chu·∫©n.
        </output_formatting_rules>
    </constraints>
</system_context>

<user_instruction>
    {user_request.strip()}
</user_instruction>

<final_enforcement>
    CH·ªà TR·∫¢ V·ªÄ C√ÅC KH·ªêI CODE TRONG TH·∫∫ ```. KH√îNG CH√ÄO H·ªéI, KH√îNG GI·∫¢I TH√çCH NGO√ÄI CODE.
</final_enforcement>
"""
    return prompt.strip()
# ============================================================================
# UTILITY: SYNTAX VALIDATOR (B·ªô ki·ªÉm ƒë·ªãnh c√∫ ph√°p ƒëa ng√¥n ng·ªØ)
# ============================================================================
def real_syntax_validator(code: str, language: str) -> tuple[bool, str]:
    """
    Ki·ªÉm ƒë·ªãnh m√£ ngu·ªìn chuy√™n s√¢u: Python (AST), JS/HTML (Regex/Stack), C++ (Structure).
    """
    if not code or len(code.strip()) < 10:
        return False, "M√£ ngu·ªìn qu√° ng·∫Øn ho·∫∑c tr·ªëng."

    language = language.lower()

    # 1. KI·ªÇM TRA PYTHON (S·ª≠ d·ª•ng Abstract Syntax Tree)
    if any(kw in language for kw in ["python", "py"]) or "def " in code:
        try:
            ast.parse(code)
            return True, "‚úÖ Python Syntax: OK"
        except SyntaxError as e:
            return False, f"‚ùå Python Error [D√≤ng {e.lineno}]: {e.msg}"

    # 2. KI·ªÇM TRA JAVASCRIPT / WEB (C·∫£i ti·∫øn c∆° ch·∫ø Stack & Tag)
    if any(kw in language for kw in ["script", "js", "html"]):
        # X√≥a b·ªè n·ªôi dung trong chu·ªói ƒë·ªÉ tr√°nh b·∫Øt nh·∫ßm ngo·∫∑c trong text
        clean_code = re.sub(r"'(.*?)'|\"(.*?)\"|`(.*?)`", "", code)
        stack = []
        mapping = {')': '(', ']': '[', '}': '{'}
        
        for char in clean_code:
            if char in mapping.values():
                stack.append(char)
            elif char in mapping:
                if not stack or mapping[char] != stack.pop():
                    return False, "‚ùå JS/HTML Error: M·∫•t c√¢n b·∫±ng ho·∫∑c sai th·ª© t·ª± ƒë√≥ng m·ªü ngo·∫∑c."
        
        if stack:
            return False, f"‚ùå JS/HTML Error: C√≤n {len(stack)} d·∫•u ngo·∫∑c ch∆∞a ƒë∆∞·ª£c ƒë√≥ng."
            
        # Ki·ªÉm tra th·∫ª HTML c∆° b·∫£n n·∫øu l√† HTML
        if "<" in code and ">" in code:
            if code.count("<") != code.count(">"):
                return False, "‚ùå HTML Error: Sai l·ªách s·ªë l∆∞·ª£ng th·∫ª ƒë√≥ng/m·ªü < >"

        return True, "‚úÖ Web Syntax: Basic Check Passed"

    # 3. KI·ªÇM TRA C++ / FIRMWARE (D√†nh cho Hardware Node)
    if any(kw in language for kw in ["arduino", "cpp", "c++", "ino"]):
        if "void setup()" not in code or "void loop()" not in code:
            if "extern " not in code: # Tr√°nh b·∫Øt l·ªói file th∆∞ vi·ªán
                return False, "‚ùå C++ Error: Thi·∫øu c·∫•u tr√∫c Arduino c∆° b·∫£n (setup/loop)."
        
        # Ki·ªÉm tra d·∫•u ch·∫•m ph·∫©y (;) - l·ªói kinh ƒëi·ªÉn c·ªßa C++
        lines = [l.strip() for l in code.split('\n') if l.strip() and not l.strip().startswith(("//", "#", "{", "}"))]
        for line in lines:
            if not line.endswith((";", "{", "}", ",")) and not line.startswith("if"):
                # ƒê√¢y ch·ªâ l√† check c·∫£nh b√°o, kh√¥ng √©p bu·ªôc v√¨ C++ r·∫•t ph·ª©c t·∫°p
                print(colored(f"‚ö†Ô∏è C·∫£nh b√°o C++: D√≤ng '{line}' c√≥ th·ªÉ thi·∫øu d·∫•u ';'", "yellow"))
        
        return True, "‚úÖ C++ Structure: OK"

    return True, "‚ö†Ô∏è Unknown language: Skip deep validation"

# ============================================================================
# SAFETY: ULTIMATE FALLBACK (H·ªá th·ªëng t·ª± ph·ª•c h·ªìi & Ch·ªëng s·ª•p ƒë·ªï)
# ============================================================================
def ultimate_fallback(state, messages):
    """
    Quy tr√¨nh x·ª≠ l√Ω s·ª± c·ªë kh·∫©n c·∫•p: Ghi log, ph√¢n t√≠ch l·ªói v√† t√°i kh·ªüi ƒë·ªông an to√†n.
    """
    # 1. Thu th·∫≠p d·ªØ li·ªáu l·ªói t·ª´ State
    error_logs = state.get("error_log", [])
    last_error = error_logs[-1] if error_logs else "L·ªói kh√¥ng x√°c ƒë·ªãnh (Internal Server Error)"
    
    print(colored(f"üö® [CRITICAL ERROR] H·ªá th·ªëng ƒëang k√≠ch ho·∫°t quy tr√¨nh ·ª©ng c·ª©u kh·∫©n c·∫•p!", "red", attrs=["bold"]))
    print(colored(f"--> Chi ti·∫øt l·ªói: {last_error}", "red"))

    # 2. Ghi nh·∫≠t k√Ω l·ªói v√†o file v·∫≠t l√Ω (ƒê·ªÉ k·ªπ thu·∫≠t vi√™n ki·ªÉm tra sau)
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    with open("system_crash_log.txt", "a", encoding="utf-8") as f:
        f.write(f"[{timestamp}] ERROR: {last_error}\n")

    # 

    # 3. X√¢y d·ª±ng th√¥ng ƒëi·ªáp chuy√™n nghi·ªáp cho CEO
    error_summary = (
        "üõë **TH√îNG B√ÅO H·ªÜ TH·ªêNG**: AI Corporation v·ª´a g·∫∑p m·ªôt s·ª± c·ªë k·ªπ thu·∫≠t ngo√†i √Ω mu·ªën.\n\n"
        f"üîç **Ph√¢n t√≠ch nhanh**: `{last_error[:200]}...`\n"
        "üõ†Ô∏è **H√†nh ƒë·ªông**: To√†n b·ªô d·ªØ li·ªáu d·ª± √°n ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°m th·ªùi. T√¥i ƒëang th·ª±c hi·ªán reset c√°c tham s·ªë ƒë·ªÉ tr√°nh treo lu·ªìng.\n\n"
        "üëâ **CEO c√≥ th·ªÉ**: Th·ª≠ nh·∫≠p l·ªánh ng·∫Øn g·ªçn h∆°n ho·∫∑c g√µ 'restart' ƒë·ªÉ l√†m m·ªõi ho√†n to√†n b·ªô n√£o."
    )

    # 4. Tr·∫£ v·ªÅ tr·∫°ng th√°i an to√†n
    return {
        "messages": [AIMessage(content=error_summary)],
        "next_step": "FINISH", # Ho·∫∑c ƒë·∫©y v·ªÅ Supervisor n·∫øu mu·ªën AI t·ª± th·ª≠ l·∫°i
        "error_log": error_logs + ["System Fallback Triggered"]
    }

# ============================================================================
# 3. H·ªá Th·ªëng B·ªô Nh·ªõ
# ============================================================================
# ============================================================================
# UTILITY: INGEST DOCUMENTS (H·ªá th·ªëng n·∫°p tri th·ª©c ƒëa ngu·ªìn)
# ============================================================================

def ingest_docs_to_memory(folder_path="./data_sources"):
    """
    Quy tr√¨nh ETL chuy√™n nghi·ªáp: Tr√≠ch xu·∫•t, Bi·∫øn ƒë·ªïi v√† N·∫°p tri th·ª©c v√†o Vector DB.
    H·ªó tr·ª£: Metadata Mapping, Batch Loading v√† Integrity Check.
    """
    # 1. Kh·ªüi t·∫°o & Ki·ªÉm tra m√¥i tr∆∞·ªùng
    if not os.path.exists(folder_path): 
        os.makedirs(folder_path)
        return f"üìÇ Th∆∞ m·ª•c '{folder_path}' ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o. H√£y th√™m t√†i li·ªáu PDF."

    print(colored(f"üöÄ [ETL PROCESS] B·∫Øt ƒë·∫ßu n·∫°p tri th·ª©c t·ª´: {folder_path}", "cyan", attrs=["bold"]))

    # 2. C·∫•u h√¨nh Loader th√¥ng minh
    try:
        # S·ª≠ d·ª•ng DirectoryLoader v·ªõi PyPDFLoader ƒë·ªÉ b√≥c t√°ch Metadata t·ª± ƒë·ªông
        loader = DirectoryLoader(
            folder_path, 
            glob="./*.pdf", 
            loader_cls=PyPDFLoader,
            show_progress=True,
            use_multithreading=True # T·ªëi ∆∞u h√≥a t·ªëc ƒë·ªô ƒë·ªçc file
        )
        docs = loader.load()
    except Exception as e:
        return f"‚ùå L·ªói tr√≠ch xu·∫•t (Extraction Error): {str(e)}"

    if not docs:
        return "‚ö†Ô∏è Tr·∫°ng th√°i: Kh√¥ng t√¨m th·∫•y t√†i li·ªáu PDF m·ªõi ƒë·ªÉ x·ª≠ l√Ω."

    # 3. Chi·∫øn l∆∞·ª£c ph√¢n m·∫£nh (Chunking Strategy) chuy√™n s√¢u
    # TƒÉng overlap l√™n 200 ƒë·ªÉ tr√°nh m·∫•t ng·ªØ c·∫£nh gi·ªØa c√°c ƒëo·∫°n (Context preservation)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, 
        chunk_overlap=200,
        length_function=len,
        add_start_index=True # L∆∞u v·ªã tr√≠ b·∫Øt ƒë·∫ßu ƒë·ªÉ truy xu·∫•t ch√≠nh x√°c
    )
    splits = text_splitter.split_documents(docs)

    # 4. L√†m s·∫°ch d·ªØ li·ªáu & Chu·∫©n h√≥a Metadata
    valid_splits = []
    for doc in splits:
        clean_content = doc.page_content.strip()
        if len(clean_content) > 50: # Lo·∫°i b·ªè c√°c m·∫©u r√°c ho·∫∑c trang tr·∫Øng
            # B·ªï sung th√¥ng tin ngu·ªìn ƒë·ªÉ AI tr√≠ch d·∫´n sau n√†y
            doc.metadata["ingested_at"] = datetime.now().isoformat()
            doc.metadata["doc_hash"] = hash(clean_content) # H·ªó tr·ª£ ch·ªëng tr√πng l·∫∑p s∆° b·ªô
            valid_splits.append(doc)

    if not valid_splits:
        return "‚ö†Ô∏è C·∫£nh b√°o: T√†i li·ªáu OCR/·∫¢nh kh√¥ng th·ªÉ b√≥c t√°ch n·ªôi dung vƒÉn b·∫£n."

    # 

    # 5. N·∫°p d·ªØ li·ªáu v√†o Vector DB theo t·ª´ng Batch (Ch·ªëng tr√†n RAM)
    try:
        batch_size = 100
        total_chunks = len(valid_splits)
        print(colored(f"üì¶ ƒêang m√£ h√≥a v√† n·∫°p {total_chunks} ph√¢n ƒëo·∫°n v√†o b·ªô n√£o...", "white"))
        
        for i in range(0, total_chunks, batch_size):
            batch = valid_splits[i:i + batch_size]
            vector_db.add_documents(batch)
            
        print(colored("‚úÖ [INGESTION SUCCESS] Tri th·ª©c ƒë√£ ƒë∆∞·ª£c ƒë·ªìng b·ªô h√≥a to√†n di·ªán.", "green", attrs=["bold"]))
        return f"üöÄ Th√†nh c√¥ng: ƒê√£ n·∫°p {total_chunks} ph√¢n ƒëo·∫°n t·ª´ {len(docs)} t√†i li·ªáu v√†o b·ªô n√£o trung t√¢m."

    except Exception as e:
        return f"‚ùå L·ªói n·∫°p d·ªØ li·ªáu (Load Error): {str(e)}"
# ============================================================================
# UTILITY: REMEMBER KNOWLEDGE (Ghi nh·ªõ tri th·ª©c & K√Ω ·ª©c ng·∫Øn h·∫°n)
# ============================================================================
def remember_knowledge(text: str, category: str = "General", priority: int = 1):
    """
    H·ªá th·ªëng ghi nh·ªõ th√¥ng minh: T·ª± ƒë·ªông ph√¢n lo·∫°i, g·∫Øn nh√£n th·ªùi gian v√† l∆∞u tr·ªØ.
    """
    if not text or len(text.strip()) < 10:
        return "‚ö†Ô∏è N·ªôi dung qu√° ng·∫Øn, h·ªá th·ªëng t·ª´ ch·ªëi ghi nh·ªõ."

    print(colored(f"üíæ [MEMORY SAVE] ƒêang n·∫°p tri th·ª©c m·ªõi v√†o danh m·ª•c: {category}...", "green"))

    try:
        # 1. T·∫°o Metadata chuy√™n nghi·ªáp
        # Vi·ªác n√†y gi√∫p sau n√†y search theo "Th·ªùi gian" ho·∫∑c "Ch·ªß ƒë·ªÅ" c·ª±c nhanh
        metadata = {
            "category": category,
            "priority": priority,
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "source": "AI_Internal_Learning" # ƒê√°nh d·∫•u ƒë√¢y l√† ki·∫øn th·ª©c t·ª± h·ªçc t·ª´ h·ªôi tho·∫°i
        }

        # 2. Chia nh·ªè vƒÉn b·∫£n (n·∫øu text qu√° d√†i) ƒë·ªÉ t·ªëi ∆∞u h√≥a t√¨m ki·∫øm sau n√†y
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        chunks = text_splitter.split_text(text)

        # 3. N·∫°p v√†o Vector DB
        # Ch√∫ng ta d√πng add_texts nh∆∞ng k√®m theo list metadata t∆∞∆°ng ·ª©ng cho t·ª´ng chunk
        vector_db.add_texts(
            texts=chunks,
            metadatas=[metadata] * len(chunks)
        )
        # Ta coi vi·ªác h·ªçc l√† c√¥ng lao c·ªßa [LIBRARY] ho·∫∑c [SECRETARY]
        log_work_to_db(
            agent="SECRETARY", 
            task=f"Ghi nh·ªõ ki·∫øn th·ª©c: {category}",
            result=f"ƒê√£ n·∫°p {len(chunks)} ph√¢n ƒëo·∫°n v√†o n√£o. N·ªôi dung: {text[:50]}...",
            tool="Memory Engine"
        )

        # 4. L∆∞u log ƒë·ªÉ CEO theo d√µi
        success_msg = f"‚úÖ ƒê√£ ghi nh·ªõ {len(chunks)} ph√¢n ƒëo·∫°n tri th·ª©c v√†o danh m·ª•c '{category}'."
        print(colored(success_msg, "green"))
        
        return success_msg

    except Exception as e:
        error_msg = f"‚ùå L·ªói ghi nh·ªõ: {str(e)}"
        print(colored(error_msg, "red"))
        return error_msg

#  --- h·ªçc ƒë·ªÉ ti·∫øn b·ªô----
def save_for_finetuning(prompt, response, metadata):
    # Ch·ªâ l∆∞u n·∫øu code n√†y ƒë√£ ƒë∆∞·ª£c Tester x√°c nh·∫≠n l√† ƒê√öNG (Pass)
    entry = {
        "instruction": prompt,
        "input": metadata.get("context", ""),
        "output": response,
        "source": metadata.get("model_name") # L∆∞u ƒë·ªÉ bi·∫øt ƒë√¢y l√† ki·∫øn th·ª©c t·ª´ Claude hay GPT-4
    }
    with open("knowledge_legacy.jsonl", "a", encoding="utf-8") as f:
        f.write(json.dumps(entry, ensure_ascii=False) + "\n")   


#  ----Th√™m vƒÉn b·∫£n v√†o ChromaDB----
def learn_knowledge(text: str):
    """
    L∆∞u ki·∫øn th·ª©c m·ªõi v√†o b·ªô n√£o trung t√¢m (ChromaDB).
    ƒê·ªìng b·ªô v·ªõi ƒë·ªëi t∆∞·ª£ng vector_db ƒë√£ kh·ªüi t·∫°o ·ªü ƒë·∫ßu file.
    """
    try:
        # Th√™m vƒÉn b·∫£n v√†o ChromaDB hi·ªán c√≥
        vector_db.add_texts([text])
        
        # Ghi ch√∫: ChromaDB trong b·∫£n m·ªõi th∆∞·ªùng t·ª± ƒë·ªông persist (l∆∞u) 
        # n√™n kh√¥ng c·∫ßn g·ªçi l·ªánh .persist() th·ªß c√¥ng nh∆∞ c√°c b·∫£n c≈©.
        
        print(colored(f"--> [MEMORY] ƒê√£ h·ªçc: {text[:50]}...", "green"))
        return "‚úÖ H·ªá th·ªëng ƒë√£ ghi nh·ªõ ki·∫øn th·ª©c n√†y v√†o b·ªô n√£o trung t√¢m (ChromaDB)."
    except Exception as e:
        return f"‚ùå L·ªói khi ghi nh·ªõ ki·∫øn th·ª©c: {e}"

def log_work_to_db(agent, task, result, tool="GPT-4"):
    """H√†m ghi ch√©p c√¥ng vi·ªác v√†o S·ªï C√°i & C·ªông XP (ƒê√£ Fix l·ªói Level)"""
    try:
        # ƒê∆∞·ªùng d·∫´n DB chu·∫©n
        db_path = "/var/data/ai_corp_projects.db" if os.path.exists("/var/data") else "ai_corp_projects.db"
        
        # T√≠nh ti·ªÅn
        cost = len(str(result)) * 0.00001 
        if "deepseek" in tool.lower(): cost = cost / 10 
        
        conn = sqlite3.connect(db_path, timeout=10) # Th√™m timeout
        c = conn.cursor()
        
        # 1. Ghi Log chi ti·∫øt (Work Logs)
        c.execute("""
            INSERT INTO work_logs (timestamp, agent_name, task_content, result_summary, tool_used, cost)
            VALUES (?, ?, ?, ?, ?, ?)
        """, (
            datetime.now().strftime("%H:%M %d/%m"),
            agent,
            str(task)[:100], 
            str(result)[:200], 
            tool,
            cost
        ))
        
        # 2. C·ªòNG ƒêI·ªÇM XP (FIX L·ªñI QUAN TR·ªåNG)
        # Chu·∫©n h√≥a t√™n Agent ƒë·ªÉ kh·ªõp v·ªõi b·∫£ng agent_status
        # V√≠ d·ª•: "Researcher" -> "[RESEARCH]"
        # Map n√†y ph·∫£i kh·ªõp v·ªõi l√∫c kh·ªüi t·∫°o DB
        role_map = {
            # --- NH√ìM S√ÅNG T·∫†O & C·ªêT L√ïI ---
            "RESEARCHER": "[RESEARCH]",
            "CODER": "[CODER]",
            "ARTIST": "[ARTIST]",
            "STORYTELLER": "[STORY]",
            "MARKETING": "[MARKETING]",
            
            # --- NH√ìM QU·∫¢N TR·ªä & ƒêI·ªÄU PH·ªêI ---
            "ORCHESTRATOR": "[ORCHESTRATOR]",
            "SUPERVISOR": "[SUPERVISOR]",
            "SECRETARY": "[SECRETARY]",
            "ROUTER": "[ROUTER]",
            "PUBLISHER": "[PUBLISHER]",
            
            # --- NH√ìM K·ª∏ THU·∫¨T & PH·∫¶N C·ª®NG ---
            "HARDWARE": "[HARDWARE]",
            "ENGINEERING": "[ENGINEERING]",
            "IOT_ENGINEER": "[IOT]",         # L∆∞u √Ω: T√™n node l√† IoT_Engineer -> Tag l√† [IOT]
            "TESTER": "[TESTER]",
            
            # --- NH√ìM NGHI·ªÜP V·ª§ (T√ÄI CH√çNH/PH√ÅP L√ù) ---
            "PROCUREMENT": "[PROCUREMENT]",  # Thu mua
            "INVESTMENT": "[INVESTMENT]",    # T√†i ch√≠nh
            "LEGAL": "[LEGAL]",              # Ph√°p l√Ω
            "STRATEGY_R_AND_D": "[STRATEGY]" # Chi·∫øn l∆∞·ª£c (T√™n node d√†i -> Tag ng·∫Øn)
        }
        
        target_role = role_map.get(agent.upper(), f"[{agent.upper()}]") # Fallback n·∫øu kh√¥ng c√≥ trong map
        
        # C·ªông 50 XP
        c.execute("UPDATE agent_status SET xp = xp + 50, last_updated = ? WHERE role_tag = ?", 
                  (datetime.now(), target_role))
        
        # N·∫øu ch∆∞a c√≥ th√¨ t·∫°o m·ªõi lu√¥n (Tr√°nh tr∆∞·ªùng h·ª£p nh√¢n vi√™n m·ªõi ch∆∞a c√≥ h·ªì s∆°)
        c.execute("""
            INSERT OR IGNORE INTO agent_status (role_tag, xp, current_topic, last_updated)
            VALUES (?, 50, ?, ?)
        """, (target_role, "V·ª´a ho√†n th√†nh nhi·ªám v·ª•", datetime.now()))

        conn.commit()
        conn.close()
        
        print(colored(f"‚úÖ [AUDIT] {agent} ({target_role}): +50 XP | Cost: ${cost:.6f}", "green"))
        
    except Exception as e:
        print(colored(f"‚ö†Ô∏è L·ªói ghi log/XP: {e}", "yellow"))

# ============================================================================
# NODE: KNOWLEDGE RETRIEVAL (Truy xu·∫•t Tri th·ª©c & K√Ω ·ª©c doanh nghi·ªáp)
# ============================================================================
def recall_knowledge(query: str, top_k: int = 3):
    """
    Truy xu·∫•t tri th·ª©c th√¥ng minh: T√¨m ki·∫øm ng·ªØ nghƒ©a, l·ªçc nhi·ªÖu v√† tr√≠ch d·∫´n ngu·ªìn.
    """
    print(colored(f"[üß† RECALL] ƒêang truy xu·∫•t k√Ω ·ª©c cho: '{query}'...", "green"))

    try:
        # 1. T√¨m ki·∫øm v·ªõi ƒëi·ªÉm tin c·∫≠y (Similarity Search with Score)
        # ƒêi·ªÉm c√†ng th·∫•p (trong ChromaDB/L2 Distance) th√¨ c√†ng ch√≠nh x√°c
        results_with_scores = vector_db.similarity_search_with_score(query, k=top_k)

        if not results_with_scores:
            return "H·ªá th·ªëng ch∆∞a c√≥ k√Ω ·ª©c v·ªÅ v·∫•n ƒë·ªÅ n√†y."

        # 

        # 2. L·ªçc k·∫øt qu·∫£ (Threshold Filtering)
        # Ch·ªâ l·∫•y nh·ªØng ƒëo·∫°n ki·∫øn th·ª©c c√≥ ƒë·ªô li√™n quan cao (ƒëi·ªÉm kho·∫£ng < 0.6 - 0.8 t√πy model)
        valid_context = []
        sources = set()

        for doc, score in results_with_scores:
            if score < 0.8:  # Ng∆∞·ª°ng tin c·∫≠y
                source_name = doc.metadata.get("source", "T√†i li·ªáu n·ªôi b·ªô")
                page = doc.metadata.get("page", "N/A")
                
                context_block = f"--- TR√çCH D·∫™N T·ª™: {source_name} (Trang {page}) ---\n{doc.page_content}"
                valid_context.append(context_block)
                sources.add(source_name)

        if not valid_context:
            return "T√¨m th·∫•y th√¥ng tin nh∆∞ng ƒë·ªô tin c·∫≠y qu√° th·∫•p ƒë·ªÉ s·ª≠ d·ª•ng."

        # 3. T·ªïng h·ª£p b√°o c√°o tri th·ª©c cho Agent
        final_memory = "\n\n".join(valid_context)
        
        print(colored(f"‚úÖ ƒê√£ t√¨m th·∫•y tri th·ª©c t·ª´ {len(sources)} ngu·ªìn uy t√≠n.", "green"))
        return final_memory

    except Exception as e:
        print(colored(f"‚ùå L·ªói truy xu·∫•t b·ªô n√£o: {e}", "red"))
        return "H·ªá th·ªëng l∆∞u tr·ªØ tri th·ª©c ƒëang g·∫∑p s·ª± c·ªë k·ªπ thu·∫≠t."

def router_node(state):
    """
    Router: ƒêi·ªÉm g√°c c·ªïng ƒë·∫ßu ti√™n.
    """
    # 1. L·∫•y d·ªØ li·ªáu an to√†n
    messages = state.get("messages", [])
    error_log = state.get("error_log", [])
    task_type = state.get("task_type", "general")
    
    # 2. Ki·ªÉm tra n·∫øu kh√¥ng c√≥ tin nh·∫Øn
    if not messages:
        return {
            "messages": [],
            "next_step": "Supervisor", 
            "current_agent": "Router",
            "error_log": error_log,
            "task_type": task_type
        }

    # 3. L·∫•y n·ªôi dung tin nh·∫Øn cu·ªëi
    last_msg = messages[-1].content.upper() if hasattr(messages[-1], 'content') else str(messages[-1]).upper()

    # 4. B·∫¢N ƒê·ªí ƒêI·ªÄU H∆Ø·ªöNG C∆Ø·ª†NG B·ª®C
    route_map = {
        "[RESEARCH]": "Researcher",
        "[INVEST]": "Investment",
        "[HARDWARE]": "Hardware",
        "[ENGINEERING]": "Engineering",
        "[IOT]": "IoT_Engineer",
        "[MARKETING]": "Marketing",
        "[LEGAL]": "Legal",
        "[STORY]": "Storyteller",
        "[PUBLISH]": "Publisher"
    }

    # 5. KI·ªÇM TRA TAG V√Ä ƒê·ªäNH TUY·∫æN
    for tag, target_node in route_map.items():
        if tag in last_msg:
            print(colored(f"üöÄ [ROUTER] Ph√°t hi·ªán TAG {tag}: ƒêi th·∫≥ng t·ªõi {target_node}", "green"))
            return {
                "messages": [], # B·∫Øt bu·ªôc c√≥
                "next_step": target_node, 
                "current_agent": "Router",
                "error_log": error_log,
                "task_type": task_type
            }

    # 6. M·∫∂C ƒê·ªäNH: Chuy·ªÉn v·ªÅ Supervisor (S·ª≠a l·ªói bi·∫øn node ch∆∞a ƒë·ªãnh nghƒ©a)
    print(colored("üß† [ROUTER] Kh√¥ng c√≥ TAG: Chuy·ªÉn h·ªì s∆° cho Supervisor ƒëi·ªÅu ph·ªëi...", "cyan"))
    return {
        "messages": [], # B·∫Øt bu·ªôc c√≥
        "next_step": "Supervisor", # Tr·∫£ v·ªÅ chu·ªói c·ª• th·ªÉ thay v√¨ bi·∫øn node
        "current_agent": "Router",
        "error_log": error_log,
        "task_type": task_type
    }

# ============================================================================
# UTILITY: SEARCH MEMORY (C√¥ng c·ª• truy v·∫•n tri th·ª©c chuy√™n s√¢u)
# ============================================================================
def search_memory(query: str, k: int = 3):
    """
    T√¨m ki·∫øm th√¥ng tin t·ª´ ChromaDB b·∫±ng thu·∫≠t to√°n Similarity Search v·ªõi ng∆∞·ª°ng tin c·∫≠y.
    """
    print(colored(f"üîç [MEMORY SEARCH] ƒêang truy v·∫•n: '{query}'", "dark_grey"))
    
    try:
        # 1. S·ª≠ d·ª•ng similarity_search_with_score ƒë·ªÉ ƒëo l∆∞·ªùng ƒë·ªô ch√≠nh x√°c
        # K·∫øt qu·∫£ tr·∫£ v·ªÅ l√† list c√°c tuple (Document, Score)
        results = vector_db.similarity_search_with_score(query, k=k)
        
        if not results:
            return "D·ªØ li·ªáu tr·ªëng ho·∫∑c kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan."

        # 

        # 2. L·ªçc k·∫øt qu·∫£ d·ª±a tr√™n Score (Kho·∫£ng c√°ch vector)
        # Trong ChromaDB, score c√†ng th·∫•p (g·∫ßn 0) th√¨ c√†ng gi·ªëng nhau
        valid_contents = []
        for doc, score in results:
            # Ng∆∞·ª°ng 0.6 l√† kh√° ch·∫∑t ch·∫Ω, ƒë·∫£m b·∫£o th√¥ng tin ch·∫•t l∆∞·ª£ng
            if score < 0.6: 
                source = doc.metadata.get('source', 'Unknown')
                content = f"[Ngu·ªìn: {source}]\n{doc.page_content}"
                valid_contents.append(content)
        
        if not valid_contents:
            return "T√¨m th·∫•y d·ªØ li·ªáu nh∆∞ng ƒë·ªô li√™n quan kh√¥ng ƒë·ªß cao ƒë·ªÉ h·ªó tr·ª£ quy·∫øt ƒë·ªãnh."

        # 3. G·ªôp c√°c m·∫©u ki·∫øn th·ª©c l·∫°i th√†nh m·ªôt kh·ªëi b·ªëi c·∫£nh (Context Block)
        formatted_result = "\n" + "="*30 + "\n"
        formatted_result += "\n\n".join(valid_contents)
        formatted_result += "\n" + "="*30
        
        return formatted_result

    except Exception as e:
        print(colored(f"‚ùå L·ªói truy v·∫•n b·ªô n√£o: {e}", "red"))
        return "L·ªói h·ªá th·ªëng khi truy xu·∫•t b·ªô nh·ªõ."

def log_to_legacy_dataset(task_type: str, prompt: str, completion: str, model_name: str, score: int):
    """
    L∆∞u tr·ªØ c√°c phi√™n l√†m vi·ªác ch·∫•t l∆∞·ª£ng cao ƒë·ªÉ ph·ª•c v·ª• Fine-tuning Local LLM sau n√†y.
    """
    # Ch·ªâ l∆∞u nh·ªØng n·ªôi dung c√≥ ƒëi·ªÉm ch·∫•t l∆∞·ª£ng cao (v√≠ d·ª• score t·ª´ Tester >= 70)
    if score < 70:
        return

    file_path = "corporate_brain_dataset.jsonl"
    
    # C·∫•u tr√∫c d·ªØ li·ªáu theo chu·∫©n Instruct Tuning
    entry = {
        "timestamp": datetime.now().isoformat(),
        "task_group": task_type,
        "instruction": f"B·∫°n l√† chuy√™n gia {task_type} t·∫°i AI Corporation. H√£y th·ª±c hi·ªán: {prompt}",
        "context": "S·ª≠ d·ª•ng ti√™u chu·∫©n Clean Code v√† ki·∫øn tr√∫c h·ªá th·ªëng t·ªëi ∆∞u.",
        "response": completion,
        "teacher_model": model_name,
        "quality_score": score
    }

    try:
        with open(file_path, "a", encoding="utf-8") as f:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")
        print(colored(f"üìî [SHADOW LEARNING] ƒê√£ l∆∞u 1 m·∫´u tri th·ª©c t·ª´ {model_name} v√†o b·ªô nh·ªõ k·∫ø th·ª´a.", "blue"))
    except Exception as e:
        print(colored(f"‚ö†Ô∏è L·ªói l∆∞u dataset: {e}", "red"))

#  ----- M·ª©c ƒë·ªô k·∫ø th·ª´a----
def legacy_audit_report():
    """
    B√°o c√°o ti·∫øn ƒë·ªô t√≠ch l≈©y tri th·ª©c ƒë·ªÉ chu·∫©n b·ªã cho vi·ªác tho√°t ly API.
    """
    file_path = "corporate_brain_dataset.jsonl"
    if not os.path.exists(file_path):
        return "üìâ H·ªá th·ªëng ch∆∞a c√≥ d·ªØ li·ªáu k·∫ø th·ª´a. H√£y b·∫Øt ƒë·∫ßu ch·∫°y c√°c d·ª± √°n!"

    stats = {}
    total_count = 0

    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            data = json.loads(line)
            group = data.get("task_group", "Unknown")
            stats[group] = stats.get(group, 0) + 1
            total_count += 1

    print(colored("\n" + "="*40, "magenta"))
    print(colored("üìú B√ÅO C√ÅO TI·∫æN ƒê·ªò K·∫æ TH·ª™A TRI TH·ª®C", "magenta", attrs=["bold"]))
    print(colored(f"T·ªïng s·ªë m·∫´u ch·∫•t l∆∞·ª£ng cao: {total_count}", "white"))
    
    for group, count in stats.items():
        # Gi·∫£ s·ª≠ 500 m·∫´u l√† ƒë·ªß ƒë·ªÉ Fine-tune s∆° b·ªô m·ªôt Agent
        progress = min((count / 500) * 100, 100)
        color = "green" if progress >= 80 else "yellow"
        print(f"- {group:15}: {count:4} m·∫´u ({progress:>5.1f}%) " + colored("‚ñà" * int(progress/5), color))
    
    print(colored("="*40 + "\n", "magenta"))

def orchestrator_router(state):
    """
    B·ªô n√£o ƒëi·ªÅu ph·ªëi: Quy·∫øt ƒë·ªãnh ai l√† ng∆∞·ªùi ti·∫øp theo d·ª±a tr√™n ti·∫øn ƒë·ªô d·ª± √°n.
    """
    messages = state.get("messages", [])
    last_msg = messages[-1].content.upper()

    # 1. N·∫øu ƒëang ·ªü giai ƒëo·∫°n t√¨m ki·∫øm th·ªã tr∆∞·ªùng
    if "KI·ªÇM TRA TH·ªä TR∆Ø·ªúNG" in last_msg or "RESEARCH" in last_msg:
        return "Researcher"
    
    # 2. N·∫øu nghi√™n c·ª©u xong v√† c·∫ßn thi·∫øt k·∫ø
    if "PH∆Ø∆†NG √ÅN THI·∫æT K·∫æ" in last_msg or "DESIGN" in last_msg:
        return "Hardware"

    # 3. N·∫øu ƒë√£ c√≥ danh m·ª•c linh ki·ªán (BOM), chuy·ªÉn sang mua h√†ng
    if "BOM" in last_msg or "LINH KI·ªÜN" in last_msg:
        return "Procurement"

    # 4. N·∫øu h√†ng v·ªÅ, chuy·ªÉn sang l·∫Øp r√°p & n·∫°p code
    if "L·∫ÆP R√ÅP" in last_msg or "ASSEMBLY" in last_msg:
        return "IoT_Engineer"

    # 5. Cu·ªëi c√πng, t√¨m ng∆∞·ªùi v·∫≠n h√†nh
    if "NH√ÇN S·ª∞" in last_msg or "RECRUIT" in last_msg:
        return "HR"

    return "Supervisor"

workflow_map = [
    {"from": "Researcher", "to": "Engineering", "condition": "if_not_exist"},
    {"from": "Engineering", "to": "Procurement", "condition": "on_approval"},
    {"from": "Procurement", "to": "IoT_Engineer", "condition": "on_arrival"}
]

def dynamic_orchestrator(state):
    """
    B·ªô ƒëi·ªÅu ph·ªëi ƒë·ªông (Server Mode - Non-blocking).
    
    L·ªñI C≈®: D√πng input() khi·∫øn Server treo khi ch·∫°y ng·∫ßm.
    S·ª¨A ƒê·ªîI: T·ª± ƒë·ªông chuy·ªÉn quy·ªÅn v·ªÅ Supervisor (CEO AI) ƒë·ªÉ quy·∫øt ƒë·ªãnh b∆∞·ªõc ti·∫øp theo.
    """
    # 1. L·∫•y th√¥ng tin ng·ªØ c·∫£nh hi·ªán t·∫°i
    last_agent = state.get("current_agent", "Unknown Agent")
    
    # L·∫•y n·ªôi dung tin nh·∫Øn cu·ªëi c√πng ƒë·ªÉ log (n·∫øu c·∫ßn)
    # last_message = state["messages"][-1].content 

    # 2. Ghi log ra Terminal Server (ƒê·ªÉ k·ªπ thu·∫≠t vi√™n theo d√µi ng·∫ßm)
    # S·ª≠ d·ª•ng m√†u s·∫Øc ƒë·ªÉ d·ªÖ ph√¢n bi·ªát trong ƒë·ªëng log h·ªón ƒë·ªôn
    print(colored(f"\n" + "="*50, "yellow"))
    print(colored(f"üö© [ORCHESTRATOR] NH·∫¨N B√ÅO C√ÅO T·ª™: {last_agent.upper()}", "yellow", attrs=["bold"]))
    print(colored("--> Tr·∫°ng th√°i: T·ª± ƒë·ªông chuy·ªÉn h·ªì s∆° v·ªÅ Supervisor.", "white"))
    print(colored("="*50, "yellow"))

    # 3. LOGIC ƒêI·ªÄU H∆Ø·ªöNG (CASE 2)
    # Thay v√¨ return {"next_step": input(...)} g√¢y treo,
    # ta tr·∫£ v·ªÅ "Supervisor".
    # Supervisor s·∫Ω ƒë·ªçc l·∫°i to√†n b·ªô l·ªãch s·ª≠, th·∫•y Agent kia ƒë√£ l√†m xong,
    # v√† t·ª± ƒë∆∞a ra quy·∫øt ƒë·ªãnh ti·∫øp theo (ho·∫∑c FINISH).
    
    return {"next_step": "Supervisor"}
# ============================================================================
# 4. ƒê·ªäNH NGHƒ®A NODE AGENTS
# ============================================================================
# ============================================================================
# NODE: SUPERVISOR (T·ªïng Gi√°m ƒë·ªëc ƒêi·ªÅu ph·ªëi - CEO AI)
# ============================================================================
async def get_smart_memory(messages):
    """
    CHI·∫æN THU·∫¨T "LAZY SUMMARY" (T√ìM T·∫ÆT THEO L√î):
    - Nguy√™n t·∫Øc: Ch·ªâ t√≥m t·∫Øt khi b·ªô nh·ªõ "tr√†n" (v∆∞·ª£t ng∆∞·ª°ng).
    - C·∫•u h√¨nh: 5 ƒê·∫ßu - 10 Cu·ªëi - Ng∆∞·ª°ng k√≠ch ho·∫°t 25.
    """
    # --- C·∫§U H√åNH C·ª¶A CEO ---
    HEAD_SIZE = 5       # Gi·ªØ 5 tin ƒë·∫ßu (System + ƒê·ªÅ b√†i g·ªëc)
    TAIL_SIZE = 10      # Gi·ªØ 10 tin cu·ªëi (H·ªôi tho·∫°i n√≥ng)
    THRESHOLD = 25      # Ch·ªâ k√≠ch ho·∫°t khi t·ªïng tin > 25
    
    total_msgs = len(messages)

    # 1. KI·ªÇM TRA NG∆Ø·ª†NG (QUAN TR·ªåNG NH·∫§T)
    # N·∫øu ch∆∞a ƒë·∫øn 25 c√¢u -> Tr·∫£ v·ªÅ ngay, KH√îNG G·ªåI API -> T·ªêN 0 ƒê·ªíNG
    if total_msgs <= THRESHOLD:
        # print(f"‚ö° [MEMORY] B·ªô nh·ªõ c√≤n nh·∫π ({total_msgs}/{THRESHOLD}). B·ªè qua n√©n.")
        return messages

    # 2. KHI V∆Ø·ª¢T NG∆Ø·ª†NG -> B·∫ÆT ƒê·∫¶U C·∫ÆT L·ªöP
    print(colored(f"üßπ [MEMORY] V∆∞·ª£t ng∆∞·ª°ng {THRESHOLD} tin. ƒêang k√≠ch ho·∫°t t√≥m t·∫Øt ƒëo·∫°n gi·ªØa...", "yellow"))
    
    head_msgs = messages[:HEAD_SIZE]
    tail_msgs = messages[-TAIL_SIZE:]
    
    # L·∫•y kh√∫c gi·ªØa ƒë·ªÉ n√©n (Bao g·ªìm c·∫£ tin nh·∫Øn t√≥m t·∫Øt c≈© n·∫øu c√≥)
    middle_msgs = messages[HEAD_SIZE:-TAIL_SIZE]
    
    # 3. G·ªåI DEEPSEEK ƒê·ªÇ G·ªòP N·ªòI DUNG (Ch·ªâ t·ªën ti·ªÅn ·ªü b∆∞·ªõc n√†y, nh∆∞ng r·∫•t √≠t)
    middle_text = "\n".join([f"{m.type}: {m.content}" for m in middle_msgs])
    
    summary_prompt = (
        "Nhi·ªám v·ª•: G·ªôp c√°c th√¥ng tin sau th√†nh 1 ƒëo·∫°n t√≥m t·∫Øt ng·∫Øn g·ªçn (d∆∞·ªõi 100 t·ª´).\n"
        "L∆∞u √Ω: N·∫øu c√≥ b·∫£n t√≥m t·∫Øt c≈©, h√£y g·ªôp n√≥ v√†o b·∫£n m·ªõi n√†y lu√¥n.\n"
        f"D·ªÆ LI·ªÜU C·∫¶N G·ªòP:\n{middle_text}"
    )
    
    try:
        # D√πng DeepSeek (R·∫ª)
        summary_res = await LLM_DEEPSEEK.ainvoke(summary_prompt)
        new_summary = summary_res.content.strip()
        
        # T·∫°o tin nh·∫Øn h·ªá th·ªëng ch·ª©a n·ªôi dung ƒë√£ g·ªôp
        summary_msg = SystemMessage(content=f"üìù [L·ªäCH S·ª¨ G·ªòP]: {new_summary}")
        
        # 4. TR·∫¢ V·ªÄ DANH S√ÅCH M·ªöI (ƒê√£ co l·∫°i c√≤n kho·∫£ng 16 tin)
        # L·∫ßn sau ch·∫°y, 'summary_msg' n√†y s·∫Ω n·∫±m trong ph·∫ßn middle v√† l·∫°i ƒë∆∞·ª£c g·ªôp ti·∫øp
        return head_msgs + [summary_msg] + tail_msgs

    except Exception as e:
        print(colored(f"‚ö†Ô∏è L·ªói t√≥m t·∫Øt: {e}. Gi·ªØ nguy√™n ƒë·ªÉ an to√†n.", "red"))
        return messages

# --- H√ÄM PH·ª§ TR·ª¢: PH√ÅT HI·ªÜN V√íNG L·∫∂P (ZOMBIE DETECTOR) ---
def check_zombie_loop(messages, threshold=3):
    """
    Ki·ªÉm tra xem h·ªá th·ªëng c√≥ ƒëang b·ªã k·∫πt ƒëƒ©a (l·∫∑p l·∫°i y h·ªát) kh√¥ng.
    Tr·∫£ v·ªÅ: True (ƒêang l·∫∑p - C·∫ßn d·ª´ng ngay) / False (ƒêang suy nghƒ© - Cho ch·∫°y ti·∫øp)
    """
    # L·∫•y 10 tin nh·∫Øn AI g·∫ßn nh·∫•t
    ai_msgs = [m.content for m in messages if isinstance(m, AIMessage)][-10:]
    
    if len(ai_msgs) < threshold: return False
    
    # Ki·ªÉm tra 3 tin nh·∫Øn AI g·∫ßn nh·∫•t c√≥ gi·ªëng h·ªát nhau kh√¥ng?
    # (D·∫•u hi·ªáu c·ªßa vi·ªác Supervisor c·ª© g·ªçi ƒëi g·ªçi l·∫°i 1 th·∫±ng m√† kh√¥ng c√≥ ti·∫øn tri·ªÉn)
    last_msg = ai_msgs[-1]
    repeats = 0
    for msg in reversed(ai_msgs[:-1]):
        if msg == last_msg:
            repeats += 1
        else:
            break # Ng·∫Øt n·∫øu g·∫∑p tin kh√°c
            
    if repeats >= threshold:
        return True # ƒê√£ l·∫∑p l·∫°i 3 l·∫ßn -> ZOMBIE LOOP
    return False

class SupervisorDecision(BaseModel):
    """C·∫•u tr√∫c quy·∫øt ƒë·ªãnh chu·∫©n c·ªßa Supervisor"""
    department: Literal["INTERNAL_OPS", "RESEARCH_LAB", "TECH_DEV", "CREATIVE_STUDIO", "PM_OFFICE", "CHAT"] = Field(
        ..., description="Ph√≤ng ban ch·ªãu tr√°ch nhi·ªám."
    )
    reason: str = Field(..., description="L√Ω do ƒëi·ªÅu ph·ªëi.")

async def supervisor_node(state):
    """
    SUPERVISOR V6: THE STRATEGIST (NH√Ä CHI·∫æN L∆Ø·ª¢C)
    Kh√¥ng ch·ªâ ph√¢n lo·∫°i, m√† c√≤n t∆∞ duy ƒë·ªÉ ch·ªçn gi·∫£i ph√°p t·ªëi ∆∞u nh·∫•t.
    """
    # 1. Thu th·∫≠p d·ªØ li·ªáu to√†n c·ª•c
    messages = state.get("messages", [])
    last_msg = messages[-1].content
    
    print(colored(f"\n[üß† SUPERVISOR] ƒêang ph√¢n t√≠ch chi·∫øn l∆∞·ª£c cho: '{last_msg[:50]}...'", "cyan", attrs=["bold"]))

    # 2. Ki·ªÉm tra an to√†n (Zombie Loop)
    if check_zombie_loop(messages):
        return {"messages": [AIMessage(content="‚ö†Ô∏è PH√ÅT HI·ªÜN V√íNG L·∫∂P: ƒê√£ d·ª´ng h·ªá th·ªëng ƒë·ªÉ b·∫£o v·ªá t√†i nguy√™n.")], "next_step": "FINISH"}

    # 3. K√çCH HO·∫†T T∆Ø DUY CHI·∫æN L∆Ø·ª¢C (Chain of Thought)
    # Thay v√¨ ch·ªçn 1 t·ª´ kh√≥a, AI s·∫Ω suy lu·∫≠n ƒë·ªÉ ch·ªçn ra "N∆∞·ªõc ƒëi ti·∫øp theo" t·ªët nh·∫•t
    strategy_prompt = """
    B·∫°n l√† T·ªïng Gi√°m ƒê·ªëc ƒêi·ªÅu H√†nh (COO) c·ªßa h·ªá th·ªëng AI.
    H√£y ph√¢n t√≠ch y√™u c·∫ßu c·ªßa CEO v√† ch·ªçn 1 trong c√°c PH√íNG BAN sau ƒë·ªÉ x·ª≠ l√Ω:

    1. [INTERNAL_OPS]: Khi CEO h·ªèi v·ªÅ: Ti·ªÅn nong, chi ph√≠, log ho·∫°t ƒë·ªông, tr·∫°ng th√°i server, ki·ªÉm tra h·ªá th·ªëng. (X·ª≠ l√Ω t·∫°i ch·ªó).
    2. [RESEARCH_LAB]: Khi CEO c·∫ßn th√¥ng tin m·ªõi, tin t·ª©c th·ªã tr∆∞·ªùng, gi√° c·∫£, ki·∫øn th·ª©c, h·ªçc thu·∫≠t, ho·∫∑c c√¢u ƒë·ªë/to√°n h·ªçc.
    3. [TECH_DEV]: Khi CEO mu·ªën vi·∫øt code, s·ª≠a l·ªói, build app, technical tasks.
    4. [CREATIVE_STUDIO]: Khi CEO mu·ªën v·∫Ω ·∫£nh, thi·∫øt k·∫ø, s√°ng t·∫°o ngh·ªá thu·∫≠t.
    5. [PM_OFFICE]: (D·ª± √°n ph·ª©c t·∫°p) Khi CEO y√™u c·∫ßu m·ªôt k·∫ø ho·∫°ch l·ªõn, m·ªôt chi·∫øn l∆∞·ª£c d√†i h·∫°n, ho·∫∑c m·ªôt quy tr√¨nh nhi·ªÅu b∆∞·ªõc (VD: "L·∫≠p k·∫ø ho·∫°ch kinh doanh", "X√¢y d·ª±ng d·ª± √°n A-Z").
    6. [CHAT]: Ch√†o h·ªèi x√£ giao ho·∫∑c kh√¥ng r√µ √Ω ƒë·ªãnh.

    Y√äU C·∫¶U: Tr·∫£ v·ªÅ ƒë·ªãnh d·∫°ng JSON duy nh·∫•t:
    {"department": "T√äN_PH√íNG_BAN", "reason": "L√Ω do ng·∫Øn g·ªçn"}
    """

    try:
        # D√πng DeepSeek/GPT ƒë·ªÉ t∆∞ duy
        llm = LLM_DEEPSEEK if LLM_DEEPSEEK else LLM_GPT4
        
        # K√≠ch ho·∫°t ch·∫ø ƒë·ªô Structured Output (√âp ki·ªÉu d·ªØ li·ªáu chu·∫©n 100%)
        structured_llm = llm.with_structured_output(SupervisorDecision)
        
        # G·ªçi AI (K·∫øt qu·∫£ tr·∫£ v·ªÅ l√† Object, kh√¥ng ph·∫£i String n·ªØa)
        decision = await structured_llm.ainvoke([
            SystemMessage(content=strategy_prompt), # L∆∞u √Ω: D√πng bi·∫øn system_prompt m·ªõi ƒë·ªãnh nghƒ©a
            HumanMessage(content=last_msg)
        ])

        # Truy xu·∫•t tr·ª±c ti·∫øp (An to√†n tuy·ªát ƒë·ªëi)
        dept = decision.department
        reason = decision.reason

        # 4. TH·ª∞C THI CHI·∫æN L∆Ø·ª¢C (ROUTING)

        # --- NH√ÅNH 1: N·ªòI B·ªò (X·ª≠ l√Ω ngay l·∫≠p t·ª©c) ---
        if dept == "INTERNAL_OPS":
            try:
                db_path = "/var/data/ai_corp_projects.db" if os.path.exists("/var/data") else "ai_corp_projects.db"
                conn = sqlite3.connect(db_path, timeout=10)
                cursor = conn.cursor()
                
                # T·ªïng h·ª£p s·ªë li·ªáu
                cursor.execute("SELECT SUM(cost) FROM work_logs")
                total_cost = cursor.fetchone()[0] or 0.0
                cursor.execute("SELECT count(*) FROM work_logs")
                total_tasks = cursor.fetchone()[0] or 0
                cursor.execute("SELECT agent_name, task_content FROM work_logs ORDER BY id DESC LIMIT 1")
                last_task = cursor.fetchone()
                conn.close()
                
                report = (
                    f"üìä **B√ÅO C√ÅO V·∫¨N H√ÄNH (LIVE)**\n"
                    f"- **T·ªïng chi ph√≠**: ${total_cost:.4f}\n"
                    f"- **T·ªïng t√°c v·ª•**: {total_tasks}\n"
                    f"- **G·∫ßn nh·∫•t**: {last_task[0] if last_task else 'N/A'} v·ª´a l√†m: *{last_task[1] if last_task else '...' }*"
                )
                return {"messages": [AIMessage(content=report)], "next_step": "FINISH"}
            except Exception as e:
                return {"messages": [AIMessage(content=f"‚ö†Ô∏è L·ªói truy xu·∫•t d·ªØ li·ªáu n·ªôi b·ªô: {e}")], "next_step": "FINISH"}

        # --- NH√ÅNH 2: D·ª∞ √ÅN L·ªöN (Chuy·ªÉn cho Orchestrator/Strategy) ---
        elif dept == "PM_OFFICE":
            # N·∫øu c√≥ Orchestrator Node th√¨ chuy·ªÉn qua, n·∫øu kh√¥ng th√¨ chuy·ªÉn Strategy
            return {"next_step": "Orchestrator", "messages": []} # Ho·∫∑c "Strategy_R_and_D"

        # --- NH√ÅNH 3: CHUY√äN M√îN ---
        elif dept == "TECH_DEV":
            return {"next_step": "Coder", "messages": []}
        
        elif dept == "CREATIVE_STUDIO":
            return {"next_step": "Artist", "messages": []}

        # --- NH√ÅNH 4: NGHI√äN C·ª®U & M·∫∂C ƒê·ªäNH ---
        else: # RESEARCH_LAB ho·∫∑c CHAT
            return {"next_step": "Researcher", "messages": []}

    except Exception as e:
        print(colored(f"‚ö†Ô∏è Supervisor Fallback: {e}", "red"))
        # N·∫øu b·ªô n√£o b·ªã l·ªói, m·∫∑c ƒë·ªãnh chuy·ªÉn Researcher ƒë·ªÉ t√¨m c√¢u tr·∫£ l·ªùi
        return {"next_step": "Researcher", "messages": []}
#  ---- Vi·∫øt Code----
async def coder_node(state): # Chuy·ªÉn sang async ƒë·ªÉ ch·∫°y song song
    """
    Claude Coder Node - Parallel Execution & AST Validation
    """
    print(colored("[üöÄ CODER V2] Parallel Ensemble Mode ACTIVATED", "green", attrs=["bold"]))
    
    # 1. SETUP CONTEXT
    errors = state.get("error_log", [])
    task_type = state.get("task_type", "general").lower()
    messages = state.get('messages', [])
    last_user_msg = messages[-1].content
    
    # An to√†n: T√¨m ki·∫øm k√Ω ·ª©c (Tr√°nh l·ªói n·∫øu h√†m search_memory ch∆∞a s·∫µn s√†ng)
    try:
        memory_context = search_memory("Ti√™u chu·∫©n vi·∫øt code Clean Code, SOLID")
    except:
        memory_context = "Tu√¢n th·ªß PEP8, Clean Code v√† th√™m comment gi·∫£i th√≠ch."
    # error_context = self_heal_analyzer(errors)
    
    # 2. PROMPT STRATEGY (Smart Selection)
    base_prompt = get_claude_perfected_prompt(task_type, memory_context, str(errors), last_user_msg)
    # Ch·ªâ ch·∫°y Ensemble n·∫øu task kh√≥ ho·∫∑c ƒëang fix l·ªói
    use_ensemble = len(errors) > 0 or "complex" in task_type or "d·ª± √°n" in last_user_msg.lower()
    prompts = [base_prompt]
    if use_ensemble:
        # Th√™m 1 bi·∫øn th·ªÉ t·ªëi ∆∞u h√≥a ƒë·ªÉ so s√°nh
        prompts.append(base_prompt + "\n[DIRECTIVE]: OPTIMIZE for performance and brevity. Remove unnecessary comments.")
    # 3. PARALLEL EXECUTION (TƒÉng t·ªëc ƒë·ªô g·∫•p 3 l·∫ßn)
    # ============================================================================
    print(colored(f"‚ö° Running {len(prompts)} parallel chains...", "cyan"))
    # Chu·∫©n b·ªã batch inputs
    batch_inputs = [[SystemMessage(content=p)] + messages for p in prompts]
    
    try:
        # --- LOGIC FALLBACK QUAN TR·ªåNG ---
        # ∆Øu ti√™n 1: CODER_PRIMARY (DeepSeek)
        # ∆Øu ti√™n 2: LLM_GPT4 (GPT-4 Turbo)
        # ∆Øu ti√™n 3: LLM_CLAUDE (Claude 3.5 Sonnet)
        
        fallbacks = []
        if LLM_GPT4: fallbacks.append(LLM_GPT4)
        if LLM_CLAUDE: fallbacks.append(LLM_CLAUDE)
        
        # X√°c ƒë·ªãnh Primary Chain
        primary_chain = CODER_PRIMARY if CODER_PRIMARY else (LLM_GPT4 if LLM_GPT4 else LLM_CLAUDE)
        
        if not primary_chain:
            raise Exception("CRITICAL: Kh√¥ng c√≥ API n√†o ho·∫°t ƒë·ªông!")

        # K√≠ch ho·∫°t Fallback
        if fallbacks and primary_chain != fallbacks[0]: 
            final_chain = primary_chain.with_fallbacks(fallbacks)
            print(colored(f"üõ°Ô∏è Chain: {primary_chain.model_name} -> Fallbacks", "green"))
        else:
            final_chain = primary_chain

        # Th·ª±c thi
        responses = await final_chain.abatch(batch_inputs)
        
    except Exception as e:
        # Ghi log l·ªói chi ti·∫øt tr∆∞·ªõc khi fallback ƒë·ªÉ CEO bi·∫øt t·∫°i sao s·∫≠p
        error_detail = f"L·ªói th·ª±c thi song song (Parallel Batch): {str(e)}"
        print(colored(f"üö® {error_detail}", "red"))
        
        # C·∫≠p nh·∫≠t error_log v√†o state tr∆∞·ªõc khi tho√°t
        state["error_log"] = state.get("error_log", []) + [error_detail]
        
        return {"messages": [AIMessage(content="H·ªá th·ªëng qu√° t·∫£i.")], "next_step": "FINISH"}

    # 4. VALIDATION & SCORING
    # ============================================================================
    valid_results = []
    for i, res in enumerate(responses):
        code = extract_code_block(res.content)
        if not code: continue
        
        is_ok, msg = real_syntax_validator(code, "python")
        score = 50 if is_ok else 0
        if len(code) > 30: score += 10
        if "# filename:" in code: score += 10
        
        valid_results.append({"code": code, "reply": res.content, "score": score, "error": msg, "variant": i})

    # 5. SELECT BEST CANDIDATE
    # ============================================================================
    if valid_results:
        # L·∫•y ·ª©ng vi√™n c√≥ ƒëi·ªÉm cao nh·∫•t
        best_result = max(valid_results, key=lambda x: x['score'])
        
        # NG∆Ø·ª†NG CH·∫§P NH·∫¨N: 60 ƒëi·ªÉm (ƒê·ªß ƒë·ªÉ ch·∫°y)
        # (T√¥i h·∫° xu·ªëng 60 ƒë·ªÉ h·ªá th·ªëng linh ho·∫°t h∆°n, nh∆∞ng ch·ªâ l∆∞u b√†i m·∫´u khi ƒë·∫°t 80)
        if best_result['score'] >= 60: 
            print(colored(f"‚úÖ SELECTED Variant {best_result['variant']} (Score: {best_result['score']})", "green"))
            
            # [T·ª∞ H·ªåC]: Ch·ªâ l∆∞u nh·ªØng ƒëo·∫°n code ch·∫•t l∆∞·ª£ng cao (>= 80)
            if best_result['score'] >= 80:
                try:
                    # D√πng h√†m log chu·∫©n m·ªõi: log_training_data
                    # Input: User Prompt, Code AI, Score, T√™n Model
                    log_training_data(
                        user_prompt=messages[-1].content,
                        best_code=best_result['code'],
                        score=best_result['score'],
                        model_used="3-Tier-Squad" 
                    )
                except: pass    
                # except Exception as e:
                #     # N·∫øu l·ªói ghi file th√¨ b·ªè qua, kh√¥ng l√†m s·∫≠p lu·ªìng ch√≠nh
                #     print(colored(f"‚ö†Ô∏è Log Error: {e}", "yellow"))

            # TR·∫¢ V·ªÄ K·∫æT QU·∫¢ TH√ÄNH C√îNG
            return {
                "messages": [AIMessage(content=best_result['full_reply'])],
                "next_node": "Tester", # Chuy·ªÉn sang Tester ki·ªÉm tra
                "error_log": []        # X√≥a s·∫°ch l·ªói c≈© v√¨ ƒë√£ th√†nh c√¥ng
            }
        
        else:
            # TR∆Ø·ªúNG H·ª¢P: Code ƒëi·ªÉm th·∫•p ho·∫∑c l·ªói c√∫ ph√°p
            print(colored(f"‚ö†Ô∏è [CODER] Variant t·ªët nh·∫•t ch·ªâ ƒë·∫°t {best_result['score']}/100. Error: {best_result['error']}", "yellow"))
            
            # 1. Ki·ªÉm tra gi·ªõi h·∫°n th·ª≠ l·∫°i (Max 3 l·∫ßn ƒë·ªÉ tr√°nh l·∫∑p v√¥ t·∫≠n)
            if len(state.get("error_log", [])) >= 3:
                print(colored("üö® [CODER] ƒê√£ th·ª≠ 3 l·∫ßn kh√¥ng ƒë∆∞·ª£c. Chuy·ªÉn sang Fallback.", "red"))
                state["error_log"].append("L·ªói: AI kh√¥ng th·ªÉ t·ª± s·ª≠a code sau 3 l·∫ßn th·ª≠.")
                
                # G·ªçi h√†m fallback cu·ªëi c√πng (Code th·ªß c√¥ng ho·∫∑c b√°o l·ªói)
                return ultimate_fallback(state, messages)

            # 2. T·∫°o ph·∫£n h·ªìi l·ªói chi ti·∫øt ƒë·ªÉ AI t·ª± s·ª≠a
            error_feedback = (
                f"SYSTEM ALERT: Code b·∫°n vi·∫øt b·ªã l·ªói c√∫ ph√°p ho·∫∑c vi ph·∫°m quy chu·∫©n.\n"
                f"- Error Details: {best_result['error']}\n"
                f"- Score: {best_result['score']}/100\n"
                f"ACTION: H√£y vi·∫øt l·∫°i code m·ªõi, s·ª≠a tri·ªát ƒë·ªÉ l·ªói tr√™n."
            )
            
            # Tr·∫£ v·ªÅ state ƒë·ªÉ k√≠ch ho·∫°t v√≤ng l·∫∑p quay l·∫°i Coder
            return {
                "messages": [
                    AIMessage(content=best_result['code']), # G·ª≠i l·∫°i code sai
                    HumanMessage(content=error_feedback)    # K√®m l·ªùi nh·∫Øc s·ª≠a
                ], 
                "error_log": state.get("error_log", []) + [f"Syntax Error: {best_result.get('error')}"],
                "next_step": "Coder" # Ch·ªâ ƒë·ªãnh r√µ b∆∞·ªõc ti·∫øp theo l√† quay l·∫°i Coder
            }

    # TR∆Ø·ªúNG H·ª¢P: Kh√¥ng c√≥ variant n√†o (L·ªói API ho·∫∑c Prompt b·ªã ch·∫∑n)
    error_msg = "Kh√¥ng c√≥ k·∫øt qu·∫£ n√†o ƒë∆∞·ª£c t·∫°o ra t·ª´ batch execution."
    print(colored(f"‚ùå [CODER] {error_msg}", "red"))
    state["error_log"] = state.get("error_log", []) + [error_msg]
    
    return ultimate_fallback(state, messages)

# ============================================================================
# NODE: TESTER (K·ªπ s∆∞ Ki·ªÉm ƒë·ªãnh Ch·∫•t l∆∞·ª£ng - QA/QC)
# ============================================================================
def tester_node(state):
    """
    Agent Tester: Ki·ªÉm ƒë·ªãnh c√∫ ph√°p ƒëa ng√¥n ng·ªØ, qu√©t l·ªói b·∫£o m·∫≠t v√† tu√¢n th·ªß quy chu·∫©n.
    """
    print(colored("[üß™ TESTER] ƒêang ki·ªÉm ƒë·ªãnh ch·∫•t l∆∞·ª£ng m√£ ngu·ªìn...", "yellow", attrs=["bold"]))
    
    messages = state.get("messages", [])
    last_ai_msg = messages[-1].content
    
    # 1. Tr√≠ch xu·∫•t code block
    code_to_test = extract_code_block(last_ai_msg)
    
    if not code_to_test:
        print(colored("‚ùå [TESTER] Kh√¥ng t√¨m th·∫•y kh·ªëi code h·ª£p l·ªá!", "red"))
        return {
            "error_log": state.get("error_log", []) + ["L·ªñI: Kh√¥ng t√¨m th·∫•y kh·ªëi code ```."],
            "next_step": "Coder"
        }

    is_valid = True
    feedback = []

    # 2. KI·ªÇM ƒê·ªäNH THEO NG√îN NG·ªÆ
    
    # --- Tr∆∞·ªùng h·ª£p 1: Code Python ---
    if "def " in code_to_test or "import " in code_to_test:
        try:
            ast.parse(code_to_test)
            feedback.append("- C√∫ ph√°p Python: ƒê·∫°t chu·∫©n.")
            
            # Ki·ªÉm tra b·∫£o m·∫≠t s∆° b·ªô (V√≠ d·ª•: c·∫•m d√πng 'eval')
            if "eval(" in code_to_test or "os.system(" in code_to_test:
                is_valid = False
                feedback.append("- B·∫£o m·∫≠t: Ph√°t hi·ªán h√†m nguy hi·ªÉm (eval/system).")
                
        except SyntaxError as e:
            is_valid = False
            feedback.append(f"- C√∫ ph√°p Python: L·ªói t·∫°i d√≤ng {e.lineno}: {e.msg}")

    # --- Tr∆∞·ªùng h·ª£p 2: Code C++ / Arduino (Hardware) ---
    elif "#include" in code_to_test or "void setup()" in code_to_test:
        # Ki·ªÉm tra ƒë√≥ng m·ªü ngo·∫∑c ƒë∆°n gi·∫£n cho C++ (V√¨ Python kh√¥ng parse ƒë∆∞·ª£c C++)
        open_braces = code_to_test.count("{")
        close_braces = code_to_test.count("}")
        if open_braces != close_braces:
            is_valid = False
            feedback.append(f"- C√∫ ph√°p C++: M·∫•t c√¢n b·∫±ng d·∫•u ngo·∫∑c ({open_braces} m·ªü, {close_braces} ƒë√≥ng).")
        else:
            feedback.append("- C√∫ ph√°p C++: Ki·ªÉm tra c·∫•u tr√∫c ƒë√≥ng/m·ªü ƒë·∫°t.")

    # 3. QUY·∫æT ƒê·ªäNH H·∫¨U KI·ªÇM
    full_feedback = "\n".join(feedback)
    
    if is_valid:
        print(colored("‚úÖ [TESTER] M√£ ngu·ªìn ƒë·∫°t ti√™u chu·∫©n ch·∫•t l∆∞·ª£ng.", "green"))
        return {
            "error_log": [], # Clear log l·ªói
            "next_step": "Supervisor"
        }
    else:
        print(colored(f"‚ùå [TESTER] Ph√°t hi·ªán vi ph·∫°m:\n{full_feedback}", "red"))
        error_msg = HumanMessage(content=(
            f"‚ö†Ô∏è B√ÅO C√ÅO KI·ªÇM ƒê·ªäNH TH·∫§T B·∫†I:\n{full_feedback}\n\n"
            f"Vui l√≤ng s·ª≠a l·∫°i m√£ ngu·ªìn, ch√∫ tr·ªçng v√†o c√°c ƒëi·ªÉm vi ph·∫°m tr√™n."
        ))
        return {
            "messages": [error_msg],
            "error_log": state.get("error_log", []) + [full_feedback],
            "next_step": "Coder"
        }
    
# ============================================================================
# NODE: HARDWARE (Ki·∫øn tr√∫c s∆∞ Robotics & H·ªá th·ªëng nh√∫ng)
# ============================================================================
def hardware_node(state):
    """
    Agent Hardware Architect: Chuy√™n tr√°ch ESP32, Robotics v√† H·ªá th·ªëng nh√∫ng.
    N√¢ng c·∫•p: Tr√≠ch xu·∫•t BOM chu·∫©n cho Procurement v√† t·ªëi ∆∞u h√≥a PINOUT.
    """
    print(colored("[üõ†Ô∏è HARDWARE] ƒêang ki·∫øn tr√∫c h·ªá th·ªëng nh√∫ng...", "cyan", attrs=["bold"]))
    
    messages = state.get("messages", [])
    last_msg = messages[-1].content
    is_pure_hw = "[HARDWARE]" in last_msg # Nh·∫≠n di·ªán Tab K·ªπ thu·∫≠t

    prompt = (
        "B·∫°n l√† Hardware Architect cao c·∫•p t·∫°i AI Corporation. "
        f"\nY√äU C·∫¶U: {last_msg}"
        "\n\nC·∫§U TR√öC B√ÅO C√ÅO K·ª∏ THU·∫¨T:"
        "\n1. [DANH M·ª§C LINH KI·ªÜN - BOM]: Li·ªát k√™ d·∫°ng b·∫£ng: T√™n | Th√¥ng s·ªë | S·ªë l∆∞·ª£ng."
        "\n2. [S∆† ƒê·ªí CH√ÇN - PINOUT]: B·∫£ng k·∫øt n·ªëi chi ti·∫øt (VD: ESP32 GPIO21 -> LCD SDA)."
        "\n3. [FIRMWARE]: Code C++/Arduino t·ªëi ∆∞u, c√≥ comment gi·∫£i th√≠ch chuy√™n s√¢u."
        "\n4. [L∆ØU √ù V·∫¨N H√ÄNH]: C·∫£nh b√°o d√≤ng √°p, t·∫£n nhi·ªát v√† nhi·ªÖu t√≠n hi·ªáu."
        "\n\nB·∫ÆT BU·ªòC: Kh√¥ng d√πng emoji, ch·ªâ d√πng k√Ω t·ª± Latin/Ti·∫øng Vi·ªát chu·∫©n."
    )
    
    try:
        # GPT-4o l√† l·ª±a ch·ªçn s·ªë 1 cho vi·ªác tra c·ª©u s∆° ƒë·ªì ch√¢n (Data Sheets)
        response = LLM_GPT4.invoke([
            SystemMessage(content=prompt),
            HumanMessage(content=last_msg)
        ])
        
        # ƒê·ªäNH TUY·∫æN:
        # N·∫øu ·ªü Tab Hardware -> FINISH (Hi·ªán k·∫øt qu·∫£ ngay)
        # N·∫øu ·ªü lu·ªìng t·ª± ƒë·ªông -> Chuy·ªÉn sang Procurement ƒë·ªÉ b√°o gi√° linh ki·ªán
        next_destination = "FINISH" if is_pure_hw else "Procurement"

        return {
            "messages": [AIMessage(content=f"üõ†Ô∏è **[THI·∫æT K·∫æ K·ª∏ THU·∫¨T PH·∫¶N C·ª®NG]**\n\n{response.content}")],
            "next_step": next_destination
        }
        
    except Exception as e:
        # 1. Ghi log chi ti·∫øt ra Terminal ƒë·ªÉ CEO theo d√µi l·ªói v·∫≠t l√Ω
        error_detail = str(e)
        print(colored(f"üö® [HARDWARE ERROR]: {error_detail}", "red", attrs=["bold"]))
        
        # 2. Tr·∫£ v·ªÅ State chu·∫©n: 
        # - messages: Ph·∫£i l√† m·ªôt LIST ch·ª©a ƒë·ªëi t∆∞·ª£ng Message
        # - next_step: Ph·∫£i l√† m·ªôt CHU·ªñI (String) ƒë·ªãnh danh Node ti·∫øp theo
        return {
            "messages": [AIMessage(content=f"‚ùå **H·ªÜ TH·ªêNG C·∫¢NH B√ÅO HARDWARE**:\n\nƒê√£ x·∫£y ra s·ª± c·ªë k·ªπ thu·∫≠t: `{error_detail}`")], 
            "next_step": "FINISH" 
        }
#  ---- V·∫Ω 3D Plotly----
def engineering_node(state):
    """
    Agent CTO/Engineer: Thi·∫øt k·∫ø m√¥ h√¨nh 3D b·∫±ng Python Plotly.
    ƒê√£ n√¢ng c·∫•p: ƒê·∫£m b·∫£o m√£ ngu·ªìn chu·∫©n ƒë·ªÉ Dashboard th·ª±c thi v·∫Ω 3D.
    """
    print(colored("[‚öôÔ∏è ENGINEERING] ƒêang thi·∫øt k·∫ø c·∫•u tr√∫c 3D...", "blue", attrs=["bold"]))
    
    messages = state.get("messages", [])
    last_msg = messages[-1].content
    is_pure_eng = "[ENGINEERING]" in last_msg

    # 1. Prompt √©p AI vi·∫øt code s·∫°ch, kh√¥ng gi·∫£i th√≠ch th·ª´a
    prompt = (
        "B·∫°n l√† K·ªπ s∆∞ Thi·∫øt k·∫ø 3D chuy√™n nghi·ªáp. "
        "\nNHI·ªÜM V·ª§: Vi·∫øt code Python s·ª≠ d·ª•ng plotly.graph_objects ƒë·ªÉ t·∫°o m√¥ h√¨nh 3D."
        "\n\nY√äU C·∫¶U K·ª∏ THU·∫¨T:"
        "\n- Ch·ªâ tr·∫£ v·ªÅ duy nh·∫•t CODE BLOCK Python trong d·∫•u ```python."
        "\n- Code ph·∫£i t·∫°o ra ƒë·ªëi t∆∞·ª£ng t√™n l√† 'fig'."
        "\n- Ph·∫£i bao g·ªìm d·ªØ li·ªáu t·ªça ƒë·ªô (x, y, z) chi ti·∫øt cho m√¥ h√¨nh."
        "\n- N·∫øu l√† Robot, h√£y v·∫Ω r√µ c√°c kh·ªõp n·ªëi v√† c√°nh tay."
        "\n- KH√îNG gi·∫£i th√≠ch, KH√îNG nh·∫≠p vƒÉn b·∫£n ngo√†i code."
    )

    try:
        # 2. S·ª≠ d·ª•ng Claude 3.5 Sonnet (ƒê·ªânh cao v·ªÅ vi·∫øt code h√¨nh h·ªçc)
        response = LLM_CLAUDE.invoke([
            SystemMessage(content=prompt),
            HumanMessage(content=f"Y√™u c·∫ßu thi·∫øt k·∫ø: {last_msg}")
        ])
        
        # 3. ƒê·ªãnh tuy·∫øn
        next_destination = "FINISH" if is_pure_eng else "Procurement"

        return {
            "messages": [AIMessage(content=f"‚öôÔ∏è **[B·∫¢N THI·∫æT K·∫æ 3D H·ªÜ TH·ªêNG]**\n\n{response.content}")],
            "next_step": next_destination
        }
        
    except Exception as e:
        # 1. Ghi log l·ªói chi ti·∫øt ra Terminal v·ªõi m√†u ƒë·ªè ƒë·∫≠m ƒë·ªÉ d·ªÖ nh·∫≠n di·ªán
        error_detail = str(e)
        print(colored(f"üö® [ENGINEERING ERROR]: {error_detail}", "red", attrs=["bold"]))
        
        # 2. Tr·∫£ v·ªÅ State chu·∫©n cho LangGraph:
        # - messages: B·∫ÆT BU·ªòC l√† m·ªôt list ch·ª©a ƒë·ªëi t∆∞·ª£ng Message (kh√¥ng ƒë∆∞·ª£c g·ª≠i dict r·ªóng)
        # - next_step: B·∫ÆT BU·ªòC l√† m·ªôt chu·ªói (String) ƒë·ªÉ tr√°nh l·ªói bƒÉm d·ªØ li·ªáu
        return {
            "messages": [AIMessage(content=f"‚ùå **L·ªñI THI·∫æT K·∫æ K·ª∏ THU·∫¨T**:\n\nH·ªá th·ªëng g·∫∑p s·ª± c·ªë khi d·ª±ng m√¥ h√¨nh: `{error_detail}`")], 
            "next_step": "FINISH" 
        }
    
def publisher_node(state):
    """
    Agent Publisher: T·ªïng h·ª£p d·ªØ li·ªáu t·ª´ t·∫•t c·∫£ c√°c Agent ƒë·ªÉ xu·∫•t b·∫£n h·ªì s∆° d·ª± √°n.
    """
    print(colored("[üìú PUBLISHER] ƒêang t·ªïng h·ª£p h·ªì s∆° d·ª± √°n cu·ªëi c√πng...", "green", attrs=["bold"]))
    
    messages = state.get("messages", [])
    
    # 1. PH√ÇN LO·∫†I D·ªÆ LI·ªÜU T·ª∞ ƒê·ªòNG
    research_report = ""
    investment_plan = ""
    technical_specs = ""
    creative_content = ""
    images = []

    for msg in messages:
        content = msg.content
        if "üîç [B√ÅO C√ÅO NGHI√äN C·ª®U]" in content: research_report = content
        if "üí∞ [H·ªí S∆† TH·∫®M ƒê·ªäNH ƒê·∫¶U T∆Ø]" in content: investment_plan = content
        if "‚öôÔ∏è [B·∫¢N THI·∫æT K·∫æ 3D]" in content: technical_specs = content
        if "üñãÔ∏è [T√ÅC PH·∫®M S√ÅNG T√ÅC]" in content: creative_content = content
        if "![·∫¢nh minh h·ªça]" in content:
            # Tr√≠ch xu·∫•t URL ·∫£nh
            urls = [line for line in content.split('\n') if "https://" in line]
            images.extend(urls)

    # 2. T·ªîNG H·ª¢P PROMPT XU·∫§T B·∫¢N
    publish_prompt = (
        "B·∫°n l√† Chuy√™n gia tr√¨nh b√†y vƒÉn b·∫£n c·∫•p cao. H√£y t·ªïng h·ª£p c√°c d·ªØ li·ªáu tr√™n th√†nh m·ªôt "
        "B√°o c√°o D·ª± √°n ho√†n ch·ªânh, chuy√™n nghi·ªáp. S·ª≠ d·ª•ng ti√™u ƒë·ªÅ, m·ª•c l·ª•c v√† ƒë·ªãnh d·∫°ng Markdown chu·∫©n."
        "\nTh·ª© t·ª±: 1. T·ªïng quan -> 2. Th·ªã tr∆∞·ªùng -> 3. T√†i ch√≠nh -> 4. K·ªπ thu·∫≠t -> 5. Ph·ª• l·ª•c h√¨nh ·∫£nh."
    )

    response = LLM_GEMINI_LOGIC.invoke([
        SystemMessage(content=publish_prompt),
        HumanMessage(content=f"D·ªØ li·ªáu gom ƒë∆∞·ª£c:\n{research_report}\n{investment_plan}\n{technical_specs}\n{creative_content}")
    ])

    return {
        "messages": [AIMessage(content=f"üìú **[H·ªí S∆† D·ª∞ √ÅN T·ªîNG TH·ªÇ - FINAL]**\n\n{response.content}")],
        "next_step": "FINISH"
    }
# ============================================================================
# NODE: IoT ENGINEER (K·ªπ s∆∞ V·∫≠n h√†nh & K·∫øt n·ªëi thi·∫øt b·ªã)
# ============================================================================
def iot_node(state):
    """
    Agent IoT: K·∫øt h·ª£p L·∫≠p tr√¨nh Firmware (Thi·∫øt k·∫ø) v√† Th·ª±c thi l·ªánh (V·∫≠n h√†nh).
    """
    print(colored("[ü§ñ IoT ENGINEER] ƒêang x·ª≠ l√Ω giao th·ª©c v√† thi·∫øt b·ªã...", "magenta", attrs=["bold"]))
    
    messages = state.get("messages", [])
    last_msg = messages[-1].content
    is_pure_iot = "[IOT]" in last_msg

    # 1. KI·ªÇM TRA NG·ªÆ C·∫¢NH: ƒê√¢y l√† l·ªánh ƒëi·ªÅu khi·ªÉn (V·∫≠n h√†nh) hay y√™u c·∫ßu vi·∫øt code (Thi·∫øt k·∫ø)?
    is_command = any(word in last_msg.upper() for word in ["B·∫¨T", "T·∫ÆT", "TURN", "CONTROL", "CH·∫†Y"])

    if is_command:
        # --- NH√ÅNH 1: V·∫¨N H√ÄNH THI·∫æT B·ªä TH·∫¨T ---
        analysis_prompt = f"Tr√≠ch xu·∫•t l·ªánh ƒëi·ªÅu khi·ªÉn t·ª´: '{last_msg}'. Ch·ªâ tr·∫£ v·ªÅ m√£ l·ªánh Uppercase."
        command_code = LLM_GPT4.invoke([SystemMessage(content=analysis_prompt)]).content.strip()
        
        try:
            # 1. G·ªçi tool hardware_controller ƒë·ªÉ ra l·ªánh cho thi·∫øt b·ªã th·ª±c t·∫ø
            hardware_response = hardware_controller.invoke(command_code)
            report = (f"üì° **[K·∫æT QU·∫¢ V·∫¨N H√ÄNH]**\n\n- M√£ l·ªánh: `{command_code}`\n- Tr·∫°ng th√°i: {hardware_response}")
            
            # N·∫øu ch·∫°y Tab IOT ri√™ng bi·ªát -> K·∫øt th√∫c. N·∫øu ch·∫°y lu·ªìng t·ª± ƒë·ªông -> V·ªÅ Supervisor b√°o c√°o.
            return {
                "messages": [AIMessage(content=report)], 
                "next_step": "FINISH" if is_pure_iot else "Supervisor"
            }
            
        except Exception as e:
            # 2. X·ª≠ l√Ω l·ªói k·∫øt n·ªëi ho·∫∑c th·ª±c thi thi·∫øt b·ªã
            error_detail = str(e)
            print(colored(f"üö® [IOT HARDWARE ERROR]: {error_detail}", "red", attrs=["bold"]))
            
            # Tr·∫£ v·ªÅ AIMessage chu·∫©n ƒë·ªÉ Dashboard hi·ªÉn th·ªã ƒë√∫ng ID: IoT_Engineer
            return {
                "messages": [AIMessage(content=f"‚ùå **L·ªñI K·∫æT N·ªêI THI·∫æT B·ªä**:\n\nKh√¥ng th·ªÉ th·ª±c thi l·ªánh `{command_code}`. \nChi ti·∫øt: `{error_detail}`")], 
                "next_step": "Supervisor" # Quay v·ªÅ ƒë·ªÉ Supervisor ra l·ªánh ki·ªÉm tra l·∫°i ho·∫∑c ƒë·ªïi ph∆∞∆°ng √°n
            }
    else:
        # --- NH√ÅNH 2: THI·∫æT K·∫æ FIRMWARE (D√†nh cho d·ª± √°n m·ªõi) ---
        # L·∫•y b·∫£n v·∫Ω Pinout t·ª´ Hardware Node n·∫øu c√≥
        hw_context = next((m.content for m in reversed(messages) if "üõ†Ô∏è" in m.content), "Ch∆∞a c√≥ s∆° ƒë·ªì ch√¢n.")
        
        design_prompt = (
            "B·∫°n l√† K·ªπ s∆∞ Firmware IoT. H√£y vi·∫øt code C++/Arduino ƒëi·ªÅu khi·ªÉn h·ªá th·ªëng d·ª±a tr√™n s∆° ƒë·ªì ch√¢n sau."
            f"\nS∆° ƒë·ªì: {hw_context}"
            "\nY√™u c·∫ßu: Vi·∫øt code c√≥ k·∫øt n·ªëi WiFi/MQTT v√† qu·∫£n l√Ω l·ªói k·∫øt n·ªëi."
        )
        
        response = LLM_CLAUDE.invoke([SystemMessage(content=design_prompt), HumanMessage(content=last_msg)])
        
        return {
            "messages": [AIMessage(content=f"üì° **[FIRMWARE & GIAO TH·ª®C ƒêI·ªÄU KHI·ªÇN]**\n\n{response.content}")],
            "next_step": "FINISH" if is_pure_iot else "Supervisor"
        }
# ============================================================================
# NODE: PROCUREMENT (Tr∆∞·ªüng ph√≤ng Thu mua & Qu·∫£n l√Ω Chu·ªói cung ·ª©ng)
# ============================================================================
BUYER_PROFILE = {
    "address": "Phan Thi·∫øt, B√¨nh Thu·∫≠n, Vi·ªát Nam",
    "delivery_method": "Fast Shipping",
    "accounts": ["Shopee_API_Key", "Taobao_Token", "Mouser_ID"]
}
def procurement_node(state):
    """
    Agent Procurement: T·ªëi ∆∞u h√≥a chu·ªói cung ·ª©ng d·ª±a tr√™n v·ªã tr√≠ th·ª±c t·∫ø c·ªßa CEO.
    """
    print(colored("[üõí PROCUREMENT] ƒêang t·ªëi ∆∞u h√≥a l·ªô tr√¨nh h√†ng h√≥a v·ªÅ Phan Thi·∫øt...", "yellow", attrs=["bold"]))
    
    # 1. Load h·ªì s∆° mua h√†ng (Mockup)
    buyer_config = BUYER_PROFILE # L·∫•y t·ª´ file c·∫•u h√¨nh tr√™n
    
    messages = state.get("messages", [])
    hw_report = next((m.content for m in reversed(messages) if "üõ†Ô∏è" in m.content), "Kh√¥ng t√¨m th·∫•y danh m·ª•c linh ki·ªán.")

    # 2. X√¢y d·ª±ng l·ªánh truy v·∫•n chuy√™n s√¢u
    prompt = (
        "B·∫°n l√† Chuy√™n gia Logisitics v√† Thu mua."
        f"\nƒê·ªäA CH·ªà NH·∫¨N: {buyer_config['address']}"
        f"\nDANH M·ª§C: {hw_report}"
        "\n\nNHI·ªÜM V·ª§:"
        "\n1. T√åM GI√Å: Tra c·ª©u gi√° th·ª±c t·∫ø nƒÉm 2026 tr√™n Mouser, Digikey v√† Shopee."
        "\n2. T√çNH PH√ç V·∫¨N CHUY·ªÇN: ∆Ø·ªõc t√≠nh ph√≠ ship v√† thu·∫ø nh·∫≠p kh·∫©u v·ªÅ Vi·ªát Nam."
        "\n3. L·∫¨P GI·ªé H√ÄNG: T·∫°o danh s√°ch link s·∫£n ph·∫©m s·∫µn s√†ng ƒë·ªÉ thanh to√°n."
    )

    # S·ª≠ d·ª•ng Perplexity ƒë·ªÉ check gi√° th·ª±c t·∫ø
    response = LLM_PERPLEXITY.invoke([SystemMessage(content=prompt)])

    return {
        "messages": [AIMessage(content=f"üõí **[PHI·∫æU ƒê·ªÄ XU·∫§T MUA S·∫ÆM & V·∫¨N CHUY·ªÇN]**\n\n{response.content}")],
        "next_step": "Investment" # Chuy·ªÉn sang T√†i ch√≠nh ƒë·ªÉ CEO duy·ªát chi
    }
# ============================================================================
# NODE: RESEARCHER (Chuy√™n gia Ph√¢n t√≠ch Th·ªã tr∆∞·ªùng & ƒê·ªëi th·ªß)
# ============================================================================
def researcher_node(state):
    """
    Agent Researcher: Chuy√™n gia ph√¢n t√≠ch th·ªã tr∆∞·ªùng 2026.
    N√¢ng c·∫•p: T·ª± ƒë·ªông nh·∫≠n di·ªán Tag ng·ªØ c·∫£nh ƒë·ªÉ quy·∫øt ƒë·ªãnh h√†nh ƒë·ªông ti·∫øp theo.
    """
    # 1. [B·∫§M GI·ªú] B·∫Øt ƒë·∫ßu t√≠nh gi·ªù l√†m vi·ªác
    start_time = time.time() 
    
    print(colored("[üîç RESEARCHER] ƒêang th·ª±c thi nhi·ªám v·ª• th√°m m√£ th·ªã tr∆∞·ªùng...", "cyan", attrs=["bold"]))
    # 2. [FIX QUAN TR·ªåNG] L·ªåC T√åM L·ªÜNH C·ª¶A CEO (HUMAN)
    messages = state.get("messages", [])
    
    # M·∫∑c ƒë·ªãnh l·∫•y tin cu·ªëi, nh∆∞ng s·∫Ω ∆∞u ti√™n t√¨m tin nh·∫Øn c·ªßa NG∆Ø·ªúI (Human) g·∫ßn nh·∫•t
    # ƒê·ªÉ tr√°nh l·∫•y nh·∫ßm tin nh·∫Øn ƒëi·ªÅu ph·ªëi c·ªßa h·ªá th·ªëng
    target_msg_content = ""
    for msg in reversed(messages):
        if isinstance(msg, HumanMessage):
            target_msg_content = msg.content
            break
            
    if not target_msg_content:
        target_msg_content = messages[-1].content # Fallback n·∫øu kh√¥ng t√¨m th·∫•y
    is_pure_research = "[RESEARCH]" in target_msg_content
    clean_query = target_msg_content.replace("[RESEARCH]", "").replace("[ORCHESTRATOR]", "").strip()
    # 2. X√¢y d·ª±ng Prompt Si√™u C·∫•u Tr√∫c (S·ª≠ d·ª•ng 4 c·ªôt tr·ª•)
    search_prompt = (
        f"Nhi·ªám v·ª•: Ph√¢n t√≠ch th·ªã tr∆∞·ªùng 2026 cho: '{clean_query}'."
        "\n\nY√äU C·∫¶U B√ÅO C√ÅO 4 C·ªòT TR·ª§:"
        "\n1. [D·ªÆ LI·ªÜU Vƒ® M√î]: T√¨nh h√¨nh th·ªã tr∆∞·ªùng v√† c√¥ng ngh·ªá m·ªõi nh·∫•t."
        "\n2. [BI·∫æN ƒê·ªòNG TH·ª∞C T·∫æ]: Xu h∆∞·ªõng ti√™u d√πng v√† 'n·ªói ƒëau' kh√°ch h√†ng."
        "\n3. [ƒê·ªêI TH·ª¶ TR·ª∞C DI·ªÜN]: Li·ªát k√™ 3 ƒë·ªëi th·ªß v√† l·ª£i th·∫ø c·ªßa h·ªç."
        "\n4. [C∆† H·ªòI CHO CEO]: Insight quan tr·ªçng v√† d·ª± b√°o 12 th√°ng t·ªõi."
        "\n\nƒê·ªãnh d·∫°ng: Markdown chuy√™n nghi·ªáp, c√≥ b·∫£ng so s√°nh."
    )
    
    try:
        # 3. Tri·ªáu h·ªìi Perplexity
        response = LLM_PERPLEXITY.invoke([
            SystemMessage(content="B·∫°n l√† Chief Research Officer. Ch·ªâ tr·∫£ v·ªÅ d·ªØ li·ªáu th·ª±c t·∫ø 2026, KH√îNG HTML."),
            HumanMessage(content=search_prompt)
        ])
        raw_res = response.content

        # --- T·∫¶NG PH√íNG TH·ª¶ 1: CH·∫∂N HTML & L·ªñI 401 ---
        if any(x in raw_res.lower() for x in ["<html>", "401 authorization", "cloudflare"]):
            return {
                "messages": [AIMessage(content="üö® [H·ªÜ TH·ªêNG] L·ªói k·∫øt n·ªëi ngu·ªìn tin (API 401). CEO h√£y ki·ªÉm tra l·∫°i Key Perplexity.")],
                "next_step": "FINISH" # D·ª´ng ngay l·∫≠p t·ª©c ƒë·ªÉ b·∫£o v·ªá t√†i nguy√™n
            }

        # --- T·∫¶NG PH√íNG TH·ª¶ 2: X·ª¨ L√ù K·∫æT QU·∫¢ TH√ÄNH C√îNG ---
        report_content = f"üîç **[B√ÅO C√ÅO CRO - {clean_query.upper()}]**\n\n{raw_res}"
        if is_pure_research:
            # N·∫øu CEO ch·ªâ mu·ªën nghi√™n c·ª©u (Tab Research), k·∫øt th√∫c t·∫°i ƒë√¢y.
            next_destination = "Secretary"
        else:
            # Thay v√¨ st.session_state, ta d√πng task_type ƒë∆∞·ª£c Dashboard g·ª≠i qua Server
            if state.get("task_type") == "dynamic":
                next_destination = "Orchestrator"
            else:
                next_destination = "Supervisor"

        # ============================================================
        # üü¢ [CH√àN ƒêO·∫†N N√ÄY V√ÄO] GHI S·ªî C√îNG VI·ªÜC
        # ============================================================
        try:
            log_work_to_db(
                agent="Researcher",
                task=clean_query,   # ƒê·ªÅ b√†i s·∫øp giao
                result=raw_res,     # K·∫øt qu·∫£ t√¨m ƒë∆∞·ª£c
                tool="Perplexity",  # S√∫ng ƒë√£ d√πng
                start_time=start_time # Th·ªùi gian b·∫Øt ƒë·∫ßu
            )
        except Exception as log_err:
            print(colored(f"‚ö†Ô∏è L·ªói ghi log k·∫ø to√°n: {log_err}", "yellow"))


        return {
            "messages": [AIMessage(content=report_content)],
            "next_step": next_destination,
            "current_agent": "Researcher" # ƒê·ªãnh danh ƒë·ªÉ Orchestrator bi·∫øt ai v·ª´a ho√†n th√†nh b√°o c√°o
        }

    except Exception as e:
        # T·∫¶NG PH√íNG TH·ª¶ 3: NGO·∫†I L·ªÜ
        print(colored(f"L·ªói Researcher: {e}", "red"))
        return {
            "messages": [AIMessage(content=f"‚ö†Ô∏è Tr·ª•c tr·∫∑c k·ªπ thu·∫≠t khi qu√©t d·ªØ li·ªáu: {str(e)}")],
            "next_step": "FINISH" 
        }

#  ---- T√†i Ch√≠nh----
def investment_node(state):
    """
    Agent CFO: Th·∫©m ƒë·ªãnh t√†i ch√≠nh v√† ROI.
    ƒê√£ n√¢ng c·∫•p: T·ª± ƒë·ªông ng·∫Øt lu·ªìng (FINISH) n·∫øu ·ªü ch·∫ø ƒë·ªô chuy√™n bi·ªát.
    """
    print(colored("[üí∞ INVESTMENT] ƒêang th·∫©m ƒë·ªãnh t√†i ch√≠nh d·ª± √°n...", "green", attrs=["bold"]))
    
    messages = state.get("messages", [])
    last_msg = messages[-1].content
    is_pure_invest = "[INVEST]" in last_msg
    
    # L·∫•y 3 tin nh·∫Øn g·∫ßn nh·∫•t ƒë·ªÉ c√≥ ƒë·ªß ng·ªØ c·∫£nh (B√°o c√°o Researcher + Coder...)
    context = "\n".join([m.content for m in messages[-3:]])
    
    prompt = (
        "B·∫°n l√† Gi√°m ƒë·ªëc T√†i ch√≠nh (CFO) c·ªßa AI Corporation. "
        "\nNHI·ªÜM V·ª§: L·∫≠p b·∫£ng ph√¢n t√≠ch CAPEX, OPEX, ROI v√† r·ªßi ro t√†i ch√≠nh."
        "\n\nY√äU C·∫¶U:"
        "\n- Tr√¨nh b√†y b·∫£ng Markdown s·∫°ch s·∫Ω."
        "\n- K·∫øt lu·∫≠n r√µ r√†ng: 'ƒê·∫¶U T∆Ø', 'THEO D√ïI' ho·∫∑c 'LO·∫†I B·ªé'."
    )
    
    try:
        # ∆Øu ti√™n GPT-4 cho t√≠nh to√°n con s·ªë ƒë·ªÉ tr√°nh sai s√≥t logic
        response = LLM_MAIN.invoke([
            SystemMessage(content=prompt), 
            HumanMessage(content=f"D·ªØ li·ªáu d·ª± √°n: {context}")
        ])
        
        # N·∫øu CEO ch·ªçn Tab INVEST -> Tr·∫£ k·∫øt qu·∫£ v√† FINISH (Nhanh)
        # N·∫øu ƒëang ch·∫°y lu·ªìng t·ª± ƒë·ªông -> Quay l·∫°i Supervisor
        next_destination = "FINISH" if is_pure_invest else "Supervisor"

        return {
            "messages": [AIMessage(content=f"üí∞ **[H·ªí S∆† TH·∫®M ƒê·ªäNH ƒê·∫¶U T∆Ø]**\n\n{response.content}")],
            "next_step": next_destination
        }
    except Exception as e:
        return {
            "messages": [AIMessage(content=f"‚ö†Ô∏è S·ª± c·ªë t√†i ch√≠nh: {str(e)}")],
            "next_step": "FINISH"
        }

#  ---- Ph√°p l√Ω----
def legal_node(state):
    """
    Agent Legal (CLO): R√† so√°t to√†n b·ªô d·ª± √°n tr∆∞·ªõc khi xu·∫•t b·∫£n.
    ƒê√£ n√¢ng c·∫•p: ƒê·ªçc to√†n b·ªô l·ªãch s·ª≠ ƒë·ªÉ ph√°t hi·ªán r·ªßi ro xuy√™n su·ªët.
    """
    print(colored("[‚öñÔ∏è LEGAL] Lu·∫≠t s∆∞ ƒëang r√† so√°t to√†n b·ªô h·ªì s∆° d·ª± √°n...", "red", attrs=["bold"]))
    
    messages = state.get("messages", [])
    last_msg = messages[-1].content
    is_pure_legal = "[LEGAL]" in last_msg
    
    # 1. T·ªîNG H·ª¢P H·ªí S∆†: Lu·∫≠t s∆∞ ph·∫£i ƒë·ªçc h·∫øt c√°c "cam k·∫øt" c·ªßa Agent kh√°c
    # Gom 10-15 tin nh·∫Øn ƒë·ªÉ th·∫•y to√†n b·ªô lu·ªìng t·ª´ K·ªπ thu·∫≠t ƒë·∫øn Marketing
    full_project_context = "\n".join([f"[{m.type.upper()}]: {m.content[:300]}..." for m in messages[-15:]])

    prompt = (
        "B·∫°n l√† Gi√°m ƒë·ªëc Ph√°p l√Ω (CLO) c·ªßa AI Corporation. "
        "\nNHI·ªÜM V·ª§: Th·∫©m ƒë·ªãnh ph√°p l√Ω v√† Qu·∫£n tr·ªã r·ªßi ro d·ª±a tr√™n H·ªí S∆† D·ª∞ √ÅN ƒë∆∞·ª£c cung c·∫•p."
        "\n\nY√äU C·∫¶U CHI·∫æN L∆Ø·ª¢C:"
        "\n1. R√Ä SO√ÅT IP: Ki·ªÉm tra b·∫£n quy·ªÅn h√¨nh ·∫£nh (Artist) v√† m√£ ngu·ªìn (Coder)."
        "\n2. TU√ÇN TH·ª¶: ƒê·ªëi chi·∫øu v·ªõi Lu·∫≠t An ninh m·∫°ng VN v√† GDPR."
        "\n3. SO·∫†N TH·∫¢O: ƒê∆∞a ra khung ƒêi·ªÅu kho·∫£n s·ª≠ d·ª•ng (ToS) v√† NDA m·∫´u cho d·ª± √°n."
        "\n4. K·∫æT LU·∫¨N: Ghi r√µ 'AN TO√ÄN' ho·∫∑c 'C·∫¢NH B√ÅO NGUY HI·ªÇM'."
    )
    
    try:
        # S·ª≠ d·ª•ng GPT-4o ƒë·ªÉ c√≥ t∆∞ duy l·∫≠p lu·∫≠n ph√°p lu·∫≠t s·∫Øc b√©n nh·∫•t
        response = LLM_GPT4.invoke([
            SystemMessage(content=prompt),
            HumanMessage(content=f"H·ªí S∆† D·ª∞ √ÅN C·∫¶N TH·∫®M ƒê·ªäNH:\n{full_project_context}\n\nY√äU C·∫¶U B·ªî SUNG: {last_msg}")
        ])
        
        # N·∫øu CEO ch·ªçn Tab Legal ri√™ng bi·ªát th√¨ k·∫øt th√∫c lu√¥n
        next_destination = "FINISH" if is_pure_legal else "Supervisor"

        return {
            "messages": [AIMessage(content=f"‚öñÔ∏è **[B√ÅO C√ÅO PH√ÅP L√ù & R·ª¶I RO CHI TI·∫æT]**\n\n{response.content}")],
            "next_step": next_destination
        }

    except Exception as e:
        # 1. Ghi log l·ªói ph√°p l√Ω ra Terminal ƒë·ªÉ CEO gi√°m s√°t r·ªßi ro h·ªá th·ªëng
        error_detail = str(e)
        print(colored(f"üö® [LEGAL CRITICAL ERROR]: {error_detail}", "red", attrs=["bold"]))
        
        # 2. Tr·∫£ v·ªÅ State chu·∫©n cho LangGraph
        # ƒê·∫£m b·∫£o next_step l√† "FINISH" ƒë·ªÉ ng·∫Øt lu·ªìng an to√†n khi c√≥ s·ª± c·ªë ph√°p l√Ω
        return {
            "messages": [AIMessage(content=f"‚ùå **C·∫¢NH B√ÅO PH√ÅP L√ù KH·∫®N C·∫§P**:\n\nQu√° tr√¨nh r√† so√°t b·ªã gi√°n ƒëo·∫°n: `{error_detail}`\n\nKhuy·∫øn ngh·ªã: CEO ki·ªÉm tra l·∫°i c√°c ƒëi·ªÅu kho·∫£n ƒë·∫ßu v√†o.")], 
            "next_step": "FINISH" 
        }
#  ---- Nh√¢n S·ª± ----
def hr_orchestrator_node(state):
    """
    Agent HR - B·ªô ƒëi·ªÅu ph·ªëi nh√¢n s·ª± & quy tr√¨nh:
    Ki·ªÉm tra xem CEO c√≥ thi·∫øt l·∫≠p k·ªãch b·∫£n t·ª± ƒë·ªông hay kh√¥ng.
    """
    print(colored("[üë• HR ORCHESTRATOR] ƒêang ki·ªÉm so√°t lu·ªìng v·∫≠n h√†nh...", "cyan", attrs=["bold"]))
    
    # 1. Ki·ªÉm tra xem c√≥ b·∫£n ƒë·ªì quy tr√¨nh (Workflow Map) n√†o ƒë∆∞·ª£c CEO v·∫Ω kh√¥ng
    workflow_script = state.get("custom_workflow", None) 
    
    if workflow_script:
        # --- CH·∫æ ƒê·ªò T·ª∞ ƒê·ªòNG (D·ª∞A TR√äN THI·∫æT L·∫¨P K√âO TH·∫¢) ---
        current_step = state.get("current_step_index", 0)
        target_node = workflow_script[current_step]
        
        print(colored(f"--> Theo k·ªãch b·∫£n CEO: Chuy·ªÉn sang {target_node}", "green"))
        
        # B√°o c√°o k·∫øt qu·∫£ ch·∫∑ng tr∆∞·ªõc v√† xin √Ω ki·∫øn duy·ªát
        return {
            "messages": [AIMessage(content=f"‚úÖ Giai ƒëo·∫°n {current_step} ho√†n t·∫•t. Ch·ªù CEO ph√™ duy·ªát ƒë·ªÉ sang {target_node}.")],
            "next_step": target_node,
            "current_step_index": current_step + 1
        }
    else:
        # --- CH·∫æ ƒê·ªò M·∫∂C ƒê·ªäNH (AI T·ª∞ SUY LU·∫¨N) ---
        print(colored("--> Ch·∫ø ƒë·ªô t·ª± ƒë·ªông: AI ƒëang ƒëi·ªÅu ph·ªëi theo ng·ªØ c·∫£nh...", "white"))
        # G·ªçi l·∫°i logic Supervisor c≈© c·ªßa ng√†i
        return {"next_step": "Supervisor"}

def secretary_node(state):
    """
    SECRETARY V3: COMMUNICATOR - C·∫¶U N·ªêI TH√îNG MINH
    Bi·∫øt c√°ch di·ªÖn ƒë·∫°t l·∫°i k·∫øt qu·∫£ t·ª´ c√°c b·ªô ph·∫≠n kh√¥ khan (Coder, Researcher) 
    th√†nh ng√¥n ng·ªØ con ng∆∞·ªùi d·ªÖ hi·ªÉu cho CEO.
    """
    print(colored("[üó£Ô∏è COMMUNICATOR] ƒêang bi√™n t·∫≠p l·∫°i n·ªôi dung...", "magenta", attrs=["bold"]))
    
    messages = state.get("messages", [])
    last_agent = state.get("current_agent", "Unknown")
    
    # L·∫•y to√†n b·ªô ng·ªØ c·∫£nh ƒë·ªÉ hi·ªÉu chuy·ªán g√¨ v·ª´a x·∫£y ra
    context = "\n".join([f"{m.type}: {m.content}" for m in messages[-3:]])

    # Prompt d·∫°y Th∆∞ k√Ω c√°ch n√≥i chuy·ªán
    prompt = (
        "B·∫°n l√† Tr·ª£ l√Ω C√° nh√¢n Th√¥ng minh c·ªßa CEO. C√°c b·ªô ph·∫≠n chuy√™n m√¥n (Coder, Artist...) v·ª´a g·ª≠i k·∫øt qu·∫£ l√™n.\n"
        "Nhi·ªám v·ª• c·ªßa b·∫°n: DI·ªÑN ƒê·∫†T L·∫†I k·∫øt qu·∫£ ƒë√≥ m·ªôt c√°ch t·ª± nhi√™n, chuy√™n nghi·ªáp.\n"
        "QUY T·∫ÆC:"
        "\n1. N·∫øu c√≥ H√åNH ·∫¢NH/CODE: Ph·∫£i hi·ªÉn th·ªã r√µ r√†ng (Gi·ªØ nguy√™n link/block code)."
        "\n2. N·∫øu l√† L·ªúI N√ìI: H√£y t√≥m t·∫Øt l·∫°i ng·∫Øn g·ªçn, d√πng gi·ªçng vƒÉn ƒë·ªëi tho·∫°i ('Th∆∞a CEO', 'T√¥i ƒë√£ ho√†n th√†nh...')."
        "\n3. KH√îNG b√°o c√°o m√°y m√≥c ki·ªÉu 'B∆∞·ªõc 1, B∆∞·ªõc 2'. H√£y n√≥i nh∆∞ ng∆∞·ªùi v·ªõi ng∆∞·ªùi."
        f"\n\nNG·ªÆ C·∫¢NH V·ª™A QUA:\n{context}"
    )

    try:
        response = LLM_GEMINI_VISION.invoke([SystemMessage(content=prompt)])
        
        # Ghi log (V·∫´n gi·ªØ ch·ª©c nƒÉng l∆∞u tr·ªØ ng·∫ßm)
        with open(f"Chat_Log_{int(time.time())}.txt", "w", encoding="utf-8") as f:
            f.write(response.content)

        return {
            "messages": [AIMessage(content=response.content)],
            "next_step": "FINISH"
        }
    except:
        return {"next_step": "FINISH"}
# ============================================================================
# NODE: MARKETING NODE (Gi√°m ƒë·ªëc Marketing - CMO)
# ============================================================================
def marketing_node(state):
    """
    Agent CMO: Chuy√™n gia Marketing v√† TƒÉng tr∆∞·ªüng.
    ƒê√£ n√¢ng c·∫•p: T·ª± ƒë·ªông ƒë·ªÅ xu·∫•t Visual Prompt cho Artist ƒë·ªÉ thi·∫øt k·∫ø ·∫£nh qu·∫£ng c√°o.
    """
    print(colored("[üì¢ MARKETING] ƒêang l·∫≠p chi·∫øn d·ªãch qu·∫£ng b√° b√πng n·ªï...", "yellow", attrs=["bold"]))
    
    messages = state.get("messages", [])
    last_msg = messages[-1].content
    is_pure_mkt = "[MARKETING]" in last_msg
    
    # L·∫•y ng·ªØ c·∫£nh s√¢u t·ª´ k·ªπ thu·∫≠t v√† t√†i ch√≠nh ƒë·ªÉ vi·∫øt b√†i c√≥ s·ª©c thuy·∫øt ph·ª•c
    project_context = "\n".join([m.content for m in messages[-5:]])
    
    prompt = (
        "B·∫°n l√† Gi√°m ƒë·ªëc Marketing (CMO) c·ªßa AI Corporation. "
        "\nNHI·ªÜM V·ª§: X√¢y d·ª±ng b·ªô n·ªôi dung qu·∫£ng b√° ƒëa k√™nh d·ª±a tr√™n th√†nh ph·∫©m k·ªπ thu·∫≠t."
        "\n\nY√äU C·∫¶U CHI·∫æN L∆Ø·ª¢C:"
        "\n- [INSIGHT]: D√πng d·ªØ li·ªáu k·ªπ thu·∫≠t ƒë·ªÉ n√™u b·∫≠t l·ª£i √≠ch cho ng∆∞·ªùi d√πng."
        "\n- [FACEBOOK]: M√¥ h√¨nh PAS, phong c√°ch th√¢n thi·ªán."
        "\n- [LINKEDIN]: M√¥ h√¨nh chuy√™n gia, t·∫≠p trung v√†o ROI v√† t√≠nh b·ªÅn v·ªØng."
        "\n- [VISUAL PROMPT]: QUAN TR·ªåNG! ƒê∆∞a ra 2 m√¥ t·∫£ h√¨nh ·∫£nh (ti·∫øng Anh) ƒë·ªÉ Agent Artist v·∫Ω ·∫£nh qu·∫£ng c√°o."
    )

    try:
        response = LLM_GPT4.invoke([
            SystemMessage(content=prompt),
            HumanMessage(content=f"D·ªØ li·ªáu s·∫£n ph·∫©m:\n{project_context}")
        ])

        # ƒê·ªäNH TUY·∫æN TH√îNG MINH:
        # N·∫øu CEO c·∫ßn ·∫£nh minh h·ªça ngay, c√≥ th·ªÉ chuy·ªÉn sang Artist
        # N·∫øu kh√¥ng, FINISH ƒë·ªÉ hi·ªán n·ªôi dung.
        next_destination = "FINISH" if is_pure_mkt else "Supervisor"

        return {
            "messages": [AIMessage(content=f"üì¢ **[CHI·∫æN D·ªäCH MARKETING ƒêA K√äNH]**\n\n{response.content}")],
            "next_step": next_destination
        }
        
    except Exception as e:
        # 1. Ghi log l·ªói Marketing ra Terminal ƒë·ªÉ CEO theo d√µi hi·ªáu su·∫•t chi·∫øn d·ªãch
        error_detail = str(e)
        print(colored(f"üö® [MARKETING CRITICAL ERROR]: {error_detail}", "red", attrs=["bold"]))
        
        # 2. Tr·∫£ v·ªÅ State chu·∫©n cho LangGraph
        # ƒê·∫£m b·∫£o messages l√† LIST v√† next_step l√† STRING "FINISH"
        return {
            "messages": [AIMessage(content=f"‚ùå **S·ª∞ C·ªê CHI·∫æN D·ªäCH MARKETING**:\n\nQu√° tr√¨nh l·∫≠p k·∫ø ho·∫°ch b·ªã gi√°n ƒëo·∫°n: `{error_detail}`\n\nKhuy·∫øn ngh·ªã: CEO h√£y ki·ªÉm tra l·∫°i y√™u c·∫ßu m·ª•c ti√™u ho·∫∑c ng√¢n s√°ch.")], 
            "next_step": "FINISH" 
        }
#  ---- V·∫Ω Thi·∫øt K·∫ø----
def artist_node(state):
    """
    ARTIST NODE V2 (REAL): V·∫Ω tranh th·∫≠t b·∫±ng DALL-E 3 HD.
    """
    print(colored("\n[üé® ARTIST] ƒêang kh·ªüi ƒë·ªông Studio DALL-E 3 HD...", "blue", attrs=["bold"]))
    
    messages = state.get("messages", [])
    # L·∫•y ƒëo·∫°n vƒÉn m√† CEO mu·ªën minh h·ªça
    last_msg_content = messages[-1].content
    
    # --- 1. TR√çCH XU·∫§T N·ªòI DUNG (H·ªó tr·ª£ c·∫£ 2 ki·ªÉu) ---
    # Ki·ªÉu 1: C√≥ d√πng d·∫•u """ (Chu·∫©n ch·ªâ)
    if '"""' in last_msg_content:
        start_idx = last_msg_content.find("\"\"\"") + 3
        end_idx = last_msg_content.rfind("\"\"\"")
        text_to_illustrate = last_msg_content[start_idx:end_idx].strip()
    # Ki·ªÉu 2: N√≥i t·ª± nhi√™n (VD: "V·∫Ω con m√®o") - S∆° cua
    else:
        # Lo·∫°i b·ªè c√°c tag h·ªá th·ªëng n·∫øu c√≥
        text_to_illustrate = last_msg_content.replace("[ARTIST]", "").strip()

    # Ki·ªÉm tra l·∫°i l·∫ßn cu·ªëi
    if not text_to_illustrate or len(text_to_illustrate) < 5:
        print(colored("üö´ [ARTIST] Kh√¥ng nh·∫≠n ƒë∆∞·ª£c n·ªôi dung ƒë·ªß ƒë·ªÉ v·∫Ω.", "red"))
        return {
            "messages": [AIMessage(content="üö´ H·ªça sƒ© c·∫ßn m√¥ t·∫£ chi ti·∫øt h∆°n ƒë·ªÉ v·∫Ω. Vui l√≤ng th·ª≠ l·∫°i.")], 
            "next_step": "FINISH" 
        }

    # --- 2. GPT-4: K·ª∏ S∆Ø PROMPT (Prompt Engineering) ---
    # Bi·∫øn y√™u c·∫ßu s∆° s√†i th√†nh Prompt ngh·ªá thu·∫≠t chi ti·∫øt
    analysis_prompt = (
        "B·∫°n l√† Gi√°m ƒë·ªëc Ngh·ªá thu·∫≠t (Art Director). Nhi·ªám v·ª•: T·∫°o Image Prompt cho DALL-E 3.\n"
        f"Y√äU C·∫¶U G·ªêC: \"{text_to_illustrate}\"\n\n"
        "H√ÉY TR·∫¢ V·ªÄ ƒê√öNG ƒê·ªäNH D·∫†NG JSON SAU (Kh√¥ng th√™m l·ªùi d·∫´n):\n"
        "```json\n"
        "{\n"
        "  \"style\": \"T√™n phong c√°ch ngh·ªá thu·∫≠t ph√π h·ª£p nh·∫•t (V√≠ d·ª•: Cyberpunk, Studio Ghibli, Photorealistic, Oil Painting...)\",\n"
        "  \"prompt\": \"M√¥ t·∫£ chi ti·∫øt h√¨nh ·∫£nh b·∫±ng ti·∫øng Anh, t·∫≠p trung v√†o √°nh s√°ng, b·ªë c·ª•c, chi ti·∫øt, c·∫£m x√∫c. T·ªëi ƒëa 70 t·ª´.\"\n"
        "}\n"
        "```"
    )

    try:
        # G·ªçi GPT-4 ƒë·ªÉ l·∫•y prompt x·ªãn
        analysis_response = LLM_GEMINI_VISION.invoke([SystemMessage(content="JSON mode."), HumanMessage(content=analysis_prompt)])
        
        # L√†m s·∫°ch chu·ªói JSON (ƒë·ªÅ ph√≤ng GPT th√™m markdown)
        json_str = analysis_response.content.replace("```json", "").replace("```", "").strip()
        analysis_data = json.loads(json_str)
        
        design_style = analysis_data.get('style', 'Cinematic')
        visual_prompt = analysis_data.get('prompt', text_to_illustrate[:100])

        # T·∫°o prompt cu·ªëi c√πng
        full_image_prompt = f"{visual_prompt}, {design_style} style. High resolution, highly detailed, masterpiece."
        print(colored(f"--> Phong c√°ch: {design_style}", "cyan"))
        print(colored(f"--> Prompt v·∫Ω: {full_image_prompt[:100]}...", "white"))
            
        # --- 3. G·ªåI DALL-E 3 V·∫º TRANH TH·∫¨T (QUAN TR·ªåNG NH·∫§T) ---
        print(colored("‚è≥ ƒêang g·ª≠i y√™u c·∫ßu ƒë·∫øn m√°y ch·ªß OpenAI DALL-E 3 (Ch·ªù 15-30s)...", "yellow"))
        
        # Kh·ªüi t·∫°o c√¥ng c·ª• v·∫Ω HD
        dalle_tool = DallEAPIWrapper(
            model="dall-e-3",
            size="1024x1024",
            quality="hd" # Ch·∫•t l∆∞·ª£ng cao nh·∫•t
        )
        
        # Th·ª±c thi v·∫Ω (C√≥ th·ªÉ t·ªën 15-30 gi√¢y)
        image_url = dalle_tool.run(full_image_prompt)
        
        print(colored(f"‚úÖ [ART COMPLETE]: ·∫¢nh ƒë√£ s·∫µn s√†ng!", "green"))

        # --- 4. TR·∫¢ K·∫æT QU·∫¢ NHANH (FAST TRACK) ---
        # Tr·∫£ v·ªÅ FINISH ngay ƒë·ªÉ hi·ªán ·∫£nh, kh√¥ng qua Th∆∞ k√Ω n·ªØa.
        # S·ª≠ d·ª•ng Markdown chu·∫©n ƒë·ªÉ Dashboard hi·ªÉn th·ªã ·∫£nh.
        
        final_content = (
            f"üé® **T√ÅC PH·∫®M HO√ÄN THI·ªÜN:**\n\n"
            f"![AI Art Generation]({image_url})\n\n"
            f"*(Phong c√°ch: {design_style})*"
        )

        return {
            "messages": [AIMessage(content=final_content)],
            "next_step": "FINISH" # K·∫øt th√∫c ngay
        }

    # --- X·ª¨ L√ù L·ªñI ---
    except json.JSONDecodeError:
        print(colored("‚ùå L·ªói: GPT-4 kh√¥ng tr·∫£ v·ªÅ JSON h·ª£p l·ªá.", "red"))
        return {"messages": [AIMessage(content="‚ö†Ô∏è L·ªói ph√¢n t√≠ch y√™u c·∫ßu v·∫Ω tranh.")], "next_step": "FINISH"}
    except Exception as e:
        error_detail = str(e)
        print(colored(f"‚ùå L·ªñI V·∫º TRANH (DALL-E/API): {error_detail}", "red"))
        # Th√¥ng b√°o l·ªói r√µ r√†ng cho CEO (V√≠ d·ª•: H·∫øt ti·ªÅn, Vi ph·∫°m ch√≠nh s√°ch n·ªôi dung...)
        return {
            "messages": [AIMessage(content=f"‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫°o ·∫£nh l√∫c n√†y. Nguy√™n nh√¢n: {error_detail}")], 
            "next_step": "FINISH"
        }
# ============================================================================
# NODE: STORYTELLER (Nh√† vƒÉn & Bi√™n k·ªãch chuy√™n nghi·ªáp)

# ============================================================================
def storyteller_node(state):
    print(colored("[‚úçÔ∏è STORYTELLER] ƒêang x√¢y d·ª±ng th·∫ø gi·ªõi v√† c·ªët truy·ªán...", "cyan", attrs=["bold"]))
    
    messages = state.get("messages", [])
    # L·∫•y log l·ªói n·∫øu c√≥ ƒë·ªÉ ƒëi·ªÅu ch·ªânh vƒÉn phong
    errors = state.get("error_log", [])
    
    last_msg = messages[-1].content
    
    # 1. PH√ÇN T√çCH NHU C·∫¶U
    is_continue = "[CONTINUE]" in last_msg.upper()
    clean_query = last_msg.replace("[STORY]", "").replace("[CONTINUE]", "").strip()

    # 2. TR√ç NH·ªö M·∫†CH TRUY·ªÜN (Thay th·∫ø st.session_state)
    # Ch√∫ng ta l·∫•y b·ªëi c·∫£nh t·ª´ tin nh·∫Øn AIMessage g·∫ßn nh·∫•t trong l·ªãch s·ª≠ h·ªôi tho·∫°i c·ªßa Graph
    previous_full_story_content = ""
    if is_continue:
        for m in reversed(messages):
            if isinstance(m, AIMessage) and len(m.content) > 100:
                previous_full_story_content = m.content
                break
        
        if previous_full_story_content:
            # L·∫•y ƒëo·∫°n k·∫øt ƒë·ªÉ AI vi·∫øt n·ªëi ti·∫øp kh√¥ng b·ªã l·∫∑p
            context_tail = previous_full_story_content[-1000:]
            print(colored(f"üìú ƒê√£ t√¨m th·∫•y m·∫°ch truy·ªán c≈©, ƒëang n·ªëi ti·∫øp...", "yellow"))
            previous_full_story_content = context_tail

    # 3. THI·∫æT L·∫¨P PROMPT CHI·∫æN THU·∫¨T
    prompt = (
        "B·∫°n l√† Nh√† vƒÉn Best-seller v√† Bi√™n k·ªãch xu·∫•t s·∫Øc. "
        "\nNHI·ªÜM V·ª§: S√°ng t√°c n·ªôi dung c√≥ chi·ªÅu s√¢u, l√¥i cu·ªën."
        "\n\nNGUY√äN T·∫ÆC V√ÄNG:"
        + (f"\n- M·∫†CH TRUY·ªÜN TR∆Ø·ªöC: '{previous_full_story_content}' (H√£y vi·∫øt ti·∫øp t·ª´ ƒë√¢y, kh√¥ng ch√†o h·ªèi l·∫°i)." if previous_full_story_content else "\n- ƒê√ÇY L√Ä KH·ªûI ƒê·∫¶U: H√£y t·∫°o m·ªôt m·ªü ƒë·∫ßu ·∫•n t∆∞·ª£ng.") +
        "\n- C·∫§U TR√öC: Show, Don't Tell. S·ª≠ d·ª•ng nhi·ªÅu t·ª´ ng·ªØ g·ª£i h√¨nh, g·ª£i c·∫£m."
        "\n- H√åNH ·∫¢NH: Sau m·ªói ph√¢n ƒëo·∫°n cao tr√†o, h√£y ch√®n m·ªôt Visual Prompt ti·∫øng Anh trong ngo·∫∑c vu√¥ng [Visual: ...]."
    )

    try:
        # L·ª±a ch·ªçn Model: ∆Øu ti√™n Claude cho s√°ng t·∫°o vƒÉn h·ªçc
        selected_llm = LLM_CLAUDE if 'LLM_CLAUDE' in globals() else LLM_GPT4
        
        response = selected_llm.invoke([
            SystemMessage(content=prompt),
            HumanMessage(content=clean_query)
        ])

        # ƒê·ªäNH TUY·∫æN: Th∆∞·ªùng sau khi k·ªÉ chuy·ªán s·∫Ω k·∫øt th√∫c ƒë·ªÉ CEO ƒë·ªçc, ho·∫∑c qua Artist ƒë·ªÉ v·∫Ω
        return {
            "messages": [AIMessage(content=response.content)],
            "next_step": "Secretary" # ƒê∆∞a qua Th∆∞ k√Ω ƒë·ªÉ ch·ªët h·ªì s∆°
        }

    except Exception as e:
        error_msg = f"L·ªói Storyteller: {str(e)}"
        print(colored(f"‚ùå {error_msg}", "red"))
        return {
            "messages": [AIMessage(content=f"‚ö†Ô∏è S√°ng t√°c gi√°n ƒëo·∫°n: {error_msg}")],
            "error_log": errors + [error_msg],
            "next_step": "Secretary"
        }
def storytelling_node(state):
    print(colored("[üñãÔ∏è STORYTELLING] ƒê·∫°i vƒÉn h√†o ƒëang n·ªëi m·∫°ch c·∫£m x√∫c...", "magenta", attrs=["bold"]))
    
    messages = state.get("messages", [])
    last_msg = messages[-1].content
    
    # 1. PH√ÇN T√çCH NHU C·∫¶U: Vi·∫øt m·ªõi hay Vi·∫øt ti·∫øp?
    is_continue = "[CONTINUE]" in last_msg
    
    # 2. TR√ç NH·ªö D√ÄI H·∫†N: L·∫•y n·ªôi dung ch∆∞∆°ng tr∆∞·ªõc ƒë√≥ (n·∫øu l√† vi·∫øt ti·∫øp)
    previous_content = ""
    if is_continue and len(messages) > 1:
        # L·∫•y n·ªôi dung m√† AI v·ª´a tr·∫£ v·ªÅ ·ªü l∆∞·ª£t tr∆∞·ªõc
        previous_content = messages[-2].content 

    prompt = (
        "B·∫°n l√† Nh√† vƒÉn Best-seller. "
        "\nNHI·ªÜM V·ª§: Vi·∫øt ch∆∞∆°ng ti·∫øp theo c·ªßa c√¢u chuy·ªán."
        "\n\nY√äU C·∫¶U DUY TR√å M·∫†CH VƒÇN:"
        f"\n- ƒêO·∫†N K·∫æT CH∆Ø∆†NG TR∆Ø·ªöC: '{previous_content[-500:]}' (H√£y n·ªëi ti·∫øp m·∫°ch n√†y)."
        "\n- KH√îNG l·∫∑p l·∫°i l·ªùi ch√†o hay t√≥m t·∫Øt ch∆∞∆°ng c≈©."
        "\n- B·∫Øt ƒë·∫ßu ngay v√†o h√†nh ƒë·ªông ho·∫∑c l·ªùi tho·∫°i ti·∫øp theo."
        "\n- Gi·ªØ nguy√™n vƒÉn phong, t√™n nh√¢n v·∫≠t v√† b·ªëi c·∫£nh."
    )

    # 3. TH·ª∞C THI (D√πng Claude 3.5 Sonnet ƒë·ªÉ c√≥ s·ª± m∆∞·ª£t m√† nh·∫•t)
    response = LLM_CLAUDE.invoke([
        SystemMessage(content=prompt),
        HumanMessage(content=last_msg.replace("[CONTINUE]", ""))
    ])

    return {
        "messages": [AIMessage(content=response.content)],
        "next_step": "FINISH"
    }

# ============================================================================
# NODE: R&D STRATEGY (Gi√°m ƒë·ªëc Chi·∫øn l∆∞·ª£c - CSO)
# ============================================================================
def research_development_agent(state):
    """
    Agent R&D: K·∫øt h·ª£p t√¨m ki·∫øm th·ªùi gian th·ª±c v√† ph√¢n t√≠ch m√¥ h√¨nh PESTLE/Roadmap.
    """
    print(colored("[üß† R&D STRATEGY] ƒêang thi·∫øt l·∫≠p t·∫ßm nh√¨n chi·∫øn l∆∞·ª£c...", "blue", attrs=["bold"]))
    
    messages = state.get("messages", [])
    user_input = messages[-1].content
    
    # 1. Truy xu·∫•t k√Ω ·ª©c c√¥ng ty ƒë·ªÉ ƒë·∫£m b·∫£o chi·∫øn l∆∞·ª£c ƒë·ªìng nh·∫•t
    company_context = search_memory("T·∫ßm nh√¨n v√† m·ª•c ti√™u chi·∫øn l∆∞·ª£c AI Corporation")
    
    # 2. B∆∞·ªõc nghi√™n c·ª©u th·ª±c t·∫ø (S·ª≠ d·ª•ng Perplexity ƒë·ªÉ tr√°nh n√≥i s√°o r·ªóng)
    # Ch√∫ng ta y√™u c·∫ßu AI t√¨m d·ªØ li·ªáu th·ª±c t·∫ø tr∆∞·ªõc khi ph√¢n t√≠ch
    search_query = f"Xu h∆∞·ªõng c√¥ng ngh·ªá, ƒë·ªëi th·ªß c·∫°nh tranh v√† r·ªßi ro th·ªã tr∆∞·ªùng nƒÉm 2026 cho: {user_input}"
    
    try:
        # L·∫•y d·ªØ li·ªáu th·ª±c t·∫ø t·ª´ internet
        market_data = LLM_PERPLEXITY.invoke([
            SystemMessage(content="B·∫°n l√† chuy√™n gia ph√¢n t√≠ch d·ªØ li·ªáu th·ªã tr∆∞·ªùng."),
            HumanMessage(content=search_query)
        ]).content
        
        # 3. T·ªïng h·ª£p th√†nh b√°o c√°o chi·∫øn l∆∞·ª£c chuy√™n s√¢u
        # K·∫øt h·ª£p: D·ªØ li·ªáu th·ª±c t·∫ø + Prompt h·ªá th·ªëng + Ng·ªØ c·∫£nh c√¥ng ty
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", STRATEGY_SYSTEM_PROMPT),
            ("human", (
                f"Y√äU C·∫¶U NGHI√äN C·ª®U: {user_input}\n\n"
                f"D·ªÆ LI·ªÜU TH·ªä TR∆Ø·ªúNG TH·ª∞C T·∫æ: {market_data}\n\n"
                f"B·ªêI C·∫¢NH C√îNG TY: {company_context}\n\n"
                "H√£y l·∫≠p b√°o c√°o chi·∫øn l∆∞·ª£c chi ti·∫øt (PESTLE, Roadmap 2-5 nƒÉm)."
            ))
        ])
        
        # S·ª≠ d·ª•ng GPT-4o ƒë·ªÉ t·ªïng h·ª£p v√¨ kh·∫£ nƒÉng vi·∫øt b√°o c√°o r·∫•t t·ªët
        chain = prompt_template | LLM_GPT4
        response = chain.invoke({})
        
        return {
            "messages": [AIMessage(content=f"üß† [B√ÅO C√ÅO CHI·∫æN L∆Ø·ª¢C R&D]:\n{response.content}")],
            "next_step": "Supervisor"
        }
        
    except Exception as e:
        print(colored(f"L·ªói R&D Agent: {e}", "red"))
        return {"next_step": "Supervisor", "error_log": [str(e)]}

# ==========================================
# --- 4. THI·∫æT L·∫¨P LU·ªíNG AGENT (GRAPH) ---
# ==========================================

workflow = StateGraph(AgentState)

# --- 4.1 ƒêƒÉng k√Ω t·∫•t c·∫£ c√°c Node (ƒê·∫£m b·∫£o t√™n kh·ªõp 100%) ---
nodes_map = {
    "Router": router_node, 
    "Supervisor": supervisor_node, 
    "Coder": coder_node,
    "Tester": tester_node, 
    "Hardware": hardware_node, 
    "Engineering": engineering_node,
    "IoT_Engineer": iot_node, 
    "Procurement": procurement_node, 
    "Investment": investment_node,
    "Researcher": researcher_node, 
    "Strategy_R_and_D": research_development_agent,
    "Legal": legal_node, 
    "Marketing": marketing_node, 
    "Artist": artist_node,
    "Storyteller": storyteller_node, 
    "Orchestrator": dynamic_orchestrator,
    "Publisher": publisher_node, 
    "Secretary": secretary_node
}

for name, func in nodes_map.items():
    workflow.add_node(name, func)

# --- 4.2 Thi·∫øt l·∫≠p ƒëi·ªÉm v√†o ---
workflow.set_entry_point("Router")

# --- 4.3 Logic Router ---
# Thay v√¨ d√πng router_node tr·ª±c ti·∫øp, ta d√πng lambda ƒë·ªÉ l·∫•y chu·ªói 'next_step'
workflow.add_conditional_edges(
    "Router", 
    lambda x: x.get("next_step", "Supervisor"), 
    {
        "Researcher": "Researcher", 
        "Investment": "Investment", 
        "Storyteller": "Storyteller",
        "Artist": "Artist", 
        "Engineering": "Engineering", 
        "Publisher": "Publisher",
        "Orchestrator": "Orchestrator", 
        "Supervisor": "Supervisor", 
        "Secretary": "Secretary"
    }
)

# --- 4.4 Logic Supervisor ---
workflow.add_conditional_edges(
    "Supervisor", 
    lambda x: x.get("next_step", "Secretary"), 
    {
        "Coder": "Coder", 
        "Hardware": "Hardware", 
        "Engineering": "Engineering",
        "IoT_Engineer": "IoT_Engineer", 
        "Procurement": "Procurement",
        "Investment": "Investment", 
        "Researcher": "Researcher", 
        "Strategy_R_and_D": "Strategy_R_and_D",
        "Legal": "Legal", 
        "Marketing": "Marketing", 
        "Artist": "Artist",
        "Storyteller": "Storyteller", 
        "Secretary": "Secretary", 
        "FINISH": "Secretary"
    }
)

# --- 4.5 Nh√≥m Agent ph·ªï th√¥ng (H·ªìi quy v·ªÅ Supervisor ho·∫∑c k·∫øt th√∫c) ---
# L∆∞u √Ω: Kh√¥ng bao g·ªìm Coder, Tester, Hardware, Procurement, Investment, Researcher, Orchestrator
general_agents = [
    "Engineering", "IoT_Engineer", "Strategy_R_and_D", "Legal", 
    "Marketing", "Artist", "Storyteller", "Publisher"
]

for node in general_agents:
    workflow.add_conditional_edges(
        node,
        lambda x: x.get("next_step", "Supervisor") if x.get("next_step") != "FINISH" else "Secretary",
        {
            "Supervisor": "Supervisor", 
            "Secretary": "Secretary",
            "Artist": "Artist",
            "Procurement": "Procurement"
        }
    )

# --- 4.6 Logic chuy√™n bi·ªát (Pipeline & ƒê·∫∑c th√π) ---

# Lu·ªìng Researcher -> Orchestrator
workflow.add_conditional_edges(
    "Researcher",
    lambda x: "Orchestrator" if x.get("task_type") == "dynamic" else "Secretary",
    {"Orchestrator": "Orchestrator", "Secretary": "Secretary"}
)

# Lu·ªìng Orchestrator t·ªèa ƒëi c√°c nh√°nh
workflow.add_conditional_edges(
    "Orchestrator",
    lambda x: x.get("next_step", "Secretary") if x.get("next_step") != "FINISH" else "Secretary",
    {
        "Engineering": "Engineering", 
        "Hardware": "Hardware", 
        "Procurement": "Procurement",
        "IoT_Engineer": "IoT_Engineer", 
        "Supervisor": "Supervisor", 
        "Secretary": "Secretary"
    }
)

# Lu·ªìng K·ªπ thu·∫≠t: Coder -> Tester
workflow.add_edge("Coder", "Tester")
workflow.add_conditional_edges(
    "Tester", 
    lambda x: x.get("next_step", "Supervisor"), 
    {"Coder": "Coder", "Supervisor": "Supervisor"}
)

# Lu·ªìng V·∫≠t l√Ω & T√†i ch√≠nh c·ªë ƒë·ªãnh: Hardware -> Procurement -> Investment -> Supervisor/Secretary
workflow.add_edge("Hardware", "Procurement")
workflow.add_edge("Procurement", "Investment")
workflow.add_conditional_edges(
    "Investment",
    lambda x: "Secretary" if x.get("next_step") == "FINISH" else "Supervisor",
    {"Secretary": "Secretary", "Supervisor": "Supervisor"}
)

# --- 4.7 K·∫øt th√∫c h·ªá th·ªëng ---
workflow.add_edge("Secretary", END)

# --- 4.8 BI√äN D·ªäCH H·ªÜ TH·ªêNG ---
ai_app = workflow.compile() 
app = ai_app
db = None # Placeholder cho ƒë·ªëi t∆∞·ª£ng Database c·ªßa ng√†i

# ============================================================================
# 5. H√ÄM V·∫¨N H√ÄNH CH√çNH (ƒê·∫∂T ·ªû ƒê√ÇY)
# ============================================================================
async def run_ai_corporation(user_input, thread_id="1"):
    """
    ƒêi·ªÉm k√≠ch ho·∫°t h·ªá th·ªëng: Qu·∫£n l√Ω phi√™n l√†m vi·ªác v√† x·ª≠ l√Ω l·ªói t·∫ßng cao nh·∫•t.
    """
    config = {"configurable": {"thread_id": thread_id}, "recursion_limit": 50}
    
    # Kh·ªüi t·∫°o tr·∫°ng th√°i ban ƒë·∫ßu
    initial_state = {
                "messages": [HumanMessage(content=user_input)],
                "next_step": "Supervisor",
                "current_agent": "User", # Th√™m d√≤ng n√†y ƒë·ªÉ tr√°nh l·ªói NoneType
                "error_log": [],
                "task_type": "general"
            }

    print(colored(f"\nüöÄ PROJECT START: {user_input[:50]}...", "blue", attrs=["bold"]))

    try:
        # Ch·∫°y Graph (Gi·∫£ s·ª≠ b·∫°n ƒë√£ compile graph th√†nh app)
        async for event in app.astream(initial_state, config):
            for node, values in event.items():
                if node != "__metadata__":
                    print(colored(f"üìç Node [{node}] has completed.", "dark_grey"))
        
        print(colored("\n‚úÖ PROJECT FINISHED SUCCESSFULLY", "green", attrs=["bold"]))

    except Exception as e:
        # N·∫øu Graph s·∫≠p, k√≠ch ho·∫°t Fallback ngay l·∫≠p t·ª©c
        return ultimate_fallback(initial_state, [str(e)])
    
# ============================================================================
# 6. CH·∫†Y H·ªÜ TH·ªêNG (ASYNC ENGINE)
# ============================================================================

async def main_loop():
    print(colored("\n" + "="*50, "cyan"))
    print(colored("üöÄ AI CORPORATION - H·ªÜ TH·ªêNG ƒêI·ªÄU H√ÄNH T·ª∞ ƒê·ªòNG", "cyan", attrs=["bold"]))
    print(colored("Ch·∫ø ƒë·ªô: Parallel Coding & AST Testing [ON]", "green"))
    print(colored("="*50 + "\n", "cyan"))
    print(colored("‚ÑπÔ∏è  H·ªá th·ªëng ƒëang ch·∫°y ng·∫ßm. H√£y g·ª≠i y√™u c·∫ßu t·ª´ Dashboard HTML.", "yellow"))
    while True:
        await asyncio.sleep(300) # Ngh·ªâ m·ªói 1 ti·∫øng r·ªìi l·∫∑p l·∫°i (v√¥ t·∫≠n)
        try:
            user_input = input(colored("CEO (Y√™u c·∫ßu): ", "white", attrs=["bold"]))
            if user_input.lower() in ['q', 'exit']: 
                auto_backup_brain() # T·ª± ƒë·ªông sao l∆∞u tr∆∞·ªõc khi t·∫Øt m√°y
                break
            
            initial_state = {
                "messages": [HumanMessage(content=user_input)],
                "next_step": "Supervisor",
                "current_agent": "User", 
                "error_log": [],
                "task_type": "general"
            }
            
            # K√≠ch ho·∫°t Graph ch·∫°y (S·ª≠ d·ª•ng astream cho c√°c h√†m async)
            print(colored("\n--- ƒêANG X·ª¨ L√ù ---", "white", attrs=["bold"]))
            config = {"configurable": {"thread_id": "ceo_session"}, "recursion_limit": 150}
            async for event in app.astream(initial_state, config=config):
                for node, values in event.items():
                    if node != "__end__":
                        print(colored(f"  [‚ûî] {node} ƒë√£ ho√†n th√†nh nhi·ªám v·ª•.", "dark_grey"))
                        # N·∫øu mu·ªën in n·ªôi dung tin nh·∫Øn cu·ªëi c√πng c·ªßa t·ª´ng b∆∞·ªõc:
                        # print(values["messages"][-1].content)

            print(colored("\n‚úÖ ƒê√É HO√ÄN T·∫§T QUY TR√åNH.", "green", attrs=["bold"]))

        except Exception as e:
            print(colored(f"‚ùå L·ªñI H·ªÜ TH·ªêNG: {e}", "red"))

# 1. GI√ÅO TR√åNH ƒê√ÄO T·∫†O (CURRICULUM)
CURRICULUM = {
    # === NH√ìM 1: QU·∫¢N TR·ªä & CHI·∫æN L∆Ø·ª¢C (C-SUITE) ===
    "[ORCHESTRATOR]": [
        "M√¥ h√¨nh OKRs vs KPIs trong qu·∫£n tr·ªã doanh nghi·ªáp AI",
        "Chi·∫øn l∆∞·ª£c qu·∫£n tr·ªã kh·ªßng ho·∫£ng (Crisis Management) th·ªùi gian th·ª±c",
        "T·ªëi ∆∞u h√≥a quy tr√¨nh ra quy·∫øt ƒë·ªãnh d·ª±a tr√™n d·ªØ li·ªáu (Data-Driven Decision Making)",
        "Tin t·ª©c c√¥ng ngh·ªá Deep Tech to√†n c·∫ßu 24h qua"
    ],
    "[FINANCE]": [
        "C√°c chi·∫øn l∆∞·ª£c Hedging r·ªßi ro t·ª∑ gi√° h·ªëi ƒëo√°i",
        "·ª®ng d·ª•ng Blockchain trong qu·∫£n l√Ω d√≤ng ti·ªÅn doanh nghi·ªáp (Corporate Treasury)",
        "Ph√¢n t√≠ch k·ªπ thu·∫≠t n√¢ng cao: S√≥ng Elliott v√† Fibonacci trong th·ªã tr∆∞·ªùng v√†ng/Crypto",
        "T·ªëi ∆∞u h√≥a thu·∫ø cho doanh nghi·ªáp s·ªë (Digital Tax Optimization)"
    ],
    "[HR_MANAGER]": [
        "X√¢y d·ª±ng khung nƒÉng l·ª±c c·ªët l√µi cho nh√¢n s·ª± AI & Blockchain",
        "T√¢m l√Ω h·ªçc h√†nh vi trong gi·ªØ ch√¢n nh√¢n t√†i Gen Z & Alpha",
        "T·ª± ƒë·ªông h√≥a quy tr√¨nh Payroll v√† C&B b·∫±ng Smart Contracts",
        "Lu·∫≠t lao ƒë·ªông qu·ªëc t·∫ø v·ªÅ l√†m vi·ªác t·ª´ xa (Remote Work Compliance)"
    ],

    # === NH√ìM 2: K·ª∏ THU·∫¨T PH·∫¶N M·ªÄM (CORE TECH) ===
    "[CODER]": [
        "L·∫≠p tr√¨nh hi·ªáu nƒÉng cao v·ªõi Rust v√† Go cho Backend",
        "T·ªëi ∆∞u h√≥a truy v·∫•n Database (Indexing, Partitioning, Sharding)",
        "Event-Driven Architecture v·ªõi Apache Kafka v√† RabbitMQ",
        "WebAssembly (Wasm): T∆∞∆°ng lai c·ªßa ·ª©ng d·ª•ng Web hi·ªáu nƒÉng cao"
    ],
    "[ARCHITECT]": [
        "Domain-Driven Design (DDD) trong thi·∫øt k·∫ø Microservices",
        "Tri·ªÉn khai Serverless tr√™n quy m√¥ l·ªõn (AWS Lambda/Google Cloud Run)",
        "M√¥ h√¨nh CQRS v√† Event Sourcing trong h·ªá th·ªëng ph√¢n t√°n",
        "Zero Trust Architecture: Ki·∫øn tr√∫c b·∫£o m·∫≠t kh√¥ng tin c·∫≠y ai"
    ],
    "[SECURITY]": [
        "K·ªπ thu·∫≠t Reverse Engineering m√£ ƒë·ªôc n√¢ng cao",
        "B·∫£o m·∫≠t API theo chu·∫©n OWASP Top 10 nƒÉm 2026",
        "Post-Quantum Cryptography: M√£ h√≥a ch·ªëng m√°y t√≠nh l∆∞·ª£ng t·ª≠",
        "DevSecOps: T√≠ch h·ª£p b·∫£o m·∫≠t v√†o quy tr√¨nh CI/CD"
    ],
    "[DATA_ANALYST]": [
        "X√¢y d·ª±ng RAG (Retrieval-Augmented Generation) cho LLM doanh nghi·ªáp",
        "Data Lakehouse: K·∫øt h·ª£p s·ª©c m·∫°nh c·ªßa Data Lake v√† Data Warehouse",
        "Ph√¢n t√≠ch d·ªØ li·ªáu th·ªùi gian th·ª±c (Real-time Analytics) v·ªõi Apache Flink",
        "M√¥ h√¨nh d·ª± b√°o chu·ªói th·ªùi gian (Time-series Forecasting) b·∫±ng Deep Learning"
    ],

    # === NH√ìM 3: PH·∫¶N C·ª®NG & IOT (HARDWARE) ===
    "[HARDWARE]": [
        "Thi·∫øt k·∫ø m·∫°ch PCB cao t·∫ßn (High-speed PCB Design)",
        "Edge AI: Ch·∫°y m√¥ h√¨nh AI tr·ª±c ti·∫øp tr√™n vi ƒëi·ªÅu khi·ªÉn (TinyML)",
        "C√¥ng ngh·ªá Pin th·∫ø h·ªá m·ªõi v√† qu·∫£n l√Ω nƒÉng l∆∞·ª£ng (Power Management)",
        "L·∫≠p tr√¨nh FPGA cho x·ª≠ l√Ω t√≠n hi·ªáu s·ªë"
    ],
    "[IOT]": [
        "M·∫°ng l∆∞·ªõi v·∫°n v·∫≠t (Mesh Networking) v·ªõi LoRaWAN v√† Zigbee",
        "Digital Twins: B·∫£n sao s·ªë trong c√¥ng nghi·ªáp s·∫£n xu·∫•t",
        "Giao th·ª©c MQTT v5 v√† t·ªëi ∆∞u h√≥a bƒÉng th√¥ng cho thi·∫øt b·ªã IoT",
        "B·∫£o m·∫≠t thi·∫øt b·ªã IoT ·ªü c·∫•p ƒë·ªô ph·∫ßn c·ª©ng (Hardware Security Modules)"
    ],

    # === NH√ìM 4: S√ÅNG T·∫†O & MARKETING (GROWTH) ===
    "[MARKETING]": [
        "Neuromarketing: ·ª®ng d·ª•ng khoa h·ªçc n√£o b·ªô v√†o qu·∫£ng c√°o",
        "Programmatic Advertising: Qu·∫£ng c√°o l·∫≠p tr√¨nh h√≥a t·ª± ƒë·ªông",
        "Chi·∫øn l∆∞·ª£c Growth Hacking d·ª±a tr√™n Ph·ªÖu AARRR",
        "T·ªëi ∆∞u h√≥a t√¨m ki·∫øm b·∫±ng gi·ªçng n√≥i (Voice Search SEO)"
    ],
    "[ARTIST]": [
        "Quy tr√¨nh s·∫£n xu·∫•t Video Generative AI (Runway Gen-3, Sora)",
        "Thi·∫øt k·∫ø tr·∫£i nghi·ªám ng∆∞·ªùi d√πng kh√¥ng gian (Spatial UX cho VR/AR)",
        "L√Ω thuy·∫øt m√†u s·∫Øc n√¢ng cao v√† t√¢m l√Ω h·ªçc h√¨nh ·∫£nh",
        "K·ªπ thu·∫≠t Prompt Engineering chuy√™n s√¢u cho Midjourney v6"
    ],
    "[CONTENT_WRITER]": [
        "K·ªπ thu·∫≠t Storytelling: C·∫•u tr√∫c h√†nh tr√¨nh anh h√πng trong B2B",
        "SEO Semantic Search v√† Topic Clusters (C·ª•m ch·ªß ƒë·ªÅ)",
        "Copywriting th√¥i mi√™n: C√°c m·∫´u c√¢u ch·ªët sale t√¢m l√Ω h·ªçc",
        "Chi·∫øn l∆∞·ª£c n·ªôi dung ƒëa k√™nh (Omnichannel Content Strategy)"
    ],

    # === NH√ìM 5: NGHI·ªÜP V·ª§ B·ªî TR·ª¢ (SUPPORT) ===
    "[LEGAL]": [
        "Khung ph√°p l√Ω v·ªÅ AI v√† b·∫£n quy·ªÅn t√°c gi·∫£ to√†n c·∫ßu",
        "H·ª£p ƒë·ªìng th√¥ng minh (Smart Contract) v√† t√≠nh ph√°p l√Ω",
        "Tu√¢n th·ªß GDPR v√† Ngh·ªã ƒë·ªãnh 13 b·∫£o v·ªá d·ªØ li·ªáu t·∫°i Vi·ªát Nam",
        "Gi·∫£i quy·∫øt tranh ch·∫•p th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠ xuy√™n bi√™n gi·ªõi"
    ],
    "[RESEARCH]": [
        "Xu h∆∞·ªõng c√¥ng ngh·ªá sinh h·ªçc (Biotech) k·∫øt h·ª£p AI",
        "V·∫≠t li·ªáu m·ªõi (Graphene, Carbon Nanotubes) trong c√¥ng nghi·ªáp",
        "T√°c ƒë·ªông c·ªßa 6G l√™n n·ªÅn kinh t·∫ø s·ªë t∆∞∆°ng lai",
        "Nghi√™n c·ª©u h√†nh vi ti√™u d√πng b·ªÅn v·ªØng (Sustainability)"
    ],
    "[SALES]": [
        "M√¥ h√¨nh b√°n h√†ng Challenger Sale (Ng∆∞·ªùi th√°ch th·ª©c)",
        "Account-Based Marketing (ABM) cho kh√°ch h√†ng doanh nghi·ªáp l·ªõn",
        "K·ªπ thu·∫≠t ƒë√†m ph√°n c·∫•p cao (High-stakes Negotiation)",
        "·ª®ng d·ª•ng CRM AI ƒë·ªÉ d·ª± ƒëo√°n t·ª∑ l·ªá ch·ªët ƒë∆°n (Win Rate Prediction)"
    ]
}

# 2. H√ÄM ƒê√ÄO T·∫†O CHUY√äN S√ÇU (PHI√äN B·∫¢N K·∫æ TH·ª™A - COST OPTIMIZED)
async def specialized_training_job(role_tag: str):
    """
    PHI√äN B·∫¢N 10.0: COST-OPTIMIZED INHERITANCE (QUY T·∫ÆC K·∫æ TH·ª™A & TI·∫æT KI·ªÜM)
    - Nguy√™n t·∫Øc: "Kh√¥ng mua l·∫°i nh·ªØng g√¨ ƒë√£ c√≥".
    """
    print(colored(f"üõ°Ô∏è [INHERITANCE CHECK] {role_tag} ƒëang ki·ªÉm tra kho tri th·ª©c...", "cyan", attrs=["bold"]))
    
    topics = CURRICULUM.get(role_tag, [])
    if not topics: return

    try:
        # A. L·∫§Y XP HI·ªÜN T·∫†I (D√πng SQLite tr·ª±c ti·∫øp cho nhanh, kh√¥ng c·∫ßn db_manager ph·ª©c t·∫°p)
        db_path = "/var/data/ai_corp_projects.db" if os.path.exists("/var/data") else "ai_corp_projects.db"
        conn = sqlite3.connect(db_path, timeout=10)
        c = conn.cursor()
        
        c.execute("SELECT xp FROM agent_status WHERE role_tag = ?", (role_tag,))
        row = c.fetchone()
        current_xp = row[0] if row else 0
        conn.close()

        # Ch·ªçn ch·ªß ƒë·ªÅ d·ª±a tr√™n Level (C·ª© 50 XP ƒë·ªïi 1 b√†i)
        topic_index = int(current_xp / 50) % len(topics)
        current_topic = topics[topic_index]
        
        # B. KI·ªÇM TRA K·∫æ TH·ª™A (QUAN TR·ªåNG)
        existing_knowledge = ""
        is_found = False
        
        # T√¨m trong Vector DB
        try:
            results = vector_db.similarity_search(current_topic, k=1)
            if results:
                existing_knowledge = results[0].page_content
                is_found = True
                print(colored(f"üí° [FOUND] ƒê√£ t√¨m th·∫•y ki·∫øn th·ª©c c≈©: {current_topic}", "green"))
        except: pass

        # C. QUY·∫æT ƒê·ªäNH CHI·∫æN L∆Ø·ª¢C
        final_output = ""
        xp_earned = 0
        mode = "UNKNOWN"

        # --- NH√ÅNH 1: K·∫æ TH·ª™A (REVIEW MODE) - MI·ªÑN PH√ç ---
        if is_found and existing_knowledge:
            mode = "REVIEW"
            print(colored("--> Ch·∫ø ƒë·ªô: REVIEW (√în t·∫≠p) - Ti·∫øt ki·ªám ti·ªÅn.", "yellow"))
            
            if LLM_GEMINI_LOGIC:
                review_prompt = f"""
                B·∫°n l√† {role_tag}. H√£y √¥n t·∫≠p l·∫°i ki·∫øn th·ª©c c≈© n√†y:
                ---
                {existing_knowledge[:2000]}
                ---
                Y√™u c·∫ßu: T√≥m t·∫Øt l·∫°i v√† ƒë·ªÅ xu·∫•t 1 √Ω t∆∞·ªüng m·ªõi t·ª´ n√≥.
                """
                res = await LLM_GEMINI_LOGIC.ainvoke(review_prompt)
                final_output = res.content
                xp_earned = 20 # ƒêi·ªÉm th·∫•p h∆°n v√¨ ch·ªâ √¥n t·∫≠p
            else:
                final_output = existing_knowledge

        # --- NH√ÅNH 2: NGHI√äN C·ª®U M·ªöI (RESEARCH MODE) - T·ªêN TI·ªÄN ---
        else:
            mode = "RESEARCH"
            print(colored("--> Ch·∫ø ƒë·ªô: RESEARCH (Nghi√™n c·ª©u m·ªõi) - G·ªçi Search API.", "magenta"))
            
            raw_data = ""
            # ∆Øu ti√™n Perplexity -> DeepSeek -> Gemini
            if LLM_PERPLEXITY:
                res = await LLM_PERPLEXITY.ainvoke(f"Nghi√™n c·ª©u m·ªõi nh·∫•t v·ªÅ: {current_topic}")
                raw_data = res.content
            elif LLM_DEEPSEEK:
                res = await LLM_DEEPSEEK.ainvoke(f"Ki·∫øn th·ª©c chuy√™n s√¢u v·ªÅ: {current_topic}")
                raw_data = res.content
            
            final_output = raw_data
            xp_earned = 50 # ƒêi·ªÉm cao

        # D. L∆ØU K·∫æT QU·∫¢ & C·ªòNG ƒêI·ªÇM
        # 1. L∆∞u v√†o n√£o (Vector DB)
        if mode == "RESEARCH" and final_output:
            vector_db.add_texts(
                texts=[final_output],
                metadatas=[{"source": "Auto_Train", "agent": role_tag, "topic": current_topic}]
            )

        # 2. Ghi s·ªï c√¥ng vi·ªác (ƒê·ªÉ hi·ªán l√™n Dashboard v√† c·ªông XP)
        # S·ª≠ d·ª•ng h√†m log_work_to_db c√≥ s·∫µn trong main.py
        clean_name = role_tag.replace("[","").replace("]","")
        log_work_to_db(
            agent=clean_name,
            task=f"ƒê√†o t·∫°o: {current_topic}",
            result=f"[{mode}] {final_output[:100]}...",
            tool=f"Auto-{mode}",
            xp_bonus=xp_earned
        )

    except Exception as e:
        print(colored(f"‚ùå L·ªói ƒë√†o t·∫°o {role_tag}: {e}", "red"))


# 2. H√ÄM CH·∫§M ƒêI·ªÇM CH·∫§T L∆Ø·ª¢NG
async def evaluate_quality(agent_name, content):
    """Gi√°m kh·∫£o AI ch·∫•m ƒëi·ªÉm n·ªôi dung h·ªçc (1-10)"""
    prompt = f"Ch·∫•m ƒëi·ªÉm n·ªôi dung c·ªßa {agent_name} (Thang 1-10). N·ªôi dung: {content[:500]}..."
    try:
        model = LLM_DEEPSEEK if LLM_DEEPSEEK else LLM_GPT4
        score_msg = await model.ainvoke(prompt)
        score = int(re.search(r'\d+', score_msg.content).group())
        return min(max(score, 1), 10)
    except: return 5


# Bi·∫øn to√†n c·ª•c ƒë·ªÉ Server c√≥ th·ªÉ set tr·∫°ng th√°i b·∫≠n
IS_SYSTEM_BUSY = False 
LAST_INTERACTION_TIME = datetime.now()
# 3. V√íNG L·∫∂P T·ª∞ H·ªåC (AUTO LEARNING CYCLE)
async def auto_learning_cycle():
    """
    ƒê·ªòNG C∆† T·ª∞ H·ªåC Vƒ®NH C·ª¨U (Smart Scheduler)
    - Lu√¢n phi√™n ƒë√°nh th·ª©c Agent ƒëi h·ªçc (specialized_training_job).
    - T·ª± ƒë·ªông ng·∫Øt khi CEO c·∫ßn d√πng h·ªá th·ªëng (Busy Check).
    """
    global IS_SYSTEM_BUSY, LAST_INTERACTION_TIME
    
    print(colored("üéì [SCHEDULER] K√≠ch ho·∫°t H·ªçc vi·ªán Agent T·ª± ƒë·ªông...", "magenta", attrs=["bold"]))
    
    # Danh s√°ch h·ªçc vi√™n
    agents_queue = list(CURRICULUM.keys())
    idx = 0

    while True:
        # --- B∆Ø·ªöC 1: KI·ªÇM TRA TR·∫†NG TH√ÅI B·∫¨N R·ªòN ---
        # N·∫øu v·ª´a c√≥ l·ªánh trong 5 ph√∫t qua -> Coi l√† b·∫≠n
        idle_seconds = (datetime.now() - LAST_INTERACTION_TIME).total_seconds()
        
        if IS_SYSTEM_BUSY or idle_seconds < 300: # 5 ph√∫t
            # print("üöß H·ªá th·ªëng ƒëang b·∫≠n. T·∫°m ho√£n h·ªçc t·∫≠p.", end="\r")
            await asyncio.sleep(60) # Ch·ªù 1 ph√∫t r·ªìi check l·∫°i
            continue

        # --- B∆Ø·ªöC 2: B·∫ÆT ƒê·∫¶U CA H·ªåC ---
        current_agent = agents_queue[idx % len(agents_queue)]
        idx += 1
        
        print(colored(f"\nüîî [DING] H·ªá th·ªëng r·∫£nh. ƒê√°nh th·ª©c {current_agent} ƒëi h·ªçc...", "magenta"))
        
        try:
            # G·ªçi h√†m ƒë√†o t·∫°o chuy√™n s√¢u (ƒë√£ c√≥ logic K·∫ø th·ª´a & C·ªông ƒëi·ªÉm)
            await specialized_training_job(current_agent)
            
            # H·ªçc xong 1 ng∆∞·ªùi -> Ngh·ªâ gi·∫£i lao d√†i (ƒë·ªÉ kh√¥ng spam API li√™n t·ª•c)
            # Ch·∫°y th·∫≠t: Ngh·ªâ 30-60 ph√∫t
            # Ch·∫°y test: Ngh·ªâ 60 gi√¢y
            print(colored(f"üí§ {current_agent} ƒë√£ h·ªçc xong. H·ªá th·ªëng ngh·ªâ gi·∫£i lao.", "dark_grey"))
            await asyncio.sleep(300) 

        except Exception as e:
            print(colored(f"‚ö†Ô∏è L·ªói Scheduler: {e}", "red"))
            await asyncio.sleep(60) # L·ªói th√¨ ngh·ªâ t√≠ r·ªìi th·ª≠ ng∆∞·ªùi kh√°c
       

def set_system_busy():
    """H√†m ƒë·ªÉ Server g·ªçi m·ªói khi c√≥ tin nh·∫Øn t·ª´ CEO"""
    global IS_SYSTEM_BUSY, LAST_INTERACTION_TIME
    IS_SYSTEM_BUSY = True
    LAST_INTERACTION_TIME = datetime.now()
    # Sau m·ªôt kho·∫£ng th·ªùi gian, c√≥ th·ªÉ set l·∫°i False ho·∫∑c d·ª±a v√†o idle time
# 4. JOB B√ÅO C√ÅO S√ÅNG (D√ôNG LOGIC M·ªöI)
# 4. JOB B√ÅO C√ÅO S√ÅNG (B·∫¢N H·ª¢P NH·∫§T: K·∫æ TH·ª™A + L∆ØU TR·ªÆ CHUY√äN NGHI·ªÜP)
async def morning_briefing_job():
    """
    PHI√äN B·∫¢N 4.0: H·ª¢P NH·∫§T TINH HOA
    - L√µi t√¨m ki·∫øm: D√πng logic K·∫ø th·ª´a (specialized_training_job) ƒë·ªÉ ti·∫øt ki·ªám ti·ªÅn.
    - ƒê·∫ßu ra: V·∫´n t·∫°o file b√°o c√°o, l∆∞u DB Projects v√† c·∫≠p nh·∫≠t Meta-Cognition nh∆∞ b·∫£n 3.0.
    """
    role_tag = "[ORCHESTRATOR]"
    print(colored(f"\n‚è∞ [CRON JOB] {role_tag} b·∫Øt ƒë·∫ßu t·ªïng h·ª£p tin t·ª©c s√°ng...", "cyan", attrs=["bold"]))
    
    # L·∫•y ch·ªß ƒë·ªÅ c·∫ßn ƒë·ªçc
    topics = CURRICULUM.get(role_tag, ["Tin t·ª©c AI m·ªõi nh·∫•t", "Th·ªã tr∆∞·ªùng c√¥ng ngh·ªá 2026"])
    report_buffer = []
    
    # --- PH·∫¶N 1: THU TH·∫¨P D·ªÆ LI·ªÜU (D√πng logic K·∫ø th·ª´a) ---
    for topic in topics:
        try:
            print(colored(f"--> ƒêang qu√©t: {topic}...", "white"))
            
            # Thay v√¨ g·ªçi Perplexity tr·ª±c ti·∫øp, ta ki·ªÉm tra Vector DB tr∆∞·ªõc (Logic K·∫ø th·ª´a)
            # 1. T√¨m trong n√£o tr∆∞·ªõc
            existing_knowledge = ""
            try:
                results = vector_db.similarity_search(topic, k=1)
                if results: existing_knowledge = results[0].page_content
            except: pass

            content = ""
            source_note = ""

            # 2. Quy·∫øt ƒë·ªãnh: D√πng c≈© hay Mua m·ªõi?
            # N·∫øu c√≥ tin c≈© (coi nh∆∞ l√† tin h√¥m qua), ta v·∫´n c·∫ßn update tin m·ªõi cho "B√°o c√°o s√°ng"
            # TUY NHI√äN, ƒë·ªÉ ti·∫øt ki·ªám, ta c√≥ th·ªÉ d√πng Gemini ƒë·ªÉ "rewrite" tin c≈© n·∫øu ch∆∞a mu·ªën t·ªën ti·ªÅn search
            # Nh∆∞ng v·ªõi B√°o c√°o s√°ng, CEO th∆∞·ªùng c·∫ßn tin M·ªöI NH·∫§T.
            # -> Chi·∫øn l∆∞·ª£c: N·∫øu tin trong DB m·ªõi update < 24h th√¨ d√πng l·∫°i. N·∫øu c≈© h∆°n th√¨ Search m·ªõi.
            
            # (·ªû ƒë√¢y ƒë·ªÉ ƒë∆°n gi·∫£n v√† ch·∫Øc ch·∫Øn c√≥ tin m·ªõi, ta ∆∞u ti√™n Search Perplexity n·∫øu c√≥)
            if LLM_PERPLEXITY:
                res = await LLM_PERPLEXITY.ainvoke(f"Tin t·ª©c m·ªõi nh·∫•t 24h qua v·ªÅ: {topic}")
                content = res.content
                source_note = "(Ngu·ªìn: Perplexity Live)"
            elif existing_knowledge:
                content = existing_knowledge
                source_note = "(Ngu·ªìn: K√Ω ·ª©c n·ªôi b·ªô)"
            else:
                content = "Kh√¥ng t√¨m th·∫•y th√¥ng tin m·ªõi."

            # L∆∞u l·∫°i v√†o b·ªô ƒë·ªám b√°o c√°o
            report_buffer.append(f"### {topic} {source_note}\n{content[:1000]}...\n")
            
            # Ghi nh·ªõ v√†o Vector DB (ƒë·ªÉ d√†nh cho l·∫ßn sau)
            if vector_db and "Perplexity" in source_note:
                await asyncio.to_thread(
                    vector_db.add_texts,
                    texts=[content],
                    metadatas=[{"source": "Morning_Briefing", "agent": role_tag, "topic": topic, "date": datetime.now().isoformat()}]
                )

        except Exception as e:
            print(colored(f"‚ö†Ô∏è L·ªói ƒë·ªçc tin '{topic}': {e}", "yellow"))

    # --- PH·∫¶N 2: L∆ØU TR·ªÆ & B√ÅO C√ÅO (Logic 3.0 x·ªãn x√≤ c·ªßa Ng√†i) ---
    if report_buffer:
        today_str = datetime.now().strftime("%Y-%m-%d")
        full_content = f"# üåÖ B·∫¢N TIN S√ÅNG {today_str}\n\n" + "\n\n".join(report_buffer)
        report_id = f"BRIEFING_{datetime.now().strftime('%Y%m%d')}"

        try:
            # S·ª≠ d·ª•ng k·∫øt n·ªëi DB tr·ª±c ti·∫øp (tr√°nh ph·ª• thu·ªôc db_manager c·ªßa server)
            db_path = "/var/data/ai_corp_projects.db" if os.path.exists("/var/data") else "ai_corp_projects.db"
            conn = sqlite3.connect(db_path)
            c = conn.cursor()

            # 1. L∆∞u v√†o b·∫£ng Projects (ƒê·ªÉ hi·ªán l√™n Dashboard)
            history_json = json.dumps([{"type": "ai", "data": {"content": full_content}}])
            
            c.execute("DELETE FROM projects WHERE id = ?", (report_id,))
            c.execute("""
                INSERT INTO projects (id, name, history, timestamp)
                VALUES (?, ?, ?, ?)
            """, (report_id, f"B√°o c√°o s√°ng {today_str}", history_json, datetime.now()))

            # 2. C·ªông ƒëi·ªÉm XP (Gamification)
            # L·∫•y XP c≈©
            c.execute("SELECT xp FROM agent_status WHERE role_tag = ?", (role_tag,))
            row = c.fetchone()
            new_xp = (row[0] if row else 0) + 100
            
            # Update tr·∫°ng th√°i
            c.execute("DELETE FROM agent_status WHERE role_tag = ?", (role_tag,))
            c.execute("""
                INSERT INTO agent_status (role_tag, xp, current_topic, last_updated) 
                VALUES (?, ?, ?, ?)
            """, (role_tag, new_xp, f"Ho√†n th√†nh b·∫£n tin {today_str}", datetime.now()))

            # 3. Ghi Nh·∫≠t k√Ω T·ª± nh·∫≠n th·ª©c (Meta-Cognition)
            c.execute("""
                INSERT INTO learning_logs (event_type, content, agent_name, timestamp)
                VALUES (?, ?, ?, ?)
            """, ("CREATED", f"ƒê√£ xu·∫•t b·∫£n B·∫£n tin s√°ng {today_str}.", role_tag, datetime.now()))

            conn.commit()
            conn.close()
            print(colored(f"‚úÖ [DATABASE] ƒê√£ l∆∞u b√°o c√°o s√°ng v√† c·ªông 100 XP cho {role_tag}!", "green"))

        except Exception as e:
            print(colored(f"‚ùå L·ªói L∆∞u Tr·ªØ Job S√°ng: {e}", "red"))

# ============================================================================
# 7. KH·ªûI CH·∫†Y TH·ª∞C T·∫æ
# ============================================================================
if __name__ == "__main__":
    try:
        # Ch·∫°y v√≤ng l·∫∑p ch√≠nh th√¥ng qua asyncio
        asyncio.run(main_loop())
    except KeyboardInterrupt:
        print("\nüëã ƒê√£ tho√°t h·ªá th·ªëng.")
