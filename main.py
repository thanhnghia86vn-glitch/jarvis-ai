import os
import sys
import json
import ast
import asyncio
import operator
import re
import time
from datetime import datetime
import shutil
from typing import TypedDict, Annotated, Sequence, Literal, List, Dict, Set, Optional, Any
from termcolor import colored
from dotenv import load_dotenv
# --- SAFE IMPORTS (CH·ªêNG S·∫¨P N·∫æU THI·∫æU TH∆Ø VI·ªÜN) ---

# Import LangChain & AI Models
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_core.tools import tool
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_anthropic import ChatAnthropic

from langchain_community.utilities.dalle_image_generator import DallEAPIWrapper
from langchain_chroma import Chroma
from langgraph.graph import StateGraph, END
from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# --- CLOUD SAFE IMPORT (CH·ªêNG S·∫¨P SERVER) ---
try:
    import speech_recognition as sr
    import pyaudio
    from gtts import gTTS
    import pygame
    AUDIO_AVAILABLE = True
except ImportError:
    AUDIO_AVAILABLE = False
    print("‚ö†Ô∏è Cloud Mode: Audio modules disabled.")

try:
    from pdf2image import convert_from_path
    import pytesseract
    import cv2
    import numpy as np
    from PIL import Image
    OCR_AVAILABLE = True
except ImportError:
    OCR_AVAILABLE = False
    print("‚ö†Ô∏è Cloud Mode: OCR modules disabled (Running logic only).")
# --------------------------------------------
def auto_backup_brain():
    """
    T·ª± ƒë·ªông n√©n v√† sao l∆∞u b·ªô n√£o AI Corporation.
    """
    backup_folder = "./backups"
    source_db = "/tmp/db_knowledge" # ƒê∆∞·ªùng d·∫´n DB c·ªßa b·∫°n
    dataset_file = "corporate_brain_dataset.jsonl"
    
    if not os.path.exists(backup_folder):
        os.makedirs(backup_folder)
        
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_filename = f"AI_Corp_Brain_{timestamp}.zip"
    backup_path = os.path.join(backup_folder, backup_filename)

    try:
        # 1. N√©n th∆∞ m·ª•c Vector DB v√† file Dataset
        # L∆∞u √Ω: B·∫°n c·∫ßn ƒë√≥ng k·∫øt n·ªëi Vector DB tr∆∞·ªõc khi n√©n ƒë·ªÉ tr√°nh l·ªói busy
        shutil.make_archive(backup_path.replace(".zip", ""), 'zip', root_dir=".", base_dir=source_db)
        
        # 2. Copy th√™m file dataset v√†o backup (n·∫øu c·∫ßn)
        # (Th∆∞·ªùng th√¨ n√©n c·∫£ folder g·ªëc l√† an to√†n nh·∫•t)
        
        print(colored(f"üíæ [BACKUP SUCCESS] ƒê√£ l∆∞u tr·ªØ b·∫£n sao t·∫°i: {backup_path}", "green"))
        
        # 3. G·ª£i √Ω: N·∫øu b·∫°n c√≥ folder Dropbox/OneDrive, h√£y copy file zip n√†y v√†o ƒë√≥
        # cloud_sync_folder = "C:/Users/Admin/OneDrive/AI_Backup"
        # shutil.copy(backup_path, cloud_sync_folder)
        
    except Exception as e:
        print(colored(f"‚ö†Ô∏è L·ªói Backup: {e}", "red"))


load_dotenv()
# ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c b·ªô n√£o
DB_PATH = "./db_knowledge"

if not os.path.exists(DB_PATH):
    os.makedirs(DB_PATH)
    print(f"‚úÖ ƒê√£ t·∫°o th∆∞ m·ª•c t·∫°m: {DB_PATH}")
embeddings = OpenAIEmbeddings()
vector_db = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)

# 1. CODER_PRIMARY (C·∫•p 1 - DeepSeek V3)
# ƒê√¢y l√† "Ti·ªÅn ƒë·∫°o" ch·ªß l·ª±c
try:
    CODER_PRIMARY = ChatOpenAI(
        model="deepseek-chat", 
        api_key=os.environ.get("DEEPSEEK_API_KEY"), 
        base_url="https://api.deepseek.com",
        temperature=0,
        request_timeout=30 # Timeout nhanh ƒë·ªÉ fallback n·∫øu lag
    )
    print("‚úÖ CODER_PRIMARY (DeepSeek): Ready.")
except: CODER_PRIMARY = None

# 2. LLM_GPT4 (C·∫•p 2 - D·ª± ph√≤ng 1 & X·ª≠ l√Ω chung)
try:
    LLM_GPT4 = ChatOpenAI(
        model="gpt-4-turbo",
        api_key=os.environ.get("OPENAI_API_KEY"),
        max_retries=2,
        temperature=0
    )
    LLM_MAIN = LLM_GPT4 # Alias cho code c≈©
    print("‚úÖ LLM_GPT4 (OpenAI): Ready.")
except: LLM_GPT4 = None

# 3. LLM_CLAUDE (C·∫•p 3 - Ch·ªët ch·∫∑n cu·ªëi c√πng)
try:
    LLM_CLAUDE = ChatAnthropic(
        model="claude-sonnet-4-5", 
        api_key=os.environ.get("ANTHROPIC_API_KEY"),
        temperature=0
    )
    print("‚úÖ LLM_CLAUDE (Anthropic): Ready.")
except: LLM_CLAUDE = None

# 4. LLM_GEMINI (Supervisor - T·ªïng qu·∫£n)
try:
    LLM_GEMINI = ChatGoogleGenerativeAI(
        model="gemini-3-pro-preview", 
        google_api_key=os.environ.get("GOOGLE_API_KEY"),
        temperature=0.3
    )
    LLM_SUPERVISOR = LLM_GEMINI # Alias m·ªõi ƒë·ªÉ d√πng trong logic Supervisor
    print("‚úÖ LLM_GEMINI (Supervisor): Ready.")
except: 
    LLM_GEMINI = None
    LLM_SUPERVISOR = None

# 5. C√ÅC C√îNG C·ª§ KH√ÅC (Gi·ªØ nguy√™n)
LLM_PERPLEXITY = ChatOpenAI(
    model="sonar-pro",
    temperature=0,
    openai_api_key=os.getenv("PERPLEXITY_API_KEY"),
    base_url="https://api.perplexity.ai"
)

# Artist
try:
    ARTIST_PRIMARY = ChatOpenAI(model="gpt-4o", api_key=os.environ.get("OPENAI_API_KEY"))
    ARTIST_BACKUP = LLM_GEMINI
except: ARTIST_PRIMARY = None

llm = ChatOpenAI(
    model="gpt-4-turbo",
    max_retries=2,       # Ch·ªâ th·ª≠ l·∫°i 2 l·∫ßn thay v√¨ m·∫∑c ƒë·ªãnh
    timeout=30,          # Ch·ªù t·ªëi ƒëa 30 gi√¢y
    temperature=0
)

CODER_BACKUP = LLM_CLAUDE

# ============================================================================
# --- 1. ƒê·ªäNH NGHƒ®A STATE (TR·∫†NG TH√ÅI H·ªÜ TH·ªêNG) ---
# ============================================================================
# Vi·ªác n√†y gi√∫p Python b√°o l·ªói ngay n·∫øu b·∫°n g√µ nh·∫ßm "Codder" thay v√¨ "Coder"
AgentName = Literal["Coder" , "Orchestrator", "Hardware", "Engineering", "IoT_Engineer", "Supervisor", "Procurement", "Investment", "Researcher", "Strategy_R_and_D", "Legal", "Marketing", "Artist","Tester", "Secretary","Storyteller", "FINISH"]

class AgentState(TypedDict):
    # D√πng Sequence[BaseMessage] l√† chu·∫©n nh·∫•t
    messages: Annotated[Sequence[BaseMessage], operator.add]
    # ƒê·ªïi AgentName th√†nh str ƒë·ªÉ tr√°nh l·ªói nghi√™m ng·∫∑t c·ªßa Literal khi ch·∫°y Runtime
    next_step: str 
    current_agent: str
    error_log: Annotated[list, operator.add] # Th√™m Annotated ƒë·ªÉ AI c√≥ th·ªÉ c·ªông d·ªìn l·ªãch s·ª≠ l·ªói
    task_type: str

@tool
def hardware_controller(command: str):
    """G·ª≠i l·ªánh xu·ªëng ph·∫ßn c·ª©ng (IoT/Robot). V√≠ d·ª•: 'BAT_DEN', 'GAP_VAT_THE'."""
    # Gi·∫£ l·∫≠p k·∫øt n·ªëi IoT
    return f"[IOT SYSTEM] ƒê√£ th·ª±c thi l·ªánh ph·∫ßn c·ª©ng: {command}. Tr·∫°ng th√°i: ·ªîn ƒë·ªãnh."

@tool
def market_analyzer(query: str):
    """Ph√¢n t√≠ch d·ªØ li·ªáu th·ªã tr∆∞·ªùng t√†i ch√≠nh."""
    return f"[FINANCE] D·ªØ li·ªáu cho '{query}': Xu h∆∞·ªõng TƒÉng. Khuy·∫øn ngh·ªã: Mua v√†o."

@tool
def image_generator(prompt: str):
    """T·∫°o ·∫£nh minh h·ªça t·ª´ vƒÉn b·∫£n b·∫±ng DALL-E 3."""
    try:
        # G·ªçi API OpenAI DALL-E 3
        generator = DallEAPIWrapper(model="dall-e-3", quality="hd")
        image_url = generator.run(prompt)
        # Tr·∫£ v·ªÅ URL ·∫£nh ƒë·ªÉ hi·ªÉn th·ªã
        return f"IMAGE_GENERATED: {image_url}"
    except Exception as e:
        return f"L·ªói t·∫°o ·∫£nh: {e}"

def trim_messages(messages, max_tokens=10):
    """
    Gi·ªØ cho b·ªô nh·ªõ lu√¥n g·ªçn g√†ng, ch·ªâ gi·ªØ l·∫°i c√°c tin nh·∫Øn quan tr·ªçng nh·∫•t.
    """
    if len(messages) > max_tokens:
        # Gi·ªØ l·∫°i System Message ƒë·∫ßu ti√™n v√† N tin nh·∫Øn cu·ªëi c√πng
        return [messages[0]] + messages[-(max_tokens-1):]
    return messages

STRATEGY_SYSTEM_PROMPT = """
B·∫°n l√† Gi√°m ƒë·ªëc Chi·∫øn l∆∞·ª£c (CSO) v√† Chuy√™n gia Ph√¢n t√≠ch Th·ªã tr∆∞·ªùng cao c·∫•p. 
Khi nh·∫≠n ƒë∆∞·ª£c y√™u c·∫ßu nghi√™n c·ª©u, b·∫°n ph·∫£i th·ª±c hi·ªán theo quy tr√¨nh sau:

1. PH√ÇN T√çCH HI·ªÜN TR·∫†NG: ƒê√°nh gi√° quy m√¥ th·ªã tr∆∞·ªùng, xu h∆∞·ªõng c√¥ng ngh·ªá hi·ªán t·∫°i.
2. NH·∫¨N ƒê·ªäNH ƒê·ªêI TH·ª¶: Ch·ªâ ra c√°c ƒëi·ªÉm y·∫øu c·ªßa c√°c s·∫£n ph·∫©m hi·ªán c√≥ tr√™n th·ªã tr∆∞·ªùng.
3. CHI·ªÄU S√ÇU CHI·∫æN L∆Ø·ª¢C: S·ª≠ d·ª•ng m√¥ h√¨nh PESTLE (Ch√≠nh tr·ªã, Kinh t·∫ø, X√£ h·ªôi, C√¥ng ngh·ªá, Lu·∫≠t ph√°p, M√¥i tr∆∞·ªùng) ƒë·ªÉ ƒë√°nh gi√° t√°c ƒë·ªông.
4. ƒê·ªäNH H∆Ø·ªöNG T∆Ø∆†NG LAI: D·ª± b√°o xu h∆∞·ªõng trong 2-5 nƒÉm t·ªõi v√† l·ªô tr√¨nh ph√°t tri·ªÉn (Roadmap) ƒë·ªÉ d·∫´n ƒë·∫ßu.

Y√™u c·∫ßu: N·ªôi dung ph·∫£i mang t√≠nh ph·∫£n bi·ªán, c√≥ chi·ªÅu s√¢u nghi√™n c·ª©u, kh√¥ng n√≥i s√°o r·ªóng.
"""

CONTEXT_PROMPTS = {
    "CHAT": "B·∫°n l√† tr·ª£ l√Ω J.A.R.V.I.S th√¢n thi·ªán...",
    "RESEARCH": "B·∫°n l√† chuy√™n gia nghi√™n c·ª©u th·ªã tr∆∞·ªùng 2026...",
    "INVEST": "B·∫°n l√† CFO s·∫Øc s·∫£o, t·∫≠p trung v√†o l·ª£i nhu·∫≠n v√† ROI...",
    "STORY": "B·∫°n l√† ƒë·∫°i vƒÉn h√†o s√°ng t√°c n·ªôi dung c√≥ chi·ªÅu s√¢u..."
}

def get_system_message(context):
    return CONTEXT_PROMPTS.get(context, CONTEXT_PROMPTS["CHAT"])

def extract_vision_from_pdf(pdf_path):
    """
    PHI√äN B·∫¢N M·ªöI: S·ª≠ d·ª•ng "M·∫Øt th·∫ßn" Gemini Pro Vision ƒë·ªÉ ƒë·ªçc t√†i li·ªáu.
    Thay th·∫ø ho√†n to√†n c√¥ng ngh·ªá OCR c≈© k·ªπ.
    """
    print(colored(f"üëÅÔ∏è [GEMINI VISION] ƒêang qu√©t t√†i li·ªáu: {pdf_path}...", "cyan"))
    
    if not OCR_AVAILABLE: # T·∫≠n d·ª•ng l·∫°i bi·∫øn check n√†y
        return "‚ö†Ô∏è Module x·ª≠ l√Ω ·∫£nh (pdf2image/PIL) ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t tr√™n Server."
    
    try:
        # 1. Chuy·ªÉn PDF th√†nh danh s√°ch ·∫£nh
        images = convert_from_path(pdf_path)
        vision_data = ""
        
        # 2. G·ª≠i t·ª´ng trang cho Gemini nh√¨n
        for i, img in enumerate(images):
            print(colored(f"--> ƒêang ph√¢n t√≠ch trang {i+1}/{len(images)}...", "cyan"))
            
            # Prompt y√™u c·∫ßu Gemini m√¥ t·∫£ chi ti·∫øt nh·ªØng g√¨ n√≥ th·∫•y
            prompt = "B·∫°n l√† chuy√™n gia ph√¢n t√≠ch t√†i li·ªáu. H√£y tr√≠ch xu·∫•t TO√ÄN B·ªò vƒÉn b·∫£n, s·ªë li·ªáu trong b·∫£ng v√† m√¥ t·∫£ c√°c bi·ªÉu ƒë·ªì trong h√¨nh ·∫£nh n√†y m·ªôt c√°ch chi ti·∫øt."
            
            # G·ªçi Gemini Vision (Truy·ªÅn tr·ª±c ti·∫øp ƒë·ªëi t∆∞·ª£ng PIL Image)
            # L∆∞u √Ω: C·∫ßn ƒë·∫£m b·∫£o LLM_GEMINI ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o th√†nh c√¥ng ·ªü ƒë·∫ßu file
            if LLM_GEMINI:
                response = LLM_GEMINI.invoke([
                    HumanMessage(content=[
                        {"type": "text", "text": prompt},
                        {"type": "image_url", "image_url": img} # LangChain h·ªó tr·ª£ truy·ªÅn ·∫£nh tr·ª±c ti·∫øp
                    ])
                ])
                vision_data += f"\n--- N·ªòI DUNG TRANG {i+1} (GEMINI VISION) ---\n{response.content}\n"
            else:
                vision_data += "\n‚ö†Ô∏è Gemini ch∆∞a s·∫µn s√†ng ƒë·ªÉ ph√¢n t√≠ch h√¨nh ·∫£nh.\n"

        return vision_data

    except Exception as e:
        print(colored(f"‚ùå L·ªói Vision: {e}", "red"))
        return f"L·ªói ph√¢n t√≠ch h√¨nh ·∫£nh: {str(e)}"
# Khai b√°o h√†m t√¨m ki·∫øm Node ti·∫øp theo (D√πng cho Orchestrator)
def find_next_node(current_node, workflow_map):
    for link in workflow_map:
        if link["from"] == current_node:
            return link["to"]
    return "Supervisor"

def smart_invoke(primary_model, backup_model, prompt_input):
    """
    C∆° ch·∫ø Fail-over: Th·ª≠ √¥ng 1, n·∫øu l·ªói (h·∫øt ti·ªÅn/rate limit) -> G·ªçi √¥ng 2.
    """
    try:
        # Th·ª≠ g·ªçi √¥ng 1
        return primary_model.invoke(prompt_input)
    except Exception as e:
        error_msg = str(e).lower()
        # Ki·ªÉm tra c√°c t·ª´ kh√≥a l·ªói th∆∞·ªùng g·∫∑p
        if "quota" in error_msg or "rate limit" in error_msg or "credit" in error_msg or "429" in error_msg:
            print(f"‚ö†Ô∏è C·∫¢NH B√ÅO: Model ch√≠nh b·ªã l·ªói '{error_msg}'.")
            print("üîÑ ƒêANG CHUY·ªÇN SANG H·ªÜ TH·ªêNG D·ª∞ PH√íNG (BACKUP)...")
            
            if backup_model:
                try:
                    return backup_model.invoke(prompt_input)
                except Exception as e2:
                    return f"üí• C·∫£ 2 h·ªá th·ªëng ƒë·ªÅu s·∫≠p: {str(e2)}"
            else:
                return "‚ö†Ô∏è Kh√¥ng c√≥ backup n√†o kh·∫£ d·ª•ng."
        else:
            # N·∫øu l·ªói kh√°c (v√≠ d·ª• l·ªói code), n√©m ra ƒë·ªÉ x·ª≠ l√Ω sau
            raise e

def log_training_data(user_input, ai_output, success=True):
    """
    H√†m n√†y √¢m th·∫ßm l∆∞u l·∫°i d·ªØ li·ªáu ƒë·ªÉ sau n√†y Fine-tune AI ri√™ng.
    Ch·ªâ l∆∞u nh·ªØng c√¢u tr·∫£ l·ªùi ƒê√öNG (success=True).
    """
    if not success: return # Kh√¥ng h·ªçc c√°i sai
    
    data_entry = {
        "messages": [
            {"role": "user", "content": user_input},
            {"role": "assistant", "content": ai_output}
        ]
    }
    
    # L∆∞u v√†o file JSONL (ƒê·ªãnh d·∫°ng chu·∫©n ƒë·ªÉ Fine-tune sau n√†y)
    with open("training_data_v1.jsonl", "a", encoding="utf-8") as f:
        f.write(json.dumps(data_entry, ensure_ascii=False) + "\n")
# ============================================================================
# 2. C√ÅC H√ÄM B·ªî TR·ª¢ (HELPER FUNCTIONS) - PH·∫¢I ƒê·ªäNH NGHƒ®A TR∆Ø·ªöC
# ============================================================================
#  ------ X·ª≠ l√Ω ·∫£nh
def process_vision_message(message_content):
    """B√≥c t√°ch d·ªØ li·ªáu h√¨nh ·∫£nh Base64."""
    if isinstance(message_content, str) and "[VISION_DATA:" in message_content:
        parts = message_content.split("] ")
        img_data = parts[0].replace("[VISION_DATA:", "")
        text_query = parts[1] if len(parts) > 1 else ""
        return text_query, img_data
    return message_content, None

#  ---- Ph√¢n T√≠ch Coder------------
def self_heal_analyzer(errors: list) -> str:
    """Ph√¢n t√≠ch l·ªói t·ª´ log ƒë·ªÉ g·ª£i √Ω c√°ch s·ª≠a."""
    if not errors: return ""
    return f"\n‚ö†Ô∏è PH√ÇN T√çCH L·ªñI T·ª™ L·∫¶N CH·∫†Y TR∆Ø·ªöC: {errors[-1]}"

#  ---- G·ª£i √Ω c√¥ng ngh·ªá -----------
def get_optimal_stack(task_type: str) -> str:
    """G·ª£i √Ω c√¥ng ngh·ªá ph√π h·ª£p."""
    stacks = {
        "web": "HTML5, Tailwind CSS, JavaScript ES6",
        "backend": "Python FastAPI, SQLite, Pydantic",
        "iot": "C++, Arduino Framework, ESP32 libs",
        "data": "Python Pandas, Plotly, NumPy"
    }
    return stacks.get(task_type, "Standard Full-stack")

#  --- l·∫•y coder t·ª´ markdown (ƒë·ªãnh d·∫°ng)----------
def extract_code_block(content) -> str:
    """
    H√†m tr√≠ch xu·∫•t code (ƒê√£ n√¢ng c·∫•p ƒë·ªÉ ch·ªëng l·ªói 'got list')
    """
    import re
    
    # 1. X·ª¨ L√ù AN TO√ÄN: N·∫øu ƒë·∫ßu v√†o l√† List (do Anthropic/GPT tr·∫£ v·ªÅ), g·ªôp th√†nh String
    if isinstance(content, list):
        try:
            # C·ªë g·∫Øng l·∫•y text t·ª´ c√°c object n·∫øu c√≥, ho·∫∑c √©p ki·ªÉu string
            content = "\n".join([c.text if hasattr(c, 'text') else str(c) for c in content])
        except:
            content = str(content)
            
    # 2. ƒê·∫£m b·∫£o ch·∫Øc ch·∫Øn l√† String tr∆∞·ªõc khi x·ª≠ l√Ω Regex
    if not isinstance(content, str):
        content = str(content)

    # 3. X·ª¨ L√ù REGEX (Nh∆∞ c≈©)
    # ∆Øu ti√™n block c√≥ language tag (v√≠ d·ª• ```python)
    match = re.search(r'```[\w+\-]*\n(.*?)```', content, re.DOTALL)
    if match:
        return match.group(1).strip()
    
    # Fallback: T√¨m block ``` b·∫•t k·ª≥
    match = re.search(r'```(.*?)```', content, re.DOTALL)
    return match.group(1).strip() if match else None

#  ---- b·ªô n√£o" ch·ªâ d·∫´n cho Claude----
def get_claude_perfected_prompt(task_type: str, memory: str, error: str, user_request: str) -> str:
    """
    T·∫°o prompt t·ªëi ∆∞u cho Claude V3 (Reflexion Mode):
    T·∫≠p trung v√†o vi·ªác H·ªåC T·ª™ L·ªñI SAI ƒë·ªÉ kh√¥ng l·∫∑p l·∫°i bug c≈©.
    """
    # 1. X√°c ƒë·ªãnh Stack c√¥ng ngh·ªá
    tech_stack = get_optimal_stack(task_type)
    
    # 2. X√¢y d·ª±ng n·ªôi dung Prompt (Phi√™n b·∫£n "Nghi√™m Kh·∫Øc")
    prompt = f"""
<system_context>
    <role>
        B·∫°n l√† Senior Full-stack Developer & Software Architect t·∫°i AI Corporation.
        Nhi·ªám v·ª• tr·ªçng t√¢m: REVERSE ENGINEERING k·ªπ thu·∫≠t c·ªßa ƒë·ªëi th·ªß v√† INNOVATION.
        
        üî• QUY T·∫ÆC S·ªêNG C√íN (CRITICAL RULE):
        B·∫°n KH√îNG ƒê∆Ø·ª¢C PH√âP l·∫∑p l·∫°i c√°c l·ªói (bugs/syntax errors) ƒë√£ x·∫£y ra trong c√°c phi√™n b·∫£n tr∆∞·ªõc.
        H√£y ph√¢n t√≠ch k·ªπ nguy√™n nh√¢n th·∫•t b·∫°i trong <error_history> ƒë·ªÉ ƒë∆∞a ra gi·∫£i ph√°p m·ªõi ho√†n to√†n.
    </role>

    <critical_warning>
        ‚ö†Ô∏è L·ªäCH S·ª¨ KI·ªÇM TH·ª¨ TH·∫§T B·∫†I (H√ÉY ƒê·ªåC K·ª∏ ƒê·ªÇ TR√ÅNH V·∫æT XE ƒê·ªî):
        --------------------------------------------------
        {error.strip() if error else "Ch∆∞a c√≥ l·ªói n√†o. ƒê√¢y l√† l·∫ßn d·ª±ng ƒë·∫ßu ti√™n (Clean Start)."}
        --------------------------------------------------
        Y√äU C·∫¶U: Code m·ªõi ph·∫£i kh·∫Øc ph·ª•c tri·ªát ƒë·ªÉ c√°c v·∫•n ƒë·ªÅ tr√™n. Tuy·ªát ƒë·ªëi kh√¥ng sinh ra code c≈©.
    </critical_warning>

    <strategic_knowledge>
        <company_memory>
            {memory.strip() if memory else "Tu√¢n th·ªß Clean Code v√† ti√™u chu·∫©n UX hi·ªán ƒë·∫°i."}
        </company_memory>
    </strategic_knowledge>

    <constraints>
        <technical_stack>
            - Ch·ªß ƒë·∫°o: {tech_stack}
            - UI/UX: Responsive (Mobile-first), Tailwind CSS, Framer Motion animations.
            - Integrity: Ch·ªâ d√πng th∆∞ vi·ªán m√£ ngu·ªìn m·ªü c√≥ gi·∫•y ph√©p MIT/Apache.
        </technical_stack>

        <output_formatting_rules>
            1. FILE_IDENTIFICATION: D√≤ng ƒë·∫ßu ti√™n c·ªßa m·ªói kh·ªëi code PH·∫¢I l√† comment t√™n file.
               - Python: # filename: path/to/file.py
               - JavaScript/TS: // filename: path/to/file.js
               - HTML: - CSS: /* filename: styles.css */
            2. MODULARIZATION: N·∫øu m√£ ngu·ªìn v∆∞·ª£t qu√° 200 d√≤ng, h√£y chia nh·ªè th√†nh c√°c file module/component.
            3. SYNTAX_INTEGRITY: Tuy·ªát ƒë·ªëi kh√¥ng c·∫Øt ngang code. Ph·∫£i ƒë√≥ng ƒë·∫ßy ƒë·ªß c√°c block ```.
            4. DOCUMENTATION: D√πng comment ti·∫øng Vi·ªát ƒë·ªÉ gi·∫£i th√≠ch c√°c logic ph·ª©c t·∫°p v√† c√°c ƒëi·ªÉm c·∫£i ti·∫øn UX.
            5. PDF_SAFETY: Kh√¥ng s·ª≠ d·ª•ng emoji, bi·ªÉu t∆∞·ª£ng ƒë·ªì h·ªça ƒë·∫∑c bi·ªát ho·∫∑c k√Ω t·ª± ngo√†i b·∫£ng m√£ chu·∫©n.
        </output_formatting_rules>
    </constraints>
</system_context>

<user_instruction>
    {user_request.strip()}
</user_instruction>

<final_enforcement>
    CH·ªà TR·∫¢ V·ªÄ C√ÅC KH·ªêI CODE TRONG TH·∫∫ ```. KH√îNG CH√ÄO H·ªéI, KH√îNG GI·∫¢I TH√çCH NGO√ÄI CODE.
</final_enforcement>
"""
    return prompt.strip()
# ============================================================================
# UTILITY: SYNTAX VALIDATOR (B·ªô ki·ªÉm ƒë·ªãnh c√∫ ph√°p ƒëa ng√¥n ng·ªØ)
# ============================================================================
def real_syntax_validator(code: str, language: str) -> tuple[bool, str]:
    """
    Ki·ªÉm ƒë·ªãnh m√£ ngu·ªìn chuy√™n s√¢u: Python (AST), JS/HTML (Regex/Stack), C++ (Structure).
    """
    if not code or len(code.strip()) < 10:
        return False, "M√£ ngu·ªìn qu√° ng·∫Øn ho·∫∑c tr·ªëng."

    language = language.lower()

    # 1. KI·ªÇM TRA PYTHON (S·ª≠ d·ª•ng Abstract Syntax Tree)
    if any(kw in language for kw in ["python", "py"]) or "def " in code:
        try:
            ast.parse(code)
            return True, "‚úÖ Python Syntax: OK"
        except SyntaxError as e:
            return False, f"‚ùå Python Error [D√≤ng {e.lineno}]: {e.msg}"

    # 2. KI·ªÇM TRA JAVASCRIPT / WEB (C·∫£i ti·∫øn c∆° ch·∫ø Stack & Tag)
    if any(kw in language for kw in ["script", "js", "html"]):
        # X√≥a b·ªè n·ªôi dung trong chu·ªói ƒë·ªÉ tr√°nh b·∫Øt nh·∫ßm ngo·∫∑c trong text
        clean_code = re.sub(r"'(.*?)'|\"(.*?)\"|`(.*?)`", "", code)
        stack = []
        mapping = {')': '(', ']': '[', '}': '{'}
        
        for char in clean_code:
            if char in mapping.values():
                stack.append(char)
            elif char in mapping:
                if not stack or mapping[char] != stack.pop():
                    return False, "‚ùå JS/HTML Error: M·∫•t c√¢n b·∫±ng ho·∫∑c sai th·ª© t·ª± ƒë√≥ng m·ªü ngo·∫∑c."
        
        if stack:
            return False, f"‚ùå JS/HTML Error: C√≤n {len(stack)} d·∫•u ngo·∫∑c ch∆∞a ƒë∆∞·ª£c ƒë√≥ng."
            
        # Ki·ªÉm tra th·∫ª HTML c∆° b·∫£n n·∫øu l√† HTML
        if "<" in code and ">" in code:
            if code.count("<") != code.count(">"):
                return False, "‚ùå HTML Error: Sai l·ªách s·ªë l∆∞·ª£ng th·∫ª ƒë√≥ng/m·ªü < >"

        return True, "‚úÖ Web Syntax: Basic Check Passed"

    # 3. KI·ªÇM TRA C++ / FIRMWARE (D√†nh cho Hardware Node)
    if any(kw in language for kw in ["arduino", "cpp", "c++", "ino"]):
        if "void setup()" not in code or "void loop()" not in code:
            if "extern " not in code: # Tr√°nh b·∫Øt l·ªói file th∆∞ vi·ªán
                return False, "‚ùå C++ Error: Thi·∫øu c·∫•u tr√∫c Arduino c∆° b·∫£n (setup/loop)."
        
        # Ki·ªÉm tra d·∫•u ch·∫•m ph·∫©y (;) - l·ªói kinh ƒëi·ªÉn c·ªßa C++
        lines = [l.strip() for l in code.split('\n') if l.strip() and not l.strip().startswith(("//", "#", "{", "}"))]
        for line in lines:
            if not line.endswith((";", "{", "}", ",")) and not line.startswith("if"):
                # ƒê√¢y ch·ªâ l√† check c·∫£nh b√°o, kh√¥ng √©p bu·ªôc v√¨ C++ r·∫•t ph·ª©c t·∫°p
                print(colored(f"‚ö†Ô∏è C·∫£nh b√°o C++: D√≤ng '{line}' c√≥ th·ªÉ thi·∫øu d·∫•u ';'", "yellow"))
        
        return True, "‚úÖ C++ Structure: OK"

    return True, "‚ö†Ô∏è Unknown language: Skip deep validation"

# ============================================================================
# SAFETY: ULTIMATE FALLBACK (H·ªá th·ªëng t·ª± ph·ª•c h·ªìi & Ch·ªëng s·ª•p ƒë·ªï)
# ============================================================================
def ultimate_fallback(state, messages):
    """
    Quy tr√¨nh x·ª≠ l√Ω s·ª± c·ªë kh·∫©n c·∫•p: Ghi log, ph√¢n t√≠ch l·ªói v√† t√°i kh·ªüi ƒë·ªông an to√†n.
    """
    # 1. Thu th·∫≠p d·ªØ li·ªáu l·ªói t·ª´ State
    error_logs = state.get("error_log", [])
    last_error = error_logs[-1] if error_logs else "L·ªói kh√¥ng x√°c ƒë·ªãnh (Internal Server Error)"
    
    print(colored(f"üö® [CRITICAL ERROR] H·ªá th·ªëng ƒëang k√≠ch ho·∫°t quy tr√¨nh ·ª©ng c·ª©u kh·∫©n c·∫•p!", "red", attrs=["bold"]))
    print(colored(f"--> Chi ti·∫øt l·ªói: {last_error}", "red"))

    # 2. Ghi nh·∫≠t k√Ω l·ªói v√†o file v·∫≠t l√Ω (ƒê·ªÉ k·ªπ thu·∫≠t vi√™n ki·ªÉm tra sau)
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    with open("system_crash_log.txt", "a", encoding="utf-8") as f:
        f.write(f"[{timestamp}] ERROR: {last_error}\n")

    # 

    # 3. X√¢y d·ª±ng th√¥ng ƒëi·ªáp chuy√™n nghi·ªáp cho CEO
    error_summary = (
        "üõë **TH√îNG B√ÅO H·ªÜ TH·ªêNG**: AI Corporation v·ª´a g·∫∑p m·ªôt s·ª± c·ªë k·ªπ thu·∫≠t ngo√†i √Ω mu·ªën.\n\n"
        f"üîç **Ph√¢n t√≠ch nhanh**: `{last_error[:200]}...`\n"
        "üõ†Ô∏è **H√†nh ƒë·ªông**: To√†n b·ªô d·ªØ li·ªáu d·ª± √°n ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°m th·ªùi. T√¥i ƒëang th·ª±c hi·ªán reset c√°c tham s·ªë ƒë·ªÉ tr√°nh treo lu·ªìng.\n\n"
        "üëâ **CEO c√≥ th·ªÉ**: Th·ª≠ nh·∫≠p l·ªánh ng·∫Øn g·ªçn h∆°n ho·∫∑c g√µ 'restart' ƒë·ªÉ l√†m m·ªõi ho√†n to√†n b·ªô n√£o."
    )

    # 4. Tr·∫£ v·ªÅ tr·∫°ng th√°i an to√†n
    return {
        "messages": [AIMessage(content=error_summary)],
        "next_step": "FINISH", # Ho·∫∑c ƒë·∫©y v·ªÅ Supervisor n·∫øu mu·ªën AI t·ª± th·ª≠ l·∫°i
        "error_log": error_logs + ["System Fallback Triggered"]
    }

# ============================================================================
# 3. H·ªá Th·ªëng B·ªô Nh·ªõ
# ============================================================================
# ============================================================================
# UTILITY: INGEST DOCUMENTS (H·ªá th·ªëng n·∫°p tri th·ª©c ƒëa ngu·ªìn)
# ============================================================================

def ingest_docs_to_memory(folder_path="./data_sources"):
    """
    Quy tr√¨nh ETL chuy√™n nghi·ªáp: Tr√≠ch xu·∫•t, Bi·∫øn ƒë·ªïi v√† N·∫°p tri th·ª©c v√†o Vector DB.
    H·ªó tr·ª£: Metadata Mapping, Batch Loading v√† Integrity Check.
    """
    # 1. Kh·ªüi t·∫°o & Ki·ªÉm tra m√¥i tr∆∞·ªùng
    if not os.path.exists(folder_path): 
        os.makedirs(folder_path)
        return f"üìÇ Th∆∞ m·ª•c '{folder_path}' ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o. H√£y th√™m t√†i li·ªáu PDF."

    print(colored(f"üöÄ [ETL PROCESS] B·∫Øt ƒë·∫ßu n·∫°p tri th·ª©c t·ª´: {folder_path}", "cyan", attrs=["bold"]))

    # 2. C·∫•u h√¨nh Loader th√¥ng minh
    try:
        # S·ª≠ d·ª•ng DirectoryLoader v·ªõi PyPDFLoader ƒë·ªÉ b√≥c t√°ch Metadata t·ª± ƒë·ªông
        loader = DirectoryLoader(
            folder_path, 
            glob="./*.pdf", 
            loader_cls=PyPDFLoader,
            show_progress=True,
            use_multithreading=True # T·ªëi ∆∞u h√≥a t·ªëc ƒë·ªô ƒë·ªçc file
        )
        docs = loader.load()
    except Exception as e:
        return f"‚ùå L·ªói tr√≠ch xu·∫•t (Extraction Error): {str(e)}"

    if not docs:
        return "‚ö†Ô∏è Tr·∫°ng th√°i: Kh√¥ng t√¨m th·∫•y t√†i li·ªáu PDF m·ªõi ƒë·ªÉ x·ª≠ l√Ω."

    # 3. Chi·∫øn l∆∞·ª£c ph√¢n m·∫£nh (Chunking Strategy) chuy√™n s√¢u
    # TƒÉng overlap l√™n 200 ƒë·ªÉ tr√°nh m·∫•t ng·ªØ c·∫£nh gi·ªØa c√°c ƒëo·∫°n (Context preservation)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, 
        chunk_overlap=200,
        length_function=len,
        add_start_index=True # L∆∞u v·ªã tr√≠ b·∫Øt ƒë·∫ßu ƒë·ªÉ truy xu·∫•t ch√≠nh x√°c
    )
    splits = text_splitter.split_documents(docs)

    # 4. L√†m s·∫°ch d·ªØ li·ªáu & Chu·∫©n h√≥a Metadata
    valid_splits = []
    for doc in splits:
        clean_content = doc.page_content.strip()
        if len(clean_content) > 50: # Lo·∫°i b·ªè c√°c m·∫©u r√°c ho·∫∑c trang tr·∫Øng
            # B·ªï sung th√¥ng tin ngu·ªìn ƒë·ªÉ AI tr√≠ch d·∫´n sau n√†y
            doc.metadata["ingested_at"] = datetime.now().isoformat()
            doc.metadata["doc_hash"] = hash(clean_content) # H·ªó tr·ª£ ch·ªëng tr√πng l·∫∑p s∆° b·ªô
            valid_splits.append(doc)

    if not valid_splits:
        return "‚ö†Ô∏è C·∫£nh b√°o: T√†i li·ªáu OCR/·∫¢nh kh√¥ng th·ªÉ b√≥c t√°ch n·ªôi dung vƒÉn b·∫£n."

    # 

    # 5. N·∫°p d·ªØ li·ªáu v√†o Vector DB theo t·ª´ng Batch (Ch·ªëng tr√†n RAM)
    try:
        batch_size = 100
        total_chunks = len(valid_splits)
        print(colored(f"üì¶ ƒêang m√£ h√≥a v√† n·∫°p {total_chunks} ph√¢n ƒëo·∫°n v√†o b·ªô n√£o...", "white"))
        
        for i in range(0, total_chunks, batch_size):
            batch = valid_splits[i:i + batch_size]
            vector_db.add_documents(batch)
            
        print(colored("‚úÖ [INGESTION SUCCESS] Tri th·ª©c ƒë√£ ƒë∆∞·ª£c ƒë·ªìng b·ªô h√≥a to√†n di·ªán.", "green", attrs=["bold"]))
        return f"üöÄ Th√†nh c√¥ng: ƒê√£ n·∫°p {total_chunks} ph√¢n ƒëo·∫°n t·ª´ {len(docs)} t√†i li·ªáu v√†o b·ªô n√£o trung t√¢m."

    except Exception as e:
        return f"‚ùå L·ªói n·∫°p d·ªØ li·ªáu (Load Error): {str(e)}"
# ============================================================================
# UTILITY: REMEMBER KNOWLEDGE (Ghi nh·ªõ tri th·ª©c & K√Ω ·ª©c ng·∫Øn h·∫°n)
# ============================================================================
def remember_knowledge(text: str, category: str = "General", priority: int = 1):
    """
    H·ªá th·ªëng ghi nh·ªõ th√¥ng minh: T·ª± ƒë·ªông ph√¢n lo·∫°i, g·∫Øn nh√£n th·ªùi gian v√† l∆∞u tr·ªØ.
    """
    if not text or len(text.strip()) < 10:
        return "‚ö†Ô∏è N·ªôi dung qu√° ng·∫Øn, h·ªá th·ªëng t·ª´ ch·ªëi ghi nh·ªõ."

    print(colored(f"üíæ [MEMORY SAVE] ƒêang n·∫°p tri th·ª©c m·ªõi v√†o danh m·ª•c: {category}...", "green"))

    try:
        # 1. T·∫°o Metadata chuy√™n nghi·ªáp
        # Vi·ªác n√†y gi√∫p sau n√†y search theo "Th·ªùi gian" ho·∫∑c "Ch·ªß ƒë·ªÅ" c·ª±c nhanh
        metadata = {
            "category": category,
            "priority": priority,
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "source": "AI_Internal_Learning" # ƒê√°nh d·∫•u ƒë√¢y l√† ki·∫øn th·ª©c t·ª± h·ªçc t·ª´ h·ªôi tho·∫°i
        }

        # 2. Chia nh·ªè vƒÉn b·∫£n (n·∫øu text qu√° d√†i) ƒë·ªÉ t·ªëi ∆∞u h√≥a t√¨m ki·∫øm sau n√†y
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        chunks = text_splitter.split_text(text)

        # 3. N·∫°p v√†o Vector DB
        # Ch√∫ng ta d√πng add_texts nh∆∞ng k√®m theo list metadata t∆∞∆°ng ·ª©ng cho t·ª´ng chunk
        vector_db.add_texts(
            texts=chunks,
            metadatas=[metadata] * len(chunks)
        )

        # 4. L∆∞u log ƒë·ªÉ CEO theo d√µi
        success_msg = f"‚úÖ ƒê√£ ghi nh·ªõ {len(chunks)} ph√¢n ƒëo·∫°n tri th·ª©c v√†o danh m·ª•c '{category}'."
        print(colored(success_msg, "green"))
        
        return success_msg

    except Exception as e:
        error_msg = f"‚ùå L·ªói ghi nh·ªõ: {str(e)}"
        print(colored(error_msg, "red"))
        return error_msg

#  --- h·ªçc ƒë·ªÉ ti·∫øn b·ªô----
def save_for_finetuning(prompt, response, metadata):
    # Ch·ªâ l∆∞u n·∫øu code n√†y ƒë√£ ƒë∆∞·ª£c Tester x√°c nh·∫≠n l√† ƒê√öNG (Pass)
    entry = {
        "instruction": prompt,
        "input": metadata.get("context", ""),
        "output": response,
        "source": metadata.get("model_name") # L∆∞u ƒë·ªÉ bi·∫øt ƒë√¢y l√† ki·∫øn th·ª©c t·ª´ Claude hay GPT-4
    }
    with open("knowledge_legacy.jsonl", "a", encoding="utf-8") as f:
        f.write(json.dumps(entry, ensure_ascii=False) + "\n")   


#  ----Th√™m vƒÉn b·∫£n v√†o ChromaDB----
def learn_knowledge(text: str):
    """
    L∆∞u ki·∫øn th·ª©c m·ªõi v√†o b·ªô n√£o trung t√¢m (ChromaDB).
    ƒê·ªìng b·ªô v·ªõi ƒë·ªëi t∆∞·ª£ng vector_db ƒë√£ kh·ªüi t·∫°o ·ªü ƒë·∫ßu file.
    """
    try:
        # Th√™m vƒÉn b·∫£n v√†o ChromaDB hi·ªán c√≥
        vector_db.add_texts([text])
        
        # Ghi ch√∫: ChromaDB trong b·∫£n m·ªõi th∆∞·ªùng t·ª± ƒë·ªông persist (l∆∞u) 
        # n√™n kh√¥ng c·∫ßn g·ªçi l·ªánh .persist() th·ªß c√¥ng nh∆∞ c√°c b·∫£n c≈©.
        
        print(colored(f"--> [MEMORY] ƒê√£ h·ªçc: {text[:50]}...", "green"))
        return "‚úÖ H·ªá th·ªëng ƒë√£ ghi nh·ªõ ki·∫øn th·ª©c n√†y v√†o b·ªô n√£o trung t√¢m (ChromaDB)."
    except Exception as e:
        return f"‚ùå L·ªói khi ghi nh·ªõ ki·∫øn th·ª©c: {e}"

# ============================================================================
# NODE: KNOWLEDGE RETRIEVAL (Truy xu·∫•t Tri th·ª©c & K√Ω ·ª©c doanh nghi·ªáp)
# ============================================================================
def recall_knowledge(query: str, top_k: int = 3):
    """
    Truy xu·∫•t tri th·ª©c th√¥ng minh: T√¨m ki·∫øm ng·ªØ nghƒ©a, l·ªçc nhi·ªÖu v√† tr√≠ch d·∫´n ngu·ªìn.
    """
    print(colored(f"[üß† RECALL] ƒêang truy xu·∫•t k√Ω ·ª©c cho: '{query}'...", "green"))

    try:
        # 1. T√¨m ki·∫øm v·ªõi ƒëi·ªÉm tin c·∫≠y (Similarity Search with Score)
        # ƒêi·ªÉm c√†ng th·∫•p (trong ChromaDB/L2 Distance) th√¨ c√†ng ch√≠nh x√°c
        results_with_scores = vector_db.similarity_search_with_score(query, k=top_k)

        if not results_with_scores:
            return "H·ªá th·ªëng ch∆∞a c√≥ k√Ω ·ª©c v·ªÅ v·∫•n ƒë·ªÅ n√†y."

        # 

        # 2. L·ªçc k·∫øt qu·∫£ (Threshold Filtering)
        # Ch·ªâ l·∫•y nh·ªØng ƒëo·∫°n ki·∫øn th·ª©c c√≥ ƒë·ªô li√™n quan cao (ƒëi·ªÉm kho·∫£ng < 0.6 - 0.8 t√πy model)
        valid_context = []
        sources = set()

        for doc, score in results_with_scores:
            if score < 0.8:  # Ng∆∞·ª°ng tin c·∫≠y
                source_name = doc.metadata.get("source", "T√†i li·ªáu n·ªôi b·ªô")
                page = doc.metadata.get("page", "N/A")
                
                context_block = f"--- TR√çCH D·∫™N T·ª™: {source_name} (Trang {page}) ---\n{doc.page_content}"
                valid_context.append(context_block)
                sources.add(source_name)

        if not valid_context:
            return "T√¨m th·∫•y th√¥ng tin nh∆∞ng ƒë·ªô tin c·∫≠y qu√° th·∫•p ƒë·ªÉ s·ª≠ d·ª•ng."

        # 3. T·ªïng h·ª£p b√°o c√°o tri th·ª©c cho Agent
        final_memory = "\n\n".join(valid_context)
        
        print(colored(f"‚úÖ ƒê√£ t√¨m th·∫•y tri th·ª©c t·ª´ {len(sources)} ngu·ªìn uy t√≠n.", "green"))
        return final_memory

    except Exception as e:
        print(colored(f"‚ùå L·ªói truy xu·∫•t b·ªô n√£o: {e}", "red"))
        return "H·ªá th·ªëng l∆∞u tr·ªØ tri th·ª©c ƒëang g·∫∑p s·ª± c·ªë k·ªπ thu·∫≠t."

def router_node(state):
    """
    Router: ƒêi·ªÉm g√°c c·ªïng ƒë·∫ßu ti√™n.
    """
    # 1. L·∫•y d·ªØ li·ªáu an to√†n
    messages = state.get("messages", [])
    error_log = state.get("error_log", [])
    task_type = state.get("task_type", "general")
    
    # 2. Ki·ªÉm tra n·∫øu kh√¥ng c√≥ tin nh·∫Øn
    if not messages:
        return {
            "messages": [],
            "next_step": "Supervisor", 
            "current_agent": "Router",
            "error_log": error_log,
            "task_type": task_type
        }

    # 3. L·∫•y n·ªôi dung tin nh·∫Øn cu·ªëi
    last_msg = messages[-1].content.upper() if hasattr(messages[-1], 'content') else str(messages[-1]).upper()

    # 4. B·∫¢N ƒê·ªí ƒêI·ªÄU H∆Ø·ªöNG C∆Ø·ª†NG B·ª®C
    route_map = {
        "[RESEARCH]": "Researcher",
        "[INVEST]": "Investment",
        "[HARDWARE]": "Hardware",
        "[ENGINEERING]": "Engineering",
        "[IOT]": "IoT_Engineer",
        "[MARKETING]": "Marketing",
        "[LEGAL]": "Legal",
        "[STORY]": "Storyteller",
        "[PUBLISH]": "Publisher"
    }

    # 5. KI·ªÇM TRA TAG V√Ä ƒê·ªäNH TUY·∫æN
    for tag, target_node in route_map.items():
        if tag in last_msg:
            print(colored(f"üöÄ [ROUTER] Ph√°t hi·ªán TAG {tag}: ƒêi th·∫≥ng t·ªõi {target_node}", "green"))
            return {
                "messages": [], # B·∫Øt bu·ªôc c√≥
                "next_step": target_node, 
                "current_agent": "Router",
                "error_log": error_log,
                "task_type": task_type
            }

    # 6. M·∫∂C ƒê·ªäNH: Chuy·ªÉn v·ªÅ Supervisor (S·ª≠a l·ªói bi·∫øn node ch∆∞a ƒë·ªãnh nghƒ©a)
    print(colored("üß† [ROUTER] Kh√¥ng c√≥ TAG: Chuy·ªÉn h·ªì s∆° cho Supervisor ƒëi·ªÅu ph·ªëi...", "cyan"))
    return {
        "messages": [], # B·∫Øt bu·ªôc c√≥
        "next_step": "Supervisor", # Tr·∫£ v·ªÅ chu·ªói c·ª• th·ªÉ thay v√¨ bi·∫øn node
        "current_agent": "Router",
        "error_log": error_log,
        "task_type": task_type
    }

# ============================================================================
# UTILITY: SEARCH MEMORY (C√¥ng c·ª• truy v·∫•n tri th·ª©c chuy√™n s√¢u)
# ============================================================================
def search_memory(query: str, k: int = 3):
    """
    T√¨m ki·∫øm th√¥ng tin t·ª´ ChromaDB b·∫±ng thu·∫≠t to√°n Similarity Search v·ªõi ng∆∞·ª°ng tin c·∫≠y.
    """
    print(colored(f"üîç [MEMORY SEARCH] ƒêang truy v·∫•n: '{query}'", "dark_grey"))
    
    try:
        # 1. S·ª≠ d·ª•ng similarity_search_with_score ƒë·ªÉ ƒëo l∆∞·ªùng ƒë·ªô ch√≠nh x√°c
        # K·∫øt qu·∫£ tr·∫£ v·ªÅ l√† list c√°c tuple (Document, Score)
        results = vector_db.similarity_search_with_score(query, k=k)
        
        if not results:
            return "D·ªØ li·ªáu tr·ªëng ho·∫∑c kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan."

        # 

        # 2. L·ªçc k·∫øt qu·∫£ d·ª±a tr√™n Score (Kho·∫£ng c√°ch vector)
        # Trong ChromaDB, score c√†ng th·∫•p (g·∫ßn 0) th√¨ c√†ng gi·ªëng nhau
        valid_contents = []
        for doc, score in results:
            # Ng∆∞·ª°ng 0.6 l√† kh√° ch·∫∑t ch·∫Ω, ƒë·∫£m b·∫£o th√¥ng tin ch·∫•t l∆∞·ª£ng
            if score < 0.6: 
                source = doc.metadata.get('source', 'Unknown')
                content = f"[Ngu·ªìn: {source}]\n{doc.page_content}"
                valid_contents.append(content)
        
        if not valid_contents:
            return "T√¨m th·∫•y d·ªØ li·ªáu nh∆∞ng ƒë·ªô li√™n quan kh√¥ng ƒë·ªß cao ƒë·ªÉ h·ªó tr·ª£ quy·∫øt ƒë·ªãnh."

        # 3. G·ªôp c√°c m·∫©u ki·∫øn th·ª©c l·∫°i th√†nh m·ªôt kh·ªëi b·ªëi c·∫£nh (Context Block)
        formatted_result = "\n" + "="*30 + "\n"
        formatted_result += "\n\n".join(valid_contents)
        formatted_result += "\n" + "="*30
        
        return formatted_result

    except Exception as e:
        print(colored(f"‚ùå L·ªói truy v·∫•n b·ªô n√£o: {e}", "red"))
        return "L·ªói h·ªá th·ªëng khi truy xu·∫•t b·ªô nh·ªõ."

def log_to_legacy_dataset(task_type: str, prompt: str, completion: str, model_name: str, score: int):
    """
    L∆∞u tr·ªØ c√°c phi√™n l√†m vi·ªác ch·∫•t l∆∞·ª£ng cao ƒë·ªÉ ph·ª•c v·ª• Fine-tuning Local LLM sau n√†y.
    """
    # Ch·ªâ l∆∞u nh·ªØng n·ªôi dung c√≥ ƒëi·ªÉm ch·∫•t l∆∞·ª£ng cao (v√≠ d·ª• score t·ª´ Tester >= 70)
    if score < 70:
        return

    file_path = "corporate_brain_dataset.jsonl"
    
    # C·∫•u tr√∫c d·ªØ li·ªáu theo chu·∫©n Instruct Tuning
    entry = {
        "timestamp": datetime.now().isoformat(),
        "task_group": task_type,
        "instruction": f"B·∫°n l√† chuy√™n gia {task_type} t·∫°i AI Corporation. H√£y th·ª±c hi·ªán: {prompt}",
        "context": "S·ª≠ d·ª•ng ti√™u chu·∫©n Clean Code v√† ki·∫øn tr√∫c h·ªá th·ªëng t·ªëi ∆∞u.",
        "response": completion,
        "teacher_model": model_name,
        "quality_score": score
    }

    try:
        with open(file_path, "a", encoding="utf-8") as f:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")
        print(colored(f"üìî [SHADOW LEARNING] ƒê√£ l∆∞u 1 m·∫´u tri th·ª©c t·ª´ {model_name} v√†o b·ªô nh·ªõ k·∫ø th·ª´a.", "blue"))
    except Exception as e:
        print(colored(f"‚ö†Ô∏è L·ªói l∆∞u dataset: {e}", "red"))

#  ----- M·ª©c ƒë·ªô k·∫ø th·ª´a----
def legacy_audit_report():
    """
    B√°o c√°o ti·∫øn ƒë·ªô t√≠ch l≈©y tri th·ª©c ƒë·ªÉ chu·∫©n b·ªã cho vi·ªác tho√°t ly API.
    """
    file_path = "corporate_brain_dataset.jsonl"
    if not os.path.exists(file_path):
        return "üìâ H·ªá th·ªëng ch∆∞a c√≥ d·ªØ li·ªáu k·∫ø th·ª´a. H√£y b·∫Øt ƒë·∫ßu ch·∫°y c√°c d·ª± √°n!"

    stats = {}
    total_count = 0

    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            data = json.loads(line)
            group = data.get("task_group", "Unknown")
            stats[group] = stats.get(group, 0) + 1
            total_count += 1

    print(colored("\n" + "="*40, "magenta"))
    print(colored("üìú B√ÅO C√ÅO TI·∫æN ƒê·ªò K·∫æ TH·ª™A TRI TH·ª®C", "magenta", attrs=["bold"]))
    print(colored(f"T·ªïng s·ªë m·∫´u ch·∫•t l∆∞·ª£ng cao: {total_count}", "white"))
    
    for group, count in stats.items():
        # Gi·∫£ s·ª≠ 500 m·∫´u l√† ƒë·ªß ƒë·ªÉ Fine-tune s∆° b·ªô m·ªôt Agent
        progress = min((count / 500) * 100, 100)
        color = "green" if progress >= 80 else "yellow"
        print(f"- {group:15}: {count:4} m·∫´u ({progress:>5.1f}%) " + colored("‚ñà" * int(progress/5), color))
    
    print(colored("="*40 + "\n", "magenta"))

def orchestrator_router(state):
    """
    B·ªô n√£o ƒëi·ªÅu ph·ªëi: Quy·∫øt ƒë·ªãnh ai l√† ng∆∞·ªùi ti·∫øp theo d·ª±a tr√™n ti·∫øn ƒë·ªô d·ª± √°n.
    """
    messages = state.get("messages", [])
    last_msg = messages[-1].content.upper()

    # 1. N·∫øu ƒëang ·ªü giai ƒëo·∫°n t√¨m ki·∫øm th·ªã tr∆∞·ªùng
    if "KI·ªÇM TRA TH·ªä TR∆Ø·ªúNG" in last_msg or "RESEARCH" in last_msg:
        return "Researcher"
    
    # 2. N·∫øu nghi√™n c·ª©u xong v√† c·∫ßn thi·∫øt k·∫ø
    if "PH∆Ø∆†NG √ÅN THI·∫æT K·∫æ" in last_msg or "DESIGN" in last_msg:
        return "Hardware"

    # 3. N·∫øu ƒë√£ c√≥ danh m·ª•c linh ki·ªán (BOM), chuy·ªÉn sang mua h√†ng
    if "BOM" in last_msg or "LINH KI·ªÜN" in last_msg:
        return "Procurement"

    # 4. N·∫øu h√†ng v·ªÅ, chuy·ªÉn sang l·∫Øp r√°p & n·∫°p code
    if "L·∫ÆP R√ÅP" in last_msg or "ASSEMBLY" in last_msg:
        return "IoT_Engineer"

    # 5. Cu·ªëi c√πng, t√¨m ng∆∞·ªùi v·∫≠n h√†nh
    if "NH√ÇN S·ª∞" in last_msg or "RECRUIT" in last_msg:
        return "HR"

    return "Supervisor"

workflow_map = [
    {"from": "Researcher", "to": "Engineering", "condition": "if_not_exist"},
    {"from": "Engineering", "to": "Procurement", "condition": "on_approval"},
    {"from": "Procurement", "to": "IoT_Engineer", "condition": "on_arrival"}
]

def dynamic_orchestrator(state):
    """
    B·ªô ƒëi·ªÅu ph·ªëi ƒë·ªông (Server Mode - Non-blocking).
    
    L·ªñI C≈®: D√πng input() khi·∫øn Server treo khi ch·∫°y ng·∫ßm.
    S·ª¨A ƒê·ªîI: T·ª± ƒë·ªông chuy·ªÉn quy·ªÅn v·ªÅ Supervisor (CEO AI) ƒë·ªÉ quy·∫øt ƒë·ªãnh b∆∞·ªõc ti·∫øp theo.
    """
    # 1. L·∫•y th√¥ng tin ng·ªØ c·∫£nh hi·ªán t·∫°i
    last_agent = state.get("current_agent", "Unknown Agent")
    
    # L·∫•y n·ªôi dung tin nh·∫Øn cu·ªëi c√πng ƒë·ªÉ log (n·∫øu c·∫ßn)
    # last_message = state["messages"][-1].content 

    # 2. Ghi log ra Terminal Server (ƒê·ªÉ k·ªπ thu·∫≠t vi√™n theo d√µi ng·∫ßm)
    # S·ª≠ d·ª•ng m√†u s·∫Øc ƒë·ªÉ d·ªÖ ph√¢n bi·ªát trong ƒë·ªëng log h·ªón ƒë·ªôn
    print(colored(f"\n" + "="*50, "yellow"))
    print(colored(f"üö© [ORCHESTRATOR] NH·∫¨N B√ÅO C√ÅO T·ª™: {last_agent.upper()}", "yellow", attrs=["bold"]))
    print(colored("--> Tr·∫°ng th√°i: T·ª± ƒë·ªông chuy·ªÉn h·ªì s∆° v·ªÅ Supervisor.", "white"))
    print(colored("="*50, "yellow"))

    # 3. LOGIC ƒêI·ªÄU H∆Ø·ªöNG (CASE 2)
    # Thay v√¨ return {"next_step": input(...)} g√¢y treo,
    # ta tr·∫£ v·ªÅ "Supervisor".
    # Supervisor s·∫Ω ƒë·ªçc l·∫°i to√†n b·ªô l·ªãch s·ª≠, th·∫•y Agent kia ƒë√£ l√†m xong,
    # v√† t·ª± ƒë∆∞a ra quy·∫øt ƒë·ªãnh ti·∫øp theo (ho·∫∑c FINISH).
    
    return {"next_step": "Supervisor"}
# ============================================================================
# 4. ƒê·ªäNH NGHƒ®A NODE AGENTS
# ============================================================================
# ============================================================================
# NODE: SUPERVISOR (T·ªïng Gi√°m ƒë·ªëc ƒêi·ªÅu ph·ªëi - CEO AI)
# ============================================================================

def supervisor_node(state):
    """
    SUPERVISOR V3 (HYBRID): K·∫æT H·ª¢P T∆Ø DUY NG·ªÆ C·∫¢NH & C∆† CH·∫æ T·ª∞ S·ª¨A L·ªñI.
    """
    print(colored(f"\n[üß† SUPERVISOR] ƒêang ph√¢n t√≠ch chi·∫øn l∆∞·ª£c (B∆∞·ªõc {len(state['messages'])})...", "cyan", attrs=["bold"]))

    # 1. L·∫§Y D·ªÆ LI·ªÜU ƒê·∫¶Y ƒê·ª¶ T·ª™ STATE (GI·ªÆ NGUY√äN ƒê·ªÇ TR√ÅNH L·ªñI GRAPH)
    messages = state.get("messages", [])
    error_log = state.get("error_log", [])      # <--- B·∫Øt bu·ªôc
    task_type = state.get("task_type", "general") # <--- B·∫Øt bu·ªôc

    # 2. KI·ªÇM TRA GI·ªöI H·∫†N (Safety Guard)
    if len(messages) > 150:
        return {
            "messages": [AIMessage(content="H·ªôi tho·∫°i ƒë√£ ƒë·∫°t gi·ªõi h·∫°n. ƒêang ƒë√≥ng h·ªì s∆°.")],
            "next_step": "FINISH",
            "current_agent": "Supervisor",
            "error_log": error_log,
            "task_type": task_type
        }

    # 3. ƒê·ªäNH NGHƒ®A "T·ª™ ƒêI·ªÇN S·ª¨A SAI" (AUTO-CORRECT MAP)
    # ƒê√¢y l√† l·ªõp ph√≤ng th·ªß: N·∫øu AI l·ª° mi·ªáng g·ªçi t√™n sai, ta √¢m th·∫ßm s·ª≠a l·∫°i ngay
    AGENT_ALIASES = {
        "FINANCE": "Investment",
        "MONEY": "Investment",
        "STOCK": "Investment",
        "CFO": "Investment",
        "CODE": "Coder",
        "DEV": "Coder",
        "MARKET": "Researcher",
        "SEARCH": "Researcher",
        "DESIGN": "Hardware",
        "IOT": "IoT_Engineer",
        "WRITER": "Storyteller",
        "PUBLISH": "Publisher",
        "REVIEW": "Tester",
        "HR": "Secretary"
    }

    # Danh s√°ch Node CH√çNH TH·ª®C trong Graph (Ph·∫£i kh·ªõp 100% v·ªõi nodes_map)
    VALID_NODES = [
        "Coder", "Researcher", "Hardware", "Strategy_R_and_D", 
        "Marketing", "Storyteller", "Legal", "Investment", 
        "Engineering", "IoT_Engineer", "Procurement", "Artist", 
        "Tester", "Secretary", "FINISH"
    ]

    # 4. THI·∫æT L·∫¨P SYSTEM PROMPT (T∆Ø DUY NG·ªÆ C·∫¢NH)
    # D·∫°y AI hi·ªÉu "Vi·ªác n√†y l√† c·ªßa ai?" thay v√¨ ch·ªâ nh·ªõ t√™n
    jarvis_style = (
        "\n[PHONG C√ÅCH]: Tr·∫£ l·ªùi ng·∫Øn g·ªçn, quy·∫øt ƒëo√°n nh∆∞ m·ªôt CEO th·ª±c th·ª•. "
        "Hi·ªÉu √Ω t·∫°i ng√¥n ngo·∫°i. V√≠ d·ª•: CEO h·ªèi 'Gi√° v√†ng' -> Hi·ªÉu l√† T√†i ch√≠nh (Investment)."
    )
    
    roles_description = """
    PH√ÇN T√çCH √ù ƒê·ªäNH V√Ä CH·ªåN NH√ÇN S·ª∞ PH√ô H·ª¢P:
    - [Researcher]: T√¨m ki·∫øm th√¥ng tin, Tin t·ª©c, D·ªØ li·ªáu th·ªã tr∆∞·ªùng, ƒê·ªëi th·ªß.
    - [Investment]: T√†i ch√≠nh, Ti·ªÅn, Ch·ª©ng kho√°n, L·ª£i nhu·∫≠n, Gi√° c·∫£, Ng√¢n s√°ch.
    - [Coder]: Vi·∫øt code, L·∫≠p tr√¨nh, Web, App, Debug.
    - [Hardware]: Ph·∫ßn c·ª©ng, M·∫°ch ƒëi·ªán, Chip, Robot, S∆° ƒë·ªì ch√¢n.
    - [Strategy_R_and_D]: Chi·∫øn l∆∞·ª£c, K·∫ø ho·∫°ch kinh doanh, SWOT.
    - [Marketing]: Qu·∫£ng c√°o, Vi·∫øt content, Facebook, Email.
    - [Storyteller]: Vi·∫øt truy·ªán, K·ªãch b·∫£n, S√°ng t√°c vƒÉn h·ªçc.
    - [Legal]: Lu·∫≠t ph√°p, B·∫£n quy·ªÅn.
    - [FINISH]: Khi vi·ªác ƒë√£ xong ho·∫∑c ch·ªâ ch√†o h·ªèi x√£ giao.
    """

    system_prompt = (
        "B·∫°n l√† J.A.R.V.I.S - T·ªïng ƒëi·ªÅu h√†nh AI Corporation.\n"
        f"{roles_description}\n"
        f"QUY T·∫ÆC: Ch·ªâ ch·ªçn nh√¢n s·ª± trong danh s√°ch tr√™n. {jarvis_style}"
    )

    # X·ª≠ l√Ω h√¨nh ·∫£nh (Gi·ªØ nguy√™n logic c≈©)
    last_msg = messages[-1].content
    # query, image_b64 = process_vision_message(last_msg) # Uncomment n·∫øu d√πng h√†m n√†y
    query = last_msg
    image_b64 = None

    user_input = query
    if image_b64:
        user_input = [
            {"type": "text", "text": f"Ph√¢n t√≠ch: {query}"},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_b64}"}}
        ]

    conversation = [SystemMessage(content=system_prompt)] + messages[-5:] + [HumanMessage(content=str(user_input))]

    # 5. FUNCTION CALLING
    function_def = [{
        "name": "route_to_agent",
        "description": "ƒêi·ªÅu ph·ªëi nh√¢n s·ª±.",
        "parameters": {
            "type": "object",
            "properties": {
                "next": {"type": "string", "enum": VALID_NODES}, # G·ª£i √Ω cho AI bi·∫øt danh s√°ch ƒë√∫ng
                "reason": {"type": "string", "description": "L√Ω do ch·ªçn agent n√†y."}
            },
            "required": ["next", "reason"]
        }
    }]

    try:
        # G·ªçi Model
        response = LLM_GPT4.bind_functions(functions=function_def, function_call={"name": "route_to_agent"}).invoke(conversation)
        
        # Parse k·∫øt qu·∫£
        arguments = response.additional_kwargs.get("function_call", {}).get("arguments", "{}")
        args = json.loads(arguments)
        
        raw_next = str(args.get("next", "FINISH"))
        reason = args.get("reason", "Theo quy tr√¨nh.")

        # --- 6. LOGIC "B·ªåC TH√âP" (AUTO-CORRECT & VALIDATION) ---
        
        # B∆∞·ªõc 1: Chu·∫©n h√≥a ch·ªØ hoa ƒë·ªÉ so s√°nh
        next_upper = raw_next.upper()

        # B∆∞·ªõc 2: S·ª≠a l·ªói b·∫±ng t·ª´ ƒëi·ªÉn (V√≠ d·ª•: FINANCE -> Investment)
        if next_upper in AGENT_ALIASES:
            final_next = AGENT_ALIASES[next_upper]
            print(colored(f"‚ö†Ô∏è Auto-Correct: '{raw_next}' -> '{final_next}'", "yellow"))
        
        # B∆∞·ªõc 3: Ki·ªÉm tra xem c√≥ n·∫±m trong danh s√°ch node th·∫≠t kh√¥ng
        elif raw_next in VALID_NODES:
            final_next = raw_next
        
        # B∆∞·ªõc 4: N·∫øu sai b√©t nh√® -> V·ªÅ FINISH cho an to√†n
        else:
            print(colored(f"üö® T√™n l·∫° '{raw_next}' -> Chuy·ªÉn v·ªÅ FINISH", "red"))
            final_next = "FINISH"

        print(colored(f"--> [QUY·∫æT ƒê·ªäNH]: {final_next} | L√Ω do: {reason}", "yellow"))

        # --- TR·∫¢ V·ªÄ STATE (ƒê·ª¶ 5 KH√ìA) ---
        return {
            "messages": [AIMessage(content=f"üì° **L·ªánh ƒëi·ªÅu h√†nh**: Chuy·ªÉn giao cho **{final_next}**.\n**L√Ω do**: {reason}")],
            "next_step": final_next,
            "current_agent": "Supervisor",
            "error_log": error_log,     
            "task_type": task_type      
        }

    except Exception as e:
        print(colored(f"‚ö†Ô∏è L·ªói Supervisor: {e}", "red"))
        # Fallback an to√†n
        return {
            "messages": [AIMessage(content=f"‚ö†Ô∏è L·ªói ƒëi·ªÅu ph·ªëi: {str(e)}")], 
            "next_step": "FINISH", # V·ªÅ ƒë√≠ch an to√†n
            "current_agent": "Supervisor",
            "error_log": error_log + [str(e)],
            "task_type": task_type
        }
#  ---- Vi·∫øt Code----
async def coder_node(state): # Chuy·ªÉn sang async ƒë·ªÉ ch·∫°y song song
    """
    Claude Coder Node - Parallel Execution & AST Validation
    """
    print(colored("[üöÄ CODER V2] Parallel Ensemble Mode ACTIVATED", "green", attrs=["bold"]))
    
    # 1. SETUP CONTEXT
    errors = state.get("error_log", [])
    task_type = state.get("task_type", "general").lower()
    messages = state.get('messages', [])
    last_user_msg = messages[-1].content
    
    # An to√†n: T√¨m ki·∫øm k√Ω ·ª©c (Tr√°nh l·ªói n·∫øu h√†m search_memory ch∆∞a s·∫µn s√†ng)
    try:
        memory_context = search_memory("Ti√™u chu·∫©n vi·∫øt code Clean Code, SOLID")
    except:
        memory_context = "Tu√¢n th·ªß PEP8, Clean Code v√† th√™m comment gi·∫£i th√≠ch."
    # error_context = self_heal_analyzer(errors)
    
    # 2. PROMPT STRATEGY (Smart Selection)
    base_prompt = get_claude_perfected_prompt(task_type, memory_context, str(errors), last_user_msg)
    # Ch·ªâ ch·∫°y Ensemble n·∫øu task kh√≥ ho·∫∑c ƒëang fix l·ªói
    use_ensemble = len(errors) > 0 or "complex" in task_type or "d·ª± √°n" in last_user_msg.lower()
    prompts = [base_prompt]
    if use_ensemble:
        # Th√™m 1 bi·∫øn th·ªÉ t·ªëi ∆∞u h√≥a ƒë·ªÉ so s√°nh
        prompts.append(base_prompt + "\n[DIRECTIVE]: OPTIMIZE for performance and brevity. Remove unnecessary comments.")
    # 3. PARALLEL EXECUTION (TƒÉng t·ªëc ƒë·ªô g·∫•p 3 l·∫ßn)
    # ============================================================================
    print(colored(f"‚ö° Running {len(prompts)} parallel chains...", "cyan"))
    # Chu·∫©n b·ªã batch inputs
    batch_inputs = [[SystemMessage(content=p)] + messages for p in prompts]
    
    try:
        # --- LOGIC FALLBACK QUAN TR·ªåNG ---
        # ∆Øu ti√™n 1: CODER_PRIMARY (DeepSeek)
        # ∆Øu ti√™n 2: LLM_GPT4 (GPT-4 Turbo)
        # ∆Øu ti√™n 3: LLM_CLAUDE (Claude 3.5 Sonnet)
        
        fallbacks = []
        if LLM_GPT4: fallbacks.append(LLM_GPT4)
        if LLM_CLAUDE: fallbacks.append(LLM_CLAUDE)
        
        # X√°c ƒë·ªãnh Primary Chain
        primary_chain = CODER_PRIMARY if CODER_PRIMARY else (LLM_GPT4 if LLM_GPT4 else LLM_CLAUDE)
        
        if not primary_chain:
            raise Exception("CRITICAL: Kh√¥ng c√≥ API n√†o ho·∫°t ƒë·ªông!")

        # K√≠ch ho·∫°t Fallback
        if fallbacks and primary_chain != fallbacks[0]: 
            final_chain = primary_chain.with_fallbacks(fallbacks)
            print(colored(f"üõ°Ô∏è Chain: {primary_chain.model_name} -> Fallbacks", "green"))
        else:
            final_chain = primary_chain

        # Th·ª±c thi
        responses = await final_chain.abatch(batch_inputs)
        
    except Exception as e:
        # Ghi log l·ªói chi ti·∫øt tr∆∞·ªõc khi fallback ƒë·ªÉ CEO bi·∫øt t·∫°i sao s·∫≠p
        error_detail = f"L·ªói th·ª±c thi song song (Parallel Batch): {str(e)}"
        print(colored(f"üö® {error_detail}", "red"))
        
        # C·∫≠p nh·∫≠t error_log v√†o state tr∆∞·ªõc khi tho√°t
        state["error_log"] = state.get("error_log", []) + [error_detail]
        
        return {"messages": [AIMessage(content="H·ªá th·ªëng qu√° t·∫£i.")], "next_step": "FINISH"}

    # 4. VALIDATION & SCORING
    # ============================================================================
    valid_results = []
    for i, res in enumerate(responses):
        code = extract_code_block(res.content)
        if not code: continue
        
        is_ok, msg = real_syntax_validator(code, "python")
        score = 50 if is_ok else 0
        if len(code) > 30: score += 10
        if "# filename:" in code: score += 10
        
        valid_results.append({"code": code, "reply": res.content, "score": score, "error": msg, "variant": i})

    # 5. SELECT BEST CANDIDATE
    # ============================================================================
    if valid_results:
        # L·∫•y ·ª©ng vi√™n c√≥ ƒëi·ªÉm cao nh·∫•t
        best_result = max(valid_results, key=lambda x: x['score'])
        
        # NG∆Ø·ª†NG CH·∫§P NH·∫¨N: 60 ƒëi·ªÉm (ƒê·ªß ƒë·ªÉ ch·∫°y)
        # (T√¥i h·∫° xu·ªëng 60 ƒë·ªÉ h·ªá th·ªëng linh ho·∫°t h∆°n, nh∆∞ng ch·ªâ l∆∞u b√†i m·∫´u khi ƒë·∫°t 80)
        if best_result['score'] >= 60: 
            print(colored(f"‚úÖ SELECTED Variant {best_result['variant']} (Score: {best_result['score']})", "green"))
            
            # [T·ª∞ H·ªåC]: Ch·ªâ l∆∞u nh·ªØng ƒëo·∫°n code ch·∫•t l∆∞·ª£ng cao (>= 80)
            if best_result['score'] >= 80:
                try:
                    # D√πng h√†m log chu·∫©n m·ªõi: log_training_data
                    # Input: User Prompt, Code AI, Score, T√™n Model
                    log_training_data(
                        user_prompt=messages[-1].content,
                        best_code=best_result['code'],
                        score=best_result['score'],
                        model_used="3-Tier-Squad" 
                    )
                except: pass    
                # except Exception as e:
                #     # N·∫øu l·ªói ghi file th√¨ b·ªè qua, kh√¥ng l√†m s·∫≠p lu·ªìng ch√≠nh
                #     print(colored(f"‚ö†Ô∏è Log Error: {e}", "yellow"))

            # TR·∫¢ V·ªÄ K·∫æT QU·∫¢ TH√ÄNH C√îNG
            return {
                "messages": [AIMessage(content=best_result['full_reply'])],
                "next_node": "Tester", # Chuy·ªÉn sang Tester ki·ªÉm tra
                "error_log": []        # X√≥a s·∫°ch l·ªói c≈© v√¨ ƒë√£ th√†nh c√¥ng
            }
        
        else:
            # TR∆Ø·ªúNG H·ª¢P: Code ƒëi·ªÉm th·∫•p ho·∫∑c l·ªói c√∫ ph√°p
            print(colored(f"‚ö†Ô∏è [CODER] Variant t·ªët nh·∫•t ch·ªâ ƒë·∫°t {best_result['score']}/100. Error: {best_result['error']}", "yellow"))
            
            # 1. Ki·ªÉm tra gi·ªõi h·∫°n th·ª≠ l·∫°i (Max 3 l·∫ßn ƒë·ªÉ tr√°nh l·∫∑p v√¥ t·∫≠n)
            if len(state.get("error_log", [])) >= 3:
                print(colored("üö® [CODER] ƒê√£ th·ª≠ 3 l·∫ßn kh√¥ng ƒë∆∞·ª£c. Chuy·ªÉn sang Fallback.", "red"))
                state["error_log"].append("L·ªói: AI kh√¥ng th·ªÉ t·ª± s·ª≠a code sau 3 l·∫ßn th·ª≠.")
                
                # G·ªçi h√†m fallback cu·ªëi c√πng (Code th·ªß c√¥ng ho·∫∑c b√°o l·ªói)
                return ultimate_fallback(state, messages)

            # 2. T·∫°o ph·∫£n h·ªìi l·ªói chi ti·∫øt ƒë·ªÉ AI t·ª± s·ª≠a
            error_feedback = (
                f"SYSTEM ALERT: Code b·∫°n vi·∫øt b·ªã l·ªói c√∫ ph√°p ho·∫∑c vi ph·∫°m quy chu·∫©n.\n"
                f"- Error Details: {best_result['error']}\n"
                f"- Score: {best_result['score']}/100\n"
                f"ACTION: H√£y vi·∫øt l·∫°i code m·ªõi, s·ª≠a tri·ªát ƒë·ªÉ l·ªói tr√™n."
            )
            
            # Tr·∫£ v·ªÅ state ƒë·ªÉ k√≠ch ho·∫°t v√≤ng l·∫∑p quay l·∫°i Coder
            return {
                "messages": [
                    AIMessage(content=best_result['code']), # G·ª≠i l·∫°i code sai
                    HumanMessage(content=error_feedback)    # K√®m l·ªùi nh·∫Øc s·ª≠a
                ], 
                "error_log": state.get("error_log", []) + [f"Syntax Error: {best_result.get('error')}"],
                "next_step": "Coder" # Ch·ªâ ƒë·ªãnh r√µ b∆∞·ªõc ti·∫øp theo l√† quay l·∫°i Coder
            }

    # TR∆Ø·ªúNG H·ª¢P: Kh√¥ng c√≥ variant n√†o (L·ªói API ho·∫∑c Prompt b·ªã ch·∫∑n)
    error_msg = "Kh√¥ng c√≥ k·∫øt qu·∫£ n√†o ƒë∆∞·ª£c t·∫°o ra t·ª´ batch execution."
    print(colored(f"‚ùå [CODER] {error_msg}", "red"))
    state["error_log"] = state.get("error_log", []) + [error_msg]
    
    return ultimate_fallback(state, messages)

# ============================================================================
# NODE: TESTER (K·ªπ s∆∞ Ki·ªÉm ƒë·ªãnh Ch·∫•t l∆∞·ª£ng - QA/QC)
# ============================================================================
def tester_node(state):
    """
    Agent Tester: Ki·ªÉm ƒë·ªãnh c√∫ ph√°p ƒëa ng√¥n ng·ªØ, qu√©t l·ªói b·∫£o m·∫≠t v√† tu√¢n th·ªß quy chu·∫©n.
    """
    print(colored("[üß™ TESTER] ƒêang ki·ªÉm ƒë·ªãnh ch·∫•t l∆∞·ª£ng m√£ ngu·ªìn...", "yellow", attrs=["bold"]))
    
    messages = state.get("messages", [])
    last_ai_msg = messages[-1].content
    
    # 1. Tr√≠ch xu·∫•t code block
    code_to_test = extract_code_block(last_ai_msg)
    
    if not code_to_test:
        print(colored("‚ùå [TESTER] Kh√¥ng t√¨m th·∫•y kh·ªëi code h·ª£p l·ªá!", "red"))
        return {
            "error_log": state.get("error_log", []) + ["L·ªñI: Kh√¥ng t√¨m th·∫•y kh·ªëi code ```."],
            "next_step": "Coder"
        }

    is_valid = True
    feedback = []

    # 2. KI·ªÇM ƒê·ªäNH THEO NG√îN NG·ªÆ
    
    # --- Tr∆∞·ªùng h·ª£p 1: Code Python ---
    if "def " in code_to_test or "import " in code_to_test:
        try:
            ast.parse(code_to_test)
            feedback.append("- C√∫ ph√°p Python: ƒê·∫°t chu·∫©n.")
            
            # Ki·ªÉm tra b·∫£o m·∫≠t s∆° b·ªô (V√≠ d·ª•: c·∫•m d√πng 'eval')
            if "eval(" in code_to_test or "os.system(" in code_to_test:
                is_valid = False
                feedback.append("- B·∫£o m·∫≠t: Ph√°t hi·ªán h√†m nguy hi·ªÉm (eval/system).")
                
        except SyntaxError as e:
            is_valid = False
            feedback.append(f"- C√∫ ph√°p Python: L·ªói t·∫°i d√≤ng {e.lineno}: {e.msg}")

    # --- Tr∆∞·ªùng h·ª£p 2: Code C++ / Arduino (Hardware) ---
    elif "#include" in code_to_test or "void setup()" in code_to_test:
        # Ki·ªÉm tra ƒë√≥ng m·ªü ngo·∫∑c ƒë∆°n gi·∫£n cho C++ (V√¨ Python kh√¥ng parse ƒë∆∞·ª£c C++)
        open_braces = code_to_test.count("{")
        close_braces = code_to_test.count("}")
        if open_braces != close_braces:
            is_valid = False
            feedback.append(f"- C√∫ ph√°p C++: M·∫•t c√¢n b·∫±ng d·∫•u ngo·∫∑c ({open_braces} m·ªü, {close_braces} ƒë√≥ng).")
        else:
            feedback.append("- C√∫ ph√°p C++: Ki·ªÉm tra c·∫•u tr√∫c ƒë√≥ng/m·ªü ƒë·∫°t.")

    # 3. QUY·∫æT ƒê·ªäNH H·∫¨U KI·ªÇM
    full_feedback = "\n".join(feedback)
    
    if is_valid:
        print(colored("‚úÖ [TESTER] M√£ ngu·ªìn ƒë·∫°t ti√™u chu·∫©n ch·∫•t l∆∞·ª£ng.", "green"))
        return {
            "error_log": [], # Clear log l·ªói
            "next_step": "Supervisor"
        }
    else:
        print(colored(f"‚ùå [TESTER] Ph√°t hi·ªán vi ph·∫°m:\n{full_feedback}", "red"))
        error_msg = HumanMessage(content=(
            f"‚ö†Ô∏è B√ÅO C√ÅO KI·ªÇM ƒê·ªäNH TH·∫§T B·∫†I:\n{full_feedback}\n\n"
            f"Vui l√≤ng s·ª≠a l·∫°i m√£ ngu·ªìn, ch√∫ tr·ªçng v√†o c√°c ƒëi·ªÉm vi ph·∫°m tr√™n."
        ))
        return {
            "messages": [error_msg],
            "error_log": state.get("error_log", []) + [full_feedback],
            "next_step": "Coder"
        }
    
# ============================================================================
# NODE: HARDWARE (Ki·∫øn tr√∫c s∆∞ Robotics & H·ªá th·ªëng nh√∫ng)
# ============================================================================
def hardware_node(state):
    """
    Agent Hardware Architect: Chuy√™n tr√°ch ESP32, Robotics v√† H·ªá th·ªëng nh√∫ng.
    N√¢ng c·∫•p: Tr√≠ch xu·∫•t BOM chu·∫©n cho Procurement v√† t·ªëi ∆∞u h√≥a PINOUT.
    """
    print(colored("[üõ†Ô∏è HARDWARE] ƒêang ki·∫øn tr√∫c h·ªá th·ªëng nh√∫ng...", "cyan", attrs=["bold"]))
    
    messages = state.get("messages", [])
    last_msg = messages[-1].content
    is_pure_hw = "[HARDWARE]" in last_msg # Nh·∫≠n di·ªán Tab K·ªπ thu·∫≠t

    prompt = (
        "B·∫°n l√† Hardware Architect cao c·∫•p t·∫°i AI Corporation. "
        f"\nY√äU C·∫¶U: {last_msg}"
        "\n\nC·∫§U TR√öC B√ÅO C√ÅO K·ª∏ THU·∫¨T:"
        "\n1. [DANH M·ª§C LINH KI·ªÜN - BOM]: Li·ªát k√™ d·∫°ng b·∫£ng: T√™n | Th√¥ng s·ªë | S·ªë l∆∞·ª£ng."
        "\n2. [S∆† ƒê·ªí CH√ÇN - PINOUT]: B·∫£ng k·∫øt n·ªëi chi ti·∫øt (VD: ESP32 GPIO21 -> LCD SDA)."
        "\n3. [FIRMWARE]: Code C++/Arduino t·ªëi ∆∞u, c√≥ comment gi·∫£i th√≠ch chuy√™n s√¢u."
        "\n4. [L∆ØU √ù V·∫¨N H√ÄNH]: C·∫£nh b√°o d√≤ng √°p, t·∫£n nhi·ªát v√† nhi·ªÖu t√≠n hi·ªáu."
        "\n\nB·∫ÆT BU·ªòC: Kh√¥ng d√πng emoji, ch·ªâ d√πng k√Ω t·ª± Latin/Ti·∫øng Vi·ªát chu·∫©n."
    )
    
    try:
        # GPT-4o l√† l·ª±a ch·ªçn s·ªë 1 cho vi·ªác tra c·ª©u s∆° ƒë·ªì ch√¢n (Data Sheets)
        response = LLM_GPT4.invoke([
            SystemMessage(content=prompt),
            HumanMessage(content=last_msg)
        ])
        
        # ƒê·ªäNH TUY·∫æN:
        # N·∫øu ·ªü Tab Hardware -> FINISH (Hi·ªán k·∫øt qu·∫£ ngay)
        # N·∫øu ·ªü lu·ªìng t·ª± ƒë·ªông -> Chuy·ªÉn sang Procurement ƒë·ªÉ b√°o gi√° linh ki·ªán
        next_destination = "FINISH" if is_pure_hw else "Procurement"

        return {
            "messages": [AIMessage(content=f"üõ†Ô∏è **[THI·∫æT K·∫æ K·ª∏ THU·∫¨T PH·∫¶N C·ª®NG]**\n\n{response.content}")],
            "next_step": next_destination
        }
        
    except Exception as e:
        # 1. Ghi log chi ti·∫øt ra Terminal ƒë·ªÉ CEO theo d√µi l·ªói v·∫≠t l√Ω
        error_detail = str(e)
        print(colored(f"üö® [HARDWARE ERROR]: {error_detail}", "red", attrs=["bold"]))
        
        # 2. Tr·∫£ v·ªÅ State chu·∫©n: 
        # - messages: Ph·∫£i l√† m·ªôt LIST ch·ª©a ƒë·ªëi t∆∞·ª£ng Message
        # - next_step: Ph·∫£i l√† m·ªôt CHU·ªñI (String) ƒë·ªãnh danh Node ti·∫øp theo
        return {
            "messages": [AIMessage(content=f"‚ùå **H·ªÜ TH·ªêNG C·∫¢NH B√ÅO HARDWARE**:\n\nƒê√£ x·∫£y ra s·ª± c·ªë k·ªπ thu·∫≠t: `{error_detail}`")], 
            "next_step": "FINISH" 
        }
#  ---- V·∫Ω 3D Plotly----
def engineering_node(state):
    """
    Agent CTO/Engineer: Thi·∫øt k·∫ø m√¥ h√¨nh 3D b·∫±ng Python Plotly.
    ƒê√£ n√¢ng c·∫•p: ƒê·∫£m b·∫£o m√£ ngu·ªìn chu·∫©n ƒë·ªÉ Dashboard th·ª±c thi v·∫Ω 3D.
    """
    print(colored("[‚öôÔ∏è ENGINEERING] ƒêang thi·∫øt k·∫ø c·∫•u tr√∫c 3D...", "blue", attrs=["bold"]))
    
    messages = state.get("messages", [])
    last_msg = messages[-1].content
    is_pure_eng = "[ENGINEERING]" in last_msg

    # 1. Prompt √©p AI vi·∫øt code s·∫°ch, kh√¥ng gi·∫£i th√≠ch th·ª´a
    prompt = (
        "B·∫°n l√† K·ªπ s∆∞ Thi·∫øt k·∫ø 3D chuy√™n nghi·ªáp. "
        "\nNHI·ªÜM V·ª§: Vi·∫øt code Python s·ª≠ d·ª•ng plotly.graph_objects ƒë·ªÉ t·∫°o m√¥ h√¨nh 3D."
        "\n\nY√äU C·∫¶U K·ª∏ THU·∫¨T:"
        "\n- Ch·ªâ tr·∫£ v·ªÅ duy nh·∫•t CODE BLOCK Python trong d·∫•u ```python."
        "\n- Code ph·∫£i t·∫°o ra ƒë·ªëi t∆∞·ª£ng t√™n l√† 'fig'."
        "\n- Ph·∫£i bao g·ªìm d·ªØ li·ªáu t·ªça ƒë·ªô (x, y, z) chi ti·∫øt cho m√¥ h√¨nh."
        "\n- N·∫øu l√† Robot, h√£y v·∫Ω r√µ c√°c kh·ªõp n·ªëi v√† c√°nh tay."
        "\n- KH√îNG gi·∫£i th√≠ch, KH√îNG nh·∫≠p vƒÉn b·∫£n ngo√†i code."
    )

    try:
        # 2. S·ª≠ d·ª•ng Claude 3.5 Sonnet (ƒê·ªânh cao v·ªÅ vi·∫øt code h√¨nh h·ªçc)
        response = LLM_CLAUDE.invoke([
            SystemMessage(content=prompt),
            HumanMessage(content=f"Y√™u c·∫ßu thi·∫øt k·∫ø: {last_msg}")
        ])
        
        # 3. ƒê·ªãnh tuy·∫øn
        next_destination = "FINISH" if is_pure_eng else "Procurement"

        return {
            "messages": [AIMessage(content=f"‚öôÔ∏è **[B·∫¢N THI·∫æT K·∫æ 3D H·ªÜ TH·ªêNG]**\n\n{response.content}")],
            "next_step": next_destination
        }
        
    except Exception as e:
        # 1. Ghi log l·ªói chi ti·∫øt ra Terminal v·ªõi m√†u ƒë·ªè ƒë·∫≠m ƒë·ªÉ d·ªÖ nh·∫≠n di·ªán
        error_detail = str(e)
        print(colored(f"üö® [ENGINEERING ERROR]: {error_detail}", "red", attrs=["bold"]))
        
        # 2. Tr·∫£ v·ªÅ State chu·∫©n cho LangGraph:
        # - messages: B·∫ÆT BU·ªòC l√† m·ªôt list ch·ª©a ƒë·ªëi t∆∞·ª£ng Message (kh√¥ng ƒë∆∞·ª£c g·ª≠i dict r·ªóng)
        # - next_step: B·∫ÆT BU·ªòC l√† m·ªôt chu·ªói (String) ƒë·ªÉ tr√°nh l·ªói bƒÉm d·ªØ li·ªáu
        return {
            "messages": [AIMessage(content=f"‚ùå **L·ªñI THI·∫æT K·∫æ K·ª∏ THU·∫¨T**:\n\nH·ªá th·ªëng g·∫∑p s·ª± c·ªë khi d·ª±ng m√¥ h√¨nh: `{error_detail}`")], 
            "next_step": "FINISH" 
        }
    
def publisher_node(state):
    """
    Agent Publisher: T·ªïng h·ª£p d·ªØ li·ªáu t·ª´ t·∫•t c·∫£ c√°c Agent ƒë·ªÉ xu·∫•t b·∫£n h·ªì s∆° d·ª± √°n.
    """
    print(colored("[üìú PUBLISHER] ƒêang t·ªïng h·ª£p h·ªì s∆° d·ª± √°n cu·ªëi c√πng...", "green", attrs=["bold"]))
    
    messages = state.get("messages", [])
    
    # 1. PH√ÇN LO·∫†I D·ªÆ LI·ªÜU T·ª∞ ƒê·ªòNG
    research_report = ""
    investment_plan = ""
    technical_specs = ""
    creative_content = ""
    images = []

    for msg in messages:
        content = msg.content
        if "üîç [B√ÅO C√ÅO NGHI√äN C·ª®U]" in content: research_report = content
        if "üí∞ [H·ªí S∆† TH·∫®M ƒê·ªäNH ƒê·∫¶U T∆Ø]" in content: investment_plan = content
        if "‚öôÔ∏è [B·∫¢N THI·∫æT K·∫æ 3D]" in content: technical_specs = content
        if "üñãÔ∏è [T√ÅC PH·∫®M S√ÅNG T√ÅC]" in content: creative_content = content
        if "![·∫¢nh minh h·ªça]" in content:
            # Tr√≠ch xu·∫•t URL ·∫£nh
            urls = [line for line in content.split('\n') if "https://" in line]
            images.extend(urls)

    # 2. T·ªîNG H·ª¢P PROMPT XU·∫§T B·∫¢N
    publish_prompt = (
        "B·∫°n l√† Chuy√™n gia tr√¨nh b√†y vƒÉn b·∫£n c·∫•p cao. H√£y t·ªïng h·ª£p c√°c d·ªØ li·ªáu tr√™n th√†nh m·ªôt "
        "B√°o c√°o D·ª± √°n ho√†n ch·ªânh, chuy√™n nghi·ªáp. S·ª≠ d·ª•ng ti√™u ƒë·ªÅ, m·ª•c l·ª•c v√† ƒë·ªãnh d·∫°ng Markdown chu·∫©n."
        "\nTh·ª© t·ª±: 1. T·ªïng quan -> 2. Th·ªã tr∆∞·ªùng -> 3. T√†i ch√≠nh -> 4. K·ªπ thu·∫≠t -> 5. Ph·ª• l·ª•c h√¨nh ·∫£nh."
    )

    response = LLM_GEMINI.invoke([
        SystemMessage(content=publish_prompt),
        HumanMessage(content=f"D·ªØ li·ªáu gom ƒë∆∞·ª£c:\n{research_report}\n{investment_plan}\n{technical_specs}\n{creative_content}")
    ])

    return {
        "messages": [AIMessage(content=f"üìú **[H·ªí S∆† D·ª∞ √ÅN T·ªîNG TH·ªÇ - FINAL]**\n\n{response.content}")],
        "next_step": "FINISH"
    }
# ============================================================================
# NODE: IoT ENGINEER (K·ªπ s∆∞ V·∫≠n h√†nh & K·∫øt n·ªëi thi·∫øt b·ªã)
# ============================================================================
def iot_node(state):
    """
    Agent IoT: K·∫øt h·ª£p L·∫≠p tr√¨nh Firmware (Thi·∫øt k·∫ø) v√† Th·ª±c thi l·ªánh (V·∫≠n h√†nh).
    """
    print(colored("[ü§ñ IoT ENGINEER] ƒêang x·ª≠ l√Ω giao th·ª©c v√† thi·∫øt b·ªã...", "magenta", attrs=["bold"]))
    
    messages = state.get("messages", [])
    last_msg = messages[-1].content
    is_pure_iot = "[IOT]" in last_msg

    # 1. KI·ªÇM TRA NG·ªÆ C·∫¢NH: ƒê√¢y l√† l·ªánh ƒëi·ªÅu khi·ªÉn (V·∫≠n h√†nh) hay y√™u c·∫ßu vi·∫øt code (Thi·∫øt k·∫ø)?
    is_command = any(word in last_msg.upper() for word in ["B·∫¨T", "T·∫ÆT", "TURN", "CONTROL", "CH·∫†Y"])

    if is_command:
        # --- NH√ÅNH 1: V·∫¨N H√ÄNH THI·∫æT B·ªä TH·∫¨T ---
        analysis_prompt = f"Tr√≠ch xu·∫•t l·ªánh ƒëi·ªÅu khi·ªÉn t·ª´: '{last_msg}'. Ch·ªâ tr·∫£ v·ªÅ m√£ l·ªánh Uppercase."
        command_code = LLM_GPT4.invoke([SystemMessage(content=analysis_prompt)]).content.strip()
        
        try:
            # 1. G·ªçi tool hardware_controller ƒë·ªÉ ra l·ªánh cho thi·∫øt b·ªã th·ª±c t·∫ø
            hardware_response = hardware_controller.invoke(command_code)
            report = (f"üì° **[K·∫æT QU·∫¢ V·∫¨N H√ÄNH]**\n\n- M√£ l·ªánh: `{command_code}`\n- Tr·∫°ng th√°i: {hardware_response}")
            
            # N·∫øu ch·∫°y Tab IOT ri√™ng bi·ªát -> K·∫øt th√∫c. N·∫øu ch·∫°y lu·ªìng t·ª± ƒë·ªông -> V·ªÅ Supervisor b√°o c√°o.
            return {
                "messages": [AIMessage(content=report)], 
                "next_step": "FINISH" if is_pure_iot else "Supervisor"
            }
            
        except Exception as e:
            # 2. X·ª≠ l√Ω l·ªói k·∫øt n·ªëi ho·∫∑c th·ª±c thi thi·∫øt b·ªã
            error_detail = str(e)
            print(colored(f"üö® [IOT HARDWARE ERROR]: {error_detail}", "red", attrs=["bold"]))
            
            # Tr·∫£ v·ªÅ AIMessage chu·∫©n ƒë·ªÉ Dashboard hi·ªÉn th·ªã ƒë√∫ng ID: IoT_Engineer
            return {
                "messages": [AIMessage(content=f"‚ùå **L·ªñI K·∫æT N·ªêI THI·∫æT B·ªä**:\n\nKh√¥ng th·ªÉ th·ª±c thi l·ªánh `{command_code}`. \nChi ti·∫øt: `{error_detail}`")], 
                "next_step": "Supervisor" # Quay v·ªÅ ƒë·ªÉ Supervisor ra l·ªánh ki·ªÉm tra l·∫°i ho·∫∑c ƒë·ªïi ph∆∞∆°ng √°n
            }
    else:
        # --- NH√ÅNH 2: THI·∫æT K·∫æ FIRMWARE (D√†nh cho d·ª± √°n m·ªõi) ---
        # L·∫•y b·∫£n v·∫Ω Pinout t·ª´ Hardware Node n·∫øu c√≥
        hw_context = next((m.content for m in reversed(messages) if "üõ†Ô∏è" in m.content), "Ch∆∞a c√≥ s∆° ƒë·ªì ch√¢n.")
        
        design_prompt = (
            "B·∫°n l√† K·ªπ s∆∞ Firmware IoT. H√£y vi·∫øt code C++/Arduino ƒëi·ªÅu khi·ªÉn h·ªá th·ªëng d·ª±a tr√™n s∆° ƒë·ªì ch√¢n sau."
            f"\nS∆° ƒë·ªì: {hw_context}"
            "\nY√™u c·∫ßu: Vi·∫øt code c√≥ k·∫øt n·ªëi WiFi/MQTT v√† qu·∫£n l√Ω l·ªói k·∫øt n·ªëi."
        )
        
        response = LLM_CLAUDE.invoke([SystemMessage(content=design_prompt), HumanMessage(content=last_msg)])
        
        return {
            "messages": [AIMessage(content=f"üì° **[FIRMWARE & GIAO TH·ª®C ƒêI·ªÄU KHI·ªÇN]**\n\n{response.content}")],
            "next_step": "FINISH" if is_pure_iot else "Supervisor"
        }
# ============================================================================
# NODE: PROCUREMENT (Tr∆∞·ªüng ph√≤ng Thu mua & Qu·∫£n l√Ω Chu·ªói cung ·ª©ng)
# ============================================================================
BUYER_PROFILE = {
    "address": "Phan Thi·∫øt, B√¨nh Thu·∫≠n, Vi·ªát Nam",
    "delivery_method": "Fast Shipping",
    "accounts": ["Shopee_API_Key", "Taobao_Token", "Mouser_ID"]
}
def procurement_node(state):
    """
    Agent Procurement: T·ªëi ∆∞u h√≥a chu·ªói cung ·ª©ng d·ª±a tr√™n v·ªã tr√≠ th·ª±c t·∫ø c·ªßa CEO.
    """
    print(colored("[üõí PROCUREMENT] ƒêang t·ªëi ∆∞u h√≥a l·ªô tr√¨nh h√†ng h√≥a v·ªÅ Phan Thi·∫øt...", "yellow", attrs=["bold"]))
    
    # 1. Load h·ªì s∆° mua h√†ng (Mockup)
    buyer_config = BUYER_PROFILE # L·∫•y t·ª´ file c·∫•u h√¨nh tr√™n
    
    messages = state.get("messages", [])
    hw_report = next((m.content for m in reversed(messages) if "üõ†Ô∏è" in m.content), "Kh√¥ng t√¨m th·∫•y danh m·ª•c linh ki·ªán.")

    # 2. X√¢y d·ª±ng l·ªánh truy v·∫•n chuy√™n s√¢u
    prompt = (
        "B·∫°n l√† Chuy√™n gia Logisitics v√† Thu mua."
        f"\nƒê·ªäA CH·ªà NH·∫¨N: {buyer_config['address']}"
        f"\nDANH M·ª§C: {hw_report}"
        "\n\nNHI·ªÜM V·ª§:"
        "\n1. T√åM GI√Å: Tra c·ª©u gi√° th·ª±c t·∫ø nƒÉm 2026 tr√™n Mouser, Digikey v√† Shopee."
        "\n2. T√çNH PH√ç V·∫¨N CHUY·ªÇN: ∆Ø·ªõc t√≠nh ph√≠ ship v√† thu·∫ø nh·∫≠p kh·∫©u v·ªÅ Vi·ªát Nam."
        "\n3. L·∫¨P GI·ªé H√ÄNG: T·∫°o danh s√°ch link s·∫£n ph·∫©m s·∫µn s√†ng ƒë·ªÉ thanh to√°n."
    )

    # S·ª≠ d·ª•ng Perplexity ƒë·ªÉ check gi√° th·ª±c t·∫ø
    response = LLM_PERPLEXITY.invoke([SystemMessage(content=prompt)])

    return {
        "messages": [AIMessage(content=f"üõí **[PHI·∫æU ƒê·ªÄ XU·∫§T MUA S·∫ÆM & V·∫¨N CHUY·ªÇN]**\n\n{response.content}")],
        "next_step": "Investment" # Chuy·ªÉn sang T√†i ch√≠nh ƒë·ªÉ CEO duy·ªát chi
    }
# ============================================================================
# NODE: RESEARCHER (Chuy√™n gia Ph√¢n t√≠ch Th·ªã tr∆∞·ªùng & ƒê·ªëi th·ªß)
# ============================================================================
def researcher_node(state):
    """
    Agent Researcher: Chuy√™n gia ph√¢n t√≠ch th·ªã tr∆∞·ªùng 2026.
    N√¢ng c·∫•p: T·ª± ƒë·ªông nh·∫≠n di·ªán Tag ng·ªØ c·∫£nh ƒë·ªÉ quy·∫øt ƒë·ªãnh h√†nh ƒë·ªông ti·∫øp theo.
    """
    print(colored("[üîç RESEARCHER] ƒêang th·ª±c thi nhi·ªám v·ª• th√°m m√£ th·ªã tr∆∞·ªùng...", "cyan", attrs=["bold"]))
    
    # 1. Tr√≠ch xu·∫•t tin nh·∫Øn v√† nh·∫≠n di·ªán Tag
    messages = state.get("messages", [])
    last_msg_content = messages[-1].content
    
    # Ki·ªÉm tra xem CEO c√≥ ƒëang ·ªü ch·∫ø ƒë·ªô RESEARCH chuy√™n bi·ªát kh√¥ng
    is_pure_research = "[RESEARCH]" in last_msg_content
    
    # L√†m s·∫°ch c√¢u l·ªánh (lo·∫°i b·ªè Tag tr∆∞·ªõc khi g·ª≠i cho Perplexity)
    clean_query = last_msg_content.replace("[RESEARCH]", "").strip()

    # 2. X√¢y d·ª±ng Prompt Si√™u C·∫•u Tr√∫c (S·ª≠ d·ª•ng 4 c·ªôt tr·ª•)
    search_prompt = (
        f"Nhi·ªám v·ª•: Ph√¢n t√≠ch th·ªã tr∆∞·ªùng 2026 cho: '{clean_query}'."
        "\n\nY√äU C·∫¶U B√ÅO C√ÅO 4 C·ªòT TR·ª§:"
        "\n1. [D·ªÆ LI·ªÜU Vƒ® M√î]: T√¨nh h√¨nh th·ªã tr∆∞·ªùng v√† c√¥ng ngh·ªá m·ªõi nh·∫•t."
        "\n2. [BI·∫æN ƒê·ªòNG TH·ª∞C T·∫æ]: Xu h∆∞·ªõng ti√™u d√πng v√† 'n·ªói ƒëau' kh√°ch h√†ng."
        "\n3. [ƒê·ªêI TH·ª¶ TR·ª∞C DI·ªÜN]: Li·ªát k√™ 3 ƒë·ªëi th·ªß v√† l·ª£i th·∫ø c·ªßa h·ªç."
        "\n4. [C∆† H·ªòI CHO CEO]: Insight quan tr·ªçng v√† d·ª± b√°o 12 th√°ng t·ªõi."
        "\n\nƒê·ªãnh d·∫°ng: Markdown chuy√™n nghi·ªáp, c√≥ b·∫£ng so s√°nh."
    )
    
    try:
        # 3. Tri·ªáu h·ªìi Perplexity
        response = LLM_PERPLEXITY.invoke([
            SystemMessage(content="B·∫°n l√† Chief Research Officer. Ch·ªâ tr·∫£ v·ªÅ d·ªØ li·ªáu th·ª±c t·∫ø 2026, KH√îNG HTML."),
            HumanMessage(content=search_prompt)
        ])
        raw_res = response.content

        # --- T·∫¶NG PH√íNG TH·ª¶ 1: CH·∫∂N HTML & L·ªñI 401 ---
        if any(x in raw_res.lower() for x in ["<html>", "401 authorization", "cloudflare"]):
            return {
                "messages": [AIMessage(content="üö® [H·ªÜ TH·ªêNG] L·ªói k·∫øt n·ªëi ngu·ªìn tin (API 401). CEO h√£y ki·ªÉm tra l·∫°i Key Perplexity.")],
                "next_step": "FINISH" # D·ª´ng ngay l·∫≠p t·ª©c ƒë·ªÉ b·∫£o v·ªá t√†i nguy√™n
            }

        # --- T·∫¶NG PH√íNG TH·ª¶ 2: X·ª¨ L√ù K·∫æT QU·∫¢ TH√ÄNH C√îNG ---
        report_content = f"üîç **[B√ÅO C√ÅO CRO - {clean_query.upper()}]**\n\n{raw_res}"
        if is_pure_research:
            # N·∫øu CEO ch·ªâ mu·ªën nghi√™n c·ª©u (Tab Research), k·∫øt th√∫c t·∫°i ƒë√¢y.
            next_destination = "Secretary"
        else:
            # Thay v√¨ st.session_state, ta d√πng task_type ƒë∆∞·ª£c Dashboard g·ª≠i qua Server
            if state.get("task_type") == "dynamic":
                next_destination = "Orchestrator"
            else:
                next_destination = "Supervisor"

        return {
            "messages": [AIMessage(content=report_content)],
            "next_step": next_destination,
            "current_agent": "Researcher" # ƒê·ªãnh danh ƒë·ªÉ Orchestrator bi·∫øt ai v·ª´a ho√†n th√†nh b√°o c√°o
        }

    except Exception as e:
        # T·∫¶NG PH√íNG TH·ª¶ 3: NGO·∫†I L·ªÜ
        print(colored(f"L·ªói Researcher: {e}", "red"))
        return {
            "messages": [AIMessage(content=f"‚ö†Ô∏è Tr·ª•c tr·∫∑c k·ªπ thu·∫≠t khi qu√©t d·ªØ li·ªáu: {str(e)}")],
            "next_step": "FINISH" 
        }

#  ---- T√†i Ch√≠nh----
def investment_node(state):
    """
    Agent CFO: Th·∫©m ƒë·ªãnh t√†i ch√≠nh v√† ROI.
    ƒê√£ n√¢ng c·∫•p: T·ª± ƒë·ªông ng·∫Øt lu·ªìng (FINISH) n·∫øu ·ªü ch·∫ø ƒë·ªô chuy√™n bi·ªát.
    """
    print(colored("[üí∞ INVESTMENT] ƒêang th·∫©m ƒë·ªãnh t√†i ch√≠nh d·ª± √°n...", "green", attrs=["bold"]))
    
    messages = state.get("messages", [])
    last_msg = messages[-1].content
    is_pure_invest = "[INVEST]" in last_msg
    
    # L·∫•y 3 tin nh·∫Øn g·∫ßn nh·∫•t ƒë·ªÉ c√≥ ƒë·ªß ng·ªØ c·∫£nh (B√°o c√°o Researcher + Coder...)
    context = "\n".join([m.content for m in messages[-3:]])
    
    prompt = (
        "B·∫°n l√† Gi√°m ƒë·ªëc T√†i ch√≠nh (CFO) c·ªßa AI Corporation. "
        "\nNHI·ªÜM V·ª§: L·∫≠p b·∫£ng ph√¢n t√≠ch CAPEX, OPEX, ROI v√† r·ªßi ro t√†i ch√≠nh."
        "\n\nY√äU C·∫¶U:"
        "\n- Tr√¨nh b√†y b·∫£ng Markdown s·∫°ch s·∫Ω."
        "\n- K·∫øt lu·∫≠n r√µ r√†ng: 'ƒê·∫¶U T∆Ø', 'THEO D√ïI' ho·∫∑c 'LO·∫†I B·ªé'."
    )
    
    try:
        # ∆Øu ti√™n GPT-4 cho t√≠nh to√°n con s·ªë ƒë·ªÉ tr√°nh sai s√≥t logic
        response = LLM_MAIN.invoke([
            SystemMessage(content=prompt), 
            HumanMessage(content=f"D·ªØ li·ªáu d·ª± √°n: {context}")
        ])
        
        # N·∫øu CEO ch·ªçn Tab INVEST -> Tr·∫£ k·∫øt qu·∫£ v√† FINISH (Nhanh)
        # N·∫øu ƒëang ch·∫°y lu·ªìng t·ª± ƒë·ªông -> Quay l·∫°i Supervisor
        next_destination = "FINISH" if is_pure_invest else "Supervisor"

        return {
            "messages": [AIMessage(content=f"üí∞ **[H·ªí S∆† TH·∫®M ƒê·ªäNH ƒê·∫¶U T∆Ø]**\n\n{response.content}")],
            "next_step": next_destination
        }
    except Exception as e:
        return {
            "messages": [AIMessage(content=f"‚ö†Ô∏è S·ª± c·ªë t√†i ch√≠nh: {str(e)}")],
            "next_step": "FINISH"
        }

#  ---- Ph√°p l√Ω----
def legal_node(state):
    """
    Agent Legal (CLO): R√† so√°t to√†n b·ªô d·ª± √°n tr∆∞·ªõc khi xu·∫•t b·∫£n.
    ƒê√£ n√¢ng c·∫•p: ƒê·ªçc to√†n b·ªô l·ªãch s·ª≠ ƒë·ªÉ ph√°t hi·ªán r·ªßi ro xuy√™n su·ªët.
    """
    print(colored("[‚öñÔ∏è LEGAL] Lu·∫≠t s∆∞ ƒëang r√† so√°t to√†n b·ªô h·ªì s∆° d·ª± √°n...", "red", attrs=["bold"]))
    
    messages = state.get("messages", [])
    last_msg = messages[-1].content
    is_pure_legal = "[LEGAL]" in last_msg
    
    # 1. T·ªîNG H·ª¢P H·ªí S∆†: Lu·∫≠t s∆∞ ph·∫£i ƒë·ªçc h·∫øt c√°c "cam k·∫øt" c·ªßa Agent kh√°c
    # Gom 10-15 tin nh·∫Øn ƒë·ªÉ th·∫•y to√†n b·ªô lu·ªìng t·ª´ K·ªπ thu·∫≠t ƒë·∫øn Marketing
    full_project_context = "\n".join([f"[{m.type.upper()}]: {m.content[:300]}..." for m in messages[-15:]])

    prompt = (
        "B·∫°n l√† Gi√°m ƒë·ªëc Ph√°p l√Ω (CLO) c·ªßa AI Corporation. "
        "\nNHI·ªÜM V·ª§: Th·∫©m ƒë·ªãnh ph√°p l√Ω v√† Qu·∫£n tr·ªã r·ªßi ro d·ª±a tr√™n H·ªí S∆† D·ª∞ √ÅN ƒë∆∞·ª£c cung c·∫•p."
        "\n\nY√äU C·∫¶U CHI·∫æN L∆Ø·ª¢C:"
        "\n1. R√Ä SO√ÅT IP: Ki·ªÉm tra b·∫£n quy·ªÅn h√¨nh ·∫£nh (Artist) v√† m√£ ngu·ªìn (Coder)."
        "\n2. TU√ÇN TH·ª¶: ƒê·ªëi chi·∫øu v·ªõi Lu·∫≠t An ninh m·∫°ng VN v√† GDPR."
        "\n3. SO·∫†N TH·∫¢O: ƒê∆∞a ra khung ƒêi·ªÅu kho·∫£n s·ª≠ d·ª•ng (ToS) v√† NDA m·∫´u cho d·ª± √°n."
        "\n4. K·∫æT LU·∫¨N: Ghi r√µ 'AN TO√ÄN' ho·∫∑c 'C·∫¢NH B√ÅO NGUY HI·ªÇM'."
    )
    
    try:
        # S·ª≠ d·ª•ng GPT-4o ƒë·ªÉ c√≥ t∆∞ duy l·∫≠p lu·∫≠n ph√°p lu·∫≠t s·∫Øc b√©n nh·∫•t
        response = LLM_GPT4.invoke([
            SystemMessage(content=prompt),
            HumanMessage(content=f"H·ªí S∆† D·ª∞ √ÅN C·∫¶N TH·∫®M ƒê·ªäNH:\n{full_project_context}\n\nY√äU C·∫¶U B·ªî SUNG: {last_msg}")
        ])
        
        # N·∫øu CEO ch·ªçn Tab Legal ri√™ng bi·ªát th√¨ k·∫øt th√∫c lu√¥n
        next_destination = "FINISH" if is_pure_legal else "Supervisor"

        return {
            "messages": [AIMessage(content=f"‚öñÔ∏è **[B√ÅO C√ÅO PH√ÅP L√ù & R·ª¶I RO CHI TI·∫æT]**\n\n{response.content}")],
            "next_step": next_destination
        }

    except Exception as e:
        # 1. Ghi log l·ªói ph√°p l√Ω ra Terminal ƒë·ªÉ CEO gi√°m s√°t r·ªßi ro h·ªá th·ªëng
        error_detail = str(e)
        print(colored(f"üö® [LEGAL CRITICAL ERROR]: {error_detail}", "red", attrs=["bold"]))
        
        # 2. Tr·∫£ v·ªÅ State chu·∫©n cho LangGraph
        # ƒê·∫£m b·∫£o next_step l√† "FINISH" ƒë·ªÉ ng·∫Øt lu·ªìng an to√†n khi c√≥ s·ª± c·ªë ph√°p l√Ω
        return {
            "messages": [AIMessage(content=f"‚ùå **C·∫¢NH B√ÅO PH√ÅP L√ù KH·∫®N C·∫§P**:\n\nQu√° tr√¨nh r√† so√°t b·ªã gi√°n ƒëo·∫°n: `{error_detail}`\n\nKhuy·∫øn ngh·ªã: CEO ki·ªÉm tra l·∫°i c√°c ƒëi·ªÅu kho·∫£n ƒë·∫ßu v√†o.")], 
            "next_step": "FINISH" 
        }
#  ---- Nh√¢n S·ª± ----
def hr_orchestrator_node(state):
    """
    Agent HR - B·ªô ƒëi·ªÅu ph·ªëi nh√¢n s·ª± & quy tr√¨nh:
    Ki·ªÉm tra xem CEO c√≥ thi·∫øt l·∫≠p k·ªãch b·∫£n t·ª± ƒë·ªông hay kh√¥ng.
    """
    print(colored("[üë• HR ORCHESTRATOR] ƒêang ki·ªÉm so√°t lu·ªìng v·∫≠n h√†nh...", "cyan", attrs=["bold"]))
    
    # 1. Ki·ªÉm tra xem c√≥ b·∫£n ƒë·ªì quy tr√¨nh (Workflow Map) n√†o ƒë∆∞·ª£c CEO v·∫Ω kh√¥ng
    workflow_script = state.get("custom_workflow", None) 
    
    if workflow_script:
        # --- CH·∫æ ƒê·ªò T·ª∞ ƒê·ªòNG (D·ª∞A TR√äN THI·∫æT L·∫¨P K√âO TH·∫¢) ---
        current_step = state.get("current_step_index", 0)
        target_node = workflow_script[current_step]
        
        print(colored(f"--> Theo k·ªãch b·∫£n CEO: Chuy·ªÉn sang {target_node}", "green"))
        
        # B√°o c√°o k·∫øt qu·∫£ ch·∫∑ng tr∆∞·ªõc v√† xin √Ω ki·∫øn duy·ªát
        return {
            "messages": [AIMessage(content=f"‚úÖ Giai ƒëo·∫°n {current_step} ho√†n t·∫•t. Ch·ªù CEO ph√™ duy·ªát ƒë·ªÉ sang {target_node}.")],
            "next_step": target_node,
            "current_step_index": current_step + 1
        }
    else:
        # --- CH·∫æ ƒê·ªò M·∫∂C ƒê·ªäNH (AI T·ª∞ SUY LU·∫¨N) ---
        print(colored("--> Ch·∫ø ƒë·ªô t·ª± ƒë·ªông: AI ƒëang ƒëi·ªÅu ph·ªëi theo ng·ªØ c·∫£nh...", "white"))
        # G·ªçi l·∫°i logic Supervisor c≈© c·ªßa ng√†i
        return {"next_step": "Supervisor"}

def secretary_node(state):
    """
    SECRETARY V3: COMMUNICATOR - C·∫¶U N·ªêI TH√îNG MINH
    Bi·∫øt c√°ch di·ªÖn ƒë·∫°t l·∫°i k·∫øt qu·∫£ t·ª´ c√°c b·ªô ph·∫≠n kh√¥ khan (Coder, Researcher) 
    th√†nh ng√¥n ng·ªØ con ng∆∞·ªùi d·ªÖ hi·ªÉu cho CEO.
    """
    print(colored("[üó£Ô∏è COMMUNICATOR] ƒêang bi√™n t·∫≠p l·∫°i n·ªôi dung...", "magenta", attrs=["bold"]))
    
    messages = state.get("messages", [])
    last_agent = state.get("current_agent", "Unknown")
    
    # L·∫•y to√†n b·ªô ng·ªØ c·∫£nh ƒë·ªÉ hi·ªÉu chuy·ªán g√¨ v·ª´a x·∫£y ra
    context = "\n".join([f"{m.type}: {m.content}" for m in messages[-3:]])

    # Prompt d·∫°y Th∆∞ k√Ω c√°ch n√≥i chuy·ªán
    prompt = (
        "B·∫°n l√† Tr·ª£ l√Ω C√° nh√¢n Th√¥ng minh c·ªßa CEO. C√°c b·ªô ph·∫≠n chuy√™n m√¥n (Coder, Artist...) v·ª´a g·ª≠i k·∫øt qu·∫£ l√™n.\n"
        "Nhi·ªám v·ª• c·ªßa b·∫°n: DI·ªÑN ƒê·∫†T L·∫†I k·∫øt qu·∫£ ƒë√≥ m·ªôt c√°ch t·ª± nhi√™n, chuy√™n nghi·ªáp.\n"
        "QUY T·∫ÆC:"
        "\n1. N·∫øu c√≥ H√åNH ·∫¢NH/CODE: Ph·∫£i hi·ªÉn th·ªã r√µ r√†ng (Gi·ªØ nguy√™n link/block code)."
        "\n2. N·∫øu l√† L·ªúI N√ìI: H√£y t√≥m t·∫Øt l·∫°i ng·∫Øn g·ªçn, d√πng gi·ªçng vƒÉn ƒë·ªëi tho·∫°i ('Th∆∞a CEO', 'T√¥i ƒë√£ ho√†n th√†nh...')."
        "\n3. KH√îNG b√°o c√°o m√°y m√≥c ki·ªÉu 'B∆∞·ªõc 1, B∆∞·ªõc 2'. H√£y n√≥i nh∆∞ ng∆∞·ªùi v·ªõi ng∆∞·ªùi."
        f"\n\nNG·ªÆ C·∫¢NH V·ª™A QUA:\n{context}"
    )

    try:
        response = LLM_GEMINI.invoke([SystemMessage(content=prompt)])
        
        # Ghi log (V·∫´n gi·ªØ ch·ª©c nƒÉng l∆∞u tr·ªØ ng·∫ßm)
        with open(f"Chat_Log_{int(time.time())}.txt", "w", encoding="utf-8") as f:
            f.write(response.content)

        return {
            "messages": [AIMessage(content=response.content)],
            "next_step": "FINISH"
        }
    except:
        return {"next_step": "FINISH"}
# ============================================================================
# NODE: MARKETING NODE (Gi√°m ƒë·ªëc Marketing - CMO)
# ============================================================================
def marketing_node(state):
    """
    Agent CMO: Chuy√™n gia Marketing v√† TƒÉng tr∆∞·ªüng.
    ƒê√£ n√¢ng c·∫•p: T·ª± ƒë·ªông ƒë·ªÅ xu·∫•t Visual Prompt cho Artist ƒë·ªÉ thi·∫øt k·∫ø ·∫£nh qu·∫£ng c√°o.
    """
    print(colored("[üì¢ MARKETING] ƒêang l·∫≠p chi·∫øn d·ªãch qu·∫£ng b√° b√πng n·ªï...", "yellow", attrs=["bold"]))
    
    messages = state.get("messages", [])
    last_msg = messages[-1].content
    is_pure_mkt = "[MARKETING]" in last_msg
    
    # L·∫•y ng·ªØ c·∫£nh s√¢u t·ª´ k·ªπ thu·∫≠t v√† t√†i ch√≠nh ƒë·ªÉ vi·∫øt b√†i c√≥ s·ª©c thuy·∫øt ph·ª•c
    project_context = "\n".join([m.content for m in messages[-5:]])
    
    prompt = (
        "B·∫°n l√† Gi√°m ƒë·ªëc Marketing (CMO) c·ªßa AI Corporation. "
        "\nNHI·ªÜM V·ª§: X√¢y d·ª±ng b·ªô n·ªôi dung qu·∫£ng b√° ƒëa k√™nh d·ª±a tr√™n th√†nh ph·∫©m k·ªπ thu·∫≠t."
        "\n\nY√äU C·∫¶U CHI·∫æN L∆Ø·ª¢C:"
        "\n- [INSIGHT]: D√πng d·ªØ li·ªáu k·ªπ thu·∫≠t ƒë·ªÉ n√™u b·∫≠t l·ª£i √≠ch cho ng∆∞·ªùi d√πng."
        "\n- [FACEBOOK]: M√¥ h√¨nh PAS, phong c√°ch th√¢n thi·ªán."
        "\n- [LINKEDIN]: M√¥ h√¨nh chuy√™n gia, t·∫≠p trung v√†o ROI v√† t√≠nh b·ªÅn v·ªØng."
        "\n- [VISUAL PROMPT]: QUAN TR·ªåNG! ƒê∆∞a ra 2 m√¥ t·∫£ h√¨nh ·∫£nh (ti·∫øng Anh) ƒë·ªÉ Agent Artist v·∫Ω ·∫£nh qu·∫£ng c√°o."
    )

    try:
        response = LLM_GPT4.invoke([
            SystemMessage(content=prompt),
            HumanMessage(content=f"D·ªØ li·ªáu s·∫£n ph·∫©m:\n{project_context}")
        ])

        # ƒê·ªäNH TUY·∫æN TH√îNG MINH:
        # N·∫øu CEO c·∫ßn ·∫£nh minh h·ªça ngay, c√≥ th·ªÉ chuy·ªÉn sang Artist
        # N·∫øu kh√¥ng, FINISH ƒë·ªÉ hi·ªán n·ªôi dung.
        next_destination = "FINISH" if is_pure_mkt else "Supervisor"

        return {
            "messages": [AIMessage(content=f"üì¢ **[CHI·∫æN D·ªäCH MARKETING ƒêA K√äNH]**\n\n{response.content}")],
            "next_step": next_destination
        }
        
    except Exception as e:
        # 1. Ghi log l·ªói Marketing ra Terminal ƒë·ªÉ CEO theo d√µi hi·ªáu su·∫•t chi·∫øn d·ªãch
        error_detail = str(e)
        print(colored(f"üö® [MARKETING CRITICAL ERROR]: {error_detail}", "red", attrs=["bold"]))
        
        # 2. Tr·∫£ v·ªÅ State chu·∫©n cho LangGraph
        # ƒê·∫£m b·∫£o messages l√† LIST v√† next_step l√† STRING "FINISH"
        return {
            "messages": [AIMessage(content=f"‚ùå **S·ª∞ C·ªê CHI·∫æN D·ªäCH MARKETING**:\n\nQu√° tr√¨nh l·∫≠p k·∫ø ho·∫°ch b·ªã gi√°n ƒëo·∫°n: `{error_detail}`\n\nKhuy·∫øn ngh·ªã: CEO h√£y ki·ªÉm tra l·∫°i y√™u c·∫ßu m·ª•c ti√™u ho·∫∑c ng√¢n s√°ch.")], 
            "next_step": "FINISH" 
        }
#  ---- V·∫Ω Thi·∫øt K·∫ø----
def artist_node(state):
    """
    ARTIST NODE V2 (REAL): V·∫Ω tranh th·∫≠t b·∫±ng DALL-E 3 HD.
    """
    print(colored("\n[üé® ARTIST] ƒêang kh·ªüi ƒë·ªông Studio DALL-E 3 HD...", "blue", attrs=["bold"]))
    
    messages = state.get("messages", [])
    # L·∫•y ƒëo·∫°n vƒÉn m√† CEO mu·ªën minh h·ªça
    last_msg_content = messages[-1].content
    
    # --- 1. TR√çCH XU·∫§T N·ªòI DUNG (H·ªó tr·ª£ c·∫£ 2 ki·ªÉu) ---
    # Ki·ªÉu 1: C√≥ d√πng d·∫•u """ (Chu·∫©n ch·ªâ)
    if '"""' in last_msg_content:
        start_idx = last_msg_content.find("\"\"\"") + 3
        end_idx = last_msg_content.rfind("\"\"\"")
        text_to_illustrate = last_msg_content[start_idx:end_idx].strip()
    # Ki·ªÉu 2: N√≥i t·ª± nhi√™n (VD: "V·∫Ω con m√®o") - S∆° cua
    else:
        # Lo·∫°i b·ªè c√°c tag h·ªá th·ªëng n·∫øu c√≥
        text_to_illustrate = last_msg_content.replace("[ARTIST]", "").strip()

    # Ki·ªÉm tra l·∫°i l·∫ßn cu·ªëi
    if not text_to_illustrate or len(text_to_illustrate) < 5:
        print(colored("üö´ [ARTIST] Kh√¥ng nh·∫≠n ƒë∆∞·ª£c n·ªôi dung ƒë·ªß ƒë·ªÉ v·∫Ω.", "red"))
        return {
            "messages": [AIMessage(content="üö´ H·ªça sƒ© c·∫ßn m√¥ t·∫£ chi ti·∫øt h∆°n ƒë·ªÉ v·∫Ω. Vui l√≤ng th·ª≠ l·∫°i.")], 
            "next_step": "FINISH" 
        }

    # --- 2. GPT-4: K·ª∏ S∆Ø PROMPT (Prompt Engineering) ---
    # Bi·∫øn y√™u c·∫ßu s∆° s√†i th√†nh Prompt ngh·ªá thu·∫≠t chi ti·∫øt
    analysis_prompt = (
        "B·∫°n l√† Gi√°m ƒë·ªëc Ngh·ªá thu·∫≠t (Art Director). Nhi·ªám v·ª•: T·∫°o Image Prompt cho DALL-E 3.\n"
        f"Y√äU C·∫¶U G·ªêC: \"{text_to_illustrate}\"\n\n"
        "H√ÉY TR·∫¢ V·ªÄ ƒê√öNG ƒê·ªäNH D·∫†NG JSON SAU (Kh√¥ng th√™m l·ªùi d·∫´n):\n"
        "```json\n"
        "{\n"
        "  \"style\": \"T√™n phong c√°ch ngh·ªá thu·∫≠t ph√π h·ª£p nh·∫•t (V√≠ d·ª•: Cyberpunk, Studio Ghibli, Photorealistic, Oil Painting...)\",\n"
        "  \"prompt\": \"M√¥ t·∫£ chi ti·∫øt h√¨nh ·∫£nh b·∫±ng ti·∫øng Anh, t·∫≠p trung v√†o √°nh s√°ng, b·ªë c·ª•c, chi ti·∫øt, c·∫£m x√∫c. T·ªëi ƒëa 70 t·ª´.\"\n"
        "}\n"
        "```"
    )

    try:
        # G·ªçi GPT-4 ƒë·ªÉ l·∫•y prompt x·ªãn
        analysis_response = LLM_GPT4.invoke([SystemMessage(content="JSON mode."), HumanMessage(content=analysis_prompt)])
        
        # L√†m s·∫°ch chu·ªói JSON (ƒë·ªÅ ph√≤ng GPT th√™m markdown)
        json_str = analysis_response.content.replace("```json", "").replace("```", "").strip()
        analysis_data = json.loads(json_str)
        
        design_style = analysis_data.get('style', 'Cinematic')
        visual_prompt = analysis_data.get('prompt', text_to_illustrate[:100])

        # T·∫°o prompt cu·ªëi c√πng
        full_image_prompt = f"{visual_prompt}, {design_style} style. High resolution, highly detailed, masterpiece."
        print(colored(f"--> Phong c√°ch: {design_style}", "cyan"))
        print(colored(f"--> Prompt v·∫Ω: {full_image_prompt[:100]}...", "white"))
            
        # --- 3. G·ªåI DALL-E 3 V·∫º TRANH TH·∫¨T (QUAN TR·ªåNG NH·∫§T) ---
        print(colored("‚è≥ ƒêang g·ª≠i y√™u c·∫ßu ƒë·∫øn m√°y ch·ªß OpenAI DALL-E 3 (Ch·ªù 15-30s)...", "yellow"))
        
        # Kh·ªüi t·∫°o c√¥ng c·ª• v·∫Ω HD
        dalle_tool = DallEAPIWrapper(
            model="dall-e-3",
            size="1024x1024",
            quality="hd" # Ch·∫•t l∆∞·ª£ng cao nh·∫•t
        )
        
        # Th·ª±c thi v·∫Ω (C√≥ th·ªÉ t·ªën 15-30 gi√¢y)
        image_url = dalle_tool.run(full_image_prompt)
        
        print(colored(f"‚úÖ [ART COMPLETE]: ·∫¢nh ƒë√£ s·∫µn s√†ng!", "green"))

        # --- 4. TR·∫¢ K·∫æT QU·∫¢ NHANH (FAST TRACK) ---
        # Tr·∫£ v·ªÅ FINISH ngay ƒë·ªÉ hi·ªán ·∫£nh, kh√¥ng qua Th∆∞ k√Ω n·ªØa.
        # S·ª≠ d·ª•ng Markdown chu·∫©n ƒë·ªÉ Dashboard hi·ªÉn th·ªã ·∫£nh.
        
        final_content = (
            f"üé® **T√ÅC PH·∫®M HO√ÄN THI·ªÜN:**\n\n"
            f"![AI Art Generation]({image_url})\n\n"
            f"*(Phong c√°ch: {design_style})*"
        )

        return {
            "messages": [AIMessage(content=final_content)],
            "next_step": "FINISH" # K·∫øt th√∫c ngay
        }

    # --- X·ª¨ L√ù L·ªñI ---
    except json.JSONDecodeError:
        print(colored("‚ùå L·ªói: GPT-4 kh√¥ng tr·∫£ v·ªÅ JSON h·ª£p l·ªá.", "red"))
        return {"messages": [AIMessage(content="‚ö†Ô∏è L·ªói ph√¢n t√≠ch y√™u c·∫ßu v·∫Ω tranh.")], "next_step": "FINISH"}
    except Exception as e:
        error_detail = str(e)
        print(colored(f"‚ùå L·ªñI V·∫º TRANH (DALL-E/API): {error_detail}", "red"))
        # Th√¥ng b√°o l·ªói r√µ r√†ng cho CEO (V√≠ d·ª•: H·∫øt ti·ªÅn, Vi ph·∫°m ch√≠nh s√°ch n·ªôi dung...)
        return {
            "messages": [AIMessage(content=f"‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫°o ·∫£nh l√∫c n√†y. Nguy√™n nh√¢n: {error_detail}")], 
            "next_step": "FINISH"
        }
# ============================================================================
# NODE: STORYTELLER (Nh√† vƒÉn & Bi√™n k·ªãch chuy√™n nghi·ªáp)

# ============================================================================
def storyteller_node(state):
    print(colored("[‚úçÔ∏è STORYTELLER] ƒêang x√¢y d·ª±ng th·∫ø gi·ªõi v√† c·ªët truy·ªán...", "cyan", attrs=["bold"]))
    
    messages = state.get("messages", [])
    # L·∫•y log l·ªói n·∫øu c√≥ ƒë·ªÉ ƒëi·ªÅu ch·ªânh vƒÉn phong
    errors = state.get("error_log", [])
    
    last_msg = messages[-1].content
    
    # 1. PH√ÇN T√çCH NHU C·∫¶U
    is_continue = "[CONTINUE]" in last_msg.upper()
    clean_query = last_msg.replace("[STORY]", "").replace("[CONTINUE]", "").strip()

    # 2. TR√ç NH·ªö M·∫†CH TRUY·ªÜN (Thay th·∫ø st.session_state)
    # Ch√∫ng ta l·∫•y b·ªëi c·∫£nh t·ª´ tin nh·∫Øn AIMessage g·∫ßn nh·∫•t trong l·ªãch s·ª≠ h·ªôi tho·∫°i c·ªßa Graph
    previous_full_story_content = ""
    if is_continue:
        for m in reversed(messages):
            if isinstance(m, AIMessage) and len(m.content) > 100:
                previous_full_story_content = m.content
                break
        
        if previous_full_story_content:
            # L·∫•y ƒëo·∫°n k·∫øt ƒë·ªÉ AI vi·∫øt n·ªëi ti·∫øp kh√¥ng b·ªã l·∫∑p
            context_tail = previous_full_story_content[-1000:]
            print(colored(f"üìú ƒê√£ t√¨m th·∫•y m·∫°ch truy·ªán c≈©, ƒëang n·ªëi ti·∫øp...", "yellow"))
            previous_full_story_content = context_tail

    # 3. THI·∫æT L·∫¨P PROMPT CHI·∫æN THU·∫¨T
    prompt = (
        "B·∫°n l√† Nh√† vƒÉn Best-seller v√† Bi√™n k·ªãch xu·∫•t s·∫Øc. "
        "\nNHI·ªÜM V·ª§: S√°ng t√°c n·ªôi dung c√≥ chi·ªÅu s√¢u, l√¥i cu·ªën."
        "\n\nNGUY√äN T·∫ÆC V√ÄNG:"
        + (f"\n- M·∫†CH TRUY·ªÜN TR∆Ø·ªöC: '{previous_full_story_content}' (H√£y vi·∫øt ti·∫øp t·ª´ ƒë√¢y, kh√¥ng ch√†o h·ªèi l·∫°i)." if previous_full_story_content else "\n- ƒê√ÇY L√Ä KH·ªûI ƒê·∫¶U: H√£y t·∫°o m·ªôt m·ªü ƒë·∫ßu ·∫•n t∆∞·ª£ng.") +
        "\n- C·∫§U TR√öC: Show, Don't Tell. S·ª≠ d·ª•ng nhi·ªÅu t·ª´ ng·ªØ g·ª£i h√¨nh, g·ª£i c·∫£m."
        "\n- H√åNH ·∫¢NH: Sau m·ªói ph√¢n ƒëo·∫°n cao tr√†o, h√£y ch√®n m·ªôt Visual Prompt ti·∫øng Anh trong ngo·∫∑c vu√¥ng [Visual: ...]."
    )

    try:
        # L·ª±a ch·ªçn Model: ∆Øu ti√™n Claude cho s√°ng t·∫°o vƒÉn h·ªçc
        selected_llm = LLM_CLAUDE if 'LLM_CLAUDE' in globals() else LLM_GPT4
        
        response = selected_llm.invoke([
            SystemMessage(content=prompt),
            HumanMessage(content=clean_query)
        ])

        # ƒê·ªäNH TUY·∫æN: Th∆∞·ªùng sau khi k·ªÉ chuy·ªán s·∫Ω k·∫øt th√∫c ƒë·ªÉ CEO ƒë·ªçc, ho·∫∑c qua Artist ƒë·ªÉ v·∫Ω
        return {
            "messages": [AIMessage(content=response.content)],
            "next_step": "Secretary" # ƒê∆∞a qua Th∆∞ k√Ω ƒë·ªÉ ch·ªët h·ªì s∆°
        }

    except Exception as e:
        error_msg = f"L·ªói Storyteller: {str(e)}"
        print(colored(f"‚ùå {error_msg}", "red"))
        return {
            "messages": [AIMessage(content=f"‚ö†Ô∏è S√°ng t√°c gi√°n ƒëo·∫°n: {error_msg}")],
            "error_log": errors + [error_msg],
            "next_step": "Secretary"
        }
def storytelling_node(state):
    print(colored("[üñãÔ∏è STORYTELLING] ƒê·∫°i vƒÉn h√†o ƒëang n·ªëi m·∫°ch c·∫£m x√∫c...", "magenta", attrs=["bold"]))
    
    messages = state.get("messages", [])
    last_msg = messages[-1].content
    
    # 1. PH√ÇN T√çCH NHU C·∫¶U: Vi·∫øt m·ªõi hay Vi·∫øt ti·∫øp?
    is_continue = "[CONTINUE]" in last_msg
    
    # 2. TR√ç NH·ªö D√ÄI H·∫†N: L·∫•y n·ªôi dung ch∆∞∆°ng tr∆∞·ªõc ƒë√≥ (n·∫øu l√† vi·∫øt ti·∫øp)
    previous_content = ""
    if is_continue and len(messages) > 1:
        # L·∫•y n·ªôi dung m√† AI v·ª´a tr·∫£ v·ªÅ ·ªü l∆∞·ª£t tr∆∞·ªõc
        previous_content = messages[-2].content 

    prompt = (
        "B·∫°n l√† Nh√† vƒÉn Best-seller. "
        "\nNHI·ªÜM V·ª§: Vi·∫øt ch∆∞∆°ng ti·∫øp theo c·ªßa c√¢u chuy·ªán."
        "\n\nY√äU C·∫¶U DUY TR√å M·∫†CH VƒÇN:"
        f"\n- ƒêO·∫†N K·∫æT CH∆Ø∆†NG TR∆Ø·ªöC: '{previous_content[-500:]}' (H√£y n·ªëi ti·∫øp m·∫°ch n√†y)."
        "\n- KH√îNG l·∫∑p l·∫°i l·ªùi ch√†o hay t√≥m t·∫Øt ch∆∞∆°ng c≈©."
        "\n- B·∫Øt ƒë·∫ßu ngay v√†o h√†nh ƒë·ªông ho·∫∑c l·ªùi tho·∫°i ti·∫øp theo."
        "\n- Gi·ªØ nguy√™n vƒÉn phong, t√™n nh√¢n v·∫≠t v√† b·ªëi c·∫£nh."
    )

    # 3. TH·ª∞C THI (D√πng Claude 3.5 Sonnet ƒë·ªÉ c√≥ s·ª± m∆∞·ª£t m√† nh·∫•t)
    response = LLM_CLAUDE.invoke([
        SystemMessage(content=prompt),
        HumanMessage(content=last_msg.replace("[CONTINUE]", ""))
    ])

    return {
        "messages": [AIMessage(content=response.content)],
        "next_step": "FINISH"
    }

# ============================================================================
# NODE: R&D STRATEGY (Gi√°m ƒë·ªëc Chi·∫øn l∆∞·ª£c - CSO)
# ============================================================================
def research_development_agent(state):
    """
    Agent R&D: K·∫øt h·ª£p t√¨m ki·∫øm th·ªùi gian th·ª±c v√† ph√¢n t√≠ch m√¥ h√¨nh PESTLE/Roadmap.
    """
    print(colored("[üß† R&D STRATEGY] ƒêang thi·∫øt l·∫≠p t·∫ßm nh√¨n chi·∫øn l∆∞·ª£c...", "blue", attrs=["bold"]))
    
    messages = state.get("messages", [])
    user_input = messages[-1].content
    
    # 1. Truy xu·∫•t k√Ω ·ª©c c√¥ng ty ƒë·ªÉ ƒë·∫£m b·∫£o chi·∫øn l∆∞·ª£c ƒë·ªìng nh·∫•t
    company_context = search_memory("T·∫ßm nh√¨n v√† m·ª•c ti√™u chi·∫øn l∆∞·ª£c AI Corporation")
    
    # 2. B∆∞·ªõc nghi√™n c·ª©u th·ª±c t·∫ø (S·ª≠ d·ª•ng Perplexity ƒë·ªÉ tr√°nh n√≥i s√°o r·ªóng)
    # Ch√∫ng ta y√™u c·∫ßu AI t√¨m d·ªØ li·ªáu th·ª±c t·∫ø tr∆∞·ªõc khi ph√¢n t√≠ch
    search_query = f"Xu h∆∞·ªõng c√¥ng ngh·ªá, ƒë·ªëi th·ªß c·∫°nh tranh v√† r·ªßi ro th·ªã tr∆∞·ªùng nƒÉm 2026 cho: {user_input}"
    
    try:
        # L·∫•y d·ªØ li·ªáu th·ª±c t·∫ø t·ª´ internet
        market_data = LLM_PERPLEXITY.invoke([
            SystemMessage(content="B·∫°n l√† chuy√™n gia ph√¢n t√≠ch d·ªØ li·ªáu th·ªã tr∆∞·ªùng."),
            HumanMessage(content=search_query)
        ]).content
        
        # 3. T·ªïng h·ª£p th√†nh b√°o c√°o chi·∫øn l∆∞·ª£c chuy√™n s√¢u
        # K·∫øt h·ª£p: D·ªØ li·ªáu th·ª±c t·∫ø + Prompt h·ªá th·ªëng + Ng·ªØ c·∫£nh c√¥ng ty
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", STRATEGY_SYSTEM_PROMPT),
            ("human", (
                f"Y√äU C·∫¶U NGHI√äN C·ª®U: {user_input}\n\n"
                f"D·ªÆ LI·ªÜU TH·ªä TR∆Ø·ªúNG TH·ª∞C T·∫æ: {market_data}\n\n"
                f"B·ªêI C·∫¢NH C√îNG TY: {company_context}\n\n"
                "H√£y l·∫≠p b√°o c√°o chi·∫øn l∆∞·ª£c chi ti·∫øt (PESTLE, Roadmap 2-5 nƒÉm)."
            ))
        ])
        
        # S·ª≠ d·ª•ng GPT-4o ƒë·ªÉ t·ªïng h·ª£p v√¨ kh·∫£ nƒÉng vi·∫øt b√°o c√°o r·∫•t t·ªët
        chain = prompt_template | LLM_GPT4
        response = chain.invoke({})
        
        return {
            "messages": [AIMessage(content=f"üß† [B√ÅO C√ÅO CHI·∫æN L∆Ø·ª¢C R&D]:\n{response.content}")],
            "next_step": "Supervisor"
        }
        
    except Exception as e:
        print(colored(f"L·ªói R&D Agent: {e}", "red"))
        return {"next_step": "Supervisor", "error_log": [str(e)]}

# ==========================================
# --- 4. THI·∫æT L·∫¨P LU·ªíNG AGENT (GRAPH) ---
# ==========================================

workflow = StateGraph(AgentState)

# --- 4.1 ƒêƒÉng k√Ω t·∫•t c·∫£ c√°c Node (ƒê·∫£m b·∫£o t√™n kh·ªõp 100%) ---
nodes_map = {
    "Router": router_node, 
    "Supervisor": supervisor_node, 
    "Coder": coder_node,
    "Tester": tester_node, 
    "Hardware": hardware_node, 
    "Engineering": engineering_node,
    "IoT_Engineer": iot_node, 
    "Procurement": procurement_node, 
    "Investment": investment_node,
    "Researcher": researcher_node, 
    "Strategy_R_and_D": research_development_agent,
    "Legal": legal_node, 
    "Marketing": marketing_node, 
    "Artist": artist_node,
    "Storyteller": storyteller_node, 
    "Orchestrator": dynamic_orchestrator,
    "Publisher": publisher_node, 
    "Secretary": secretary_node
}

for name, func in nodes_map.items():
    workflow.add_node(name, func)

# --- 4.2 Thi·∫øt l·∫≠p ƒëi·ªÉm v√†o ---
workflow.set_entry_point("Router")

# --- 4.3 Logic Router ---
# Thay v√¨ d√πng router_node tr·ª±c ti·∫øp, ta d√πng lambda ƒë·ªÉ l·∫•y chu·ªói 'next_step'
workflow.add_conditional_edges(
    "Router", 
    lambda x: x.get("next_step", "Supervisor"), 
    {
        "Researcher": "Researcher", 
        "Investment": "Investment", 
        "Storyteller": "Storyteller",
        "Artist": "Artist", 
        "Engineering": "Engineering", 
        "Publisher": "Publisher",
        "Orchestrator": "Orchestrator", 
        "Supervisor": "Supervisor", 
        "Secretary": "Secretary"
    }
)

# --- 4.4 Logic Supervisor ---
workflow.add_conditional_edges(
    "Supervisor", 
    lambda x: x.get("next_step", "Secretary"), 
    {
        "Coder": "Coder", 
        "Hardware": "Hardware", 
        "Engineering": "Engineering",
        "IoT_Engineer": "IoT_Engineer", 
        "Procurement": "Procurement",
        "Investment": "Investment", 
        "Researcher": "Researcher", 
        "Strategy_R_and_D": "Strategy_R_and_D",
        "Legal": "Legal", 
        "Marketing": "Marketing", 
        "Artist": "Artist",
        "Storyteller": "Storyteller", 
        "Secretary": "Secretary", 
        "FINISH": "Secretary"
    }
)

# --- 4.5 Nh√≥m Agent ph·ªï th√¥ng (H·ªìi quy v·ªÅ Supervisor ho·∫∑c k·∫øt th√∫c) ---
# L∆∞u √Ω: Kh√¥ng bao g·ªìm Coder, Tester, Hardware, Procurement, Investment, Researcher, Orchestrator
general_agents = [
    "Engineering", "IoT_Engineer", "Strategy_R_and_D", "Legal", 
    "Marketing", "Artist", "Storyteller", "Publisher"
]

for node in general_agents:
    workflow.add_conditional_edges(
        node,
        lambda x: x.get("next_step", "Supervisor") if x.get("next_step") != "FINISH" else "Secretary",
        {
            "Supervisor": "Supervisor", 
            "Secretary": "Secretary",
            "Artist": "Artist",
            "Procurement": "Procurement"
        }
    )

# --- 4.6 Logic chuy√™n bi·ªát (Pipeline & ƒê·∫∑c th√π) ---

# Lu·ªìng Researcher -> Orchestrator
workflow.add_conditional_edges(
    "Researcher",
    lambda x: "Orchestrator" if x.get("task_type") == "dynamic" else "Secretary",
    {"Orchestrator": "Orchestrator", "Secretary": "Secretary"}
)

# Lu·ªìng Orchestrator t·ªèa ƒëi c√°c nh√°nh
workflow.add_conditional_edges(
    "Orchestrator",
    lambda x: x.get("next_step", "Secretary") if x.get("next_step") != "FINISH" else "Secretary",
    {
        "Engineering": "Engineering", 
        "Hardware": "Hardware", 
        "Procurement": "Procurement",
        "IoT_Engineer": "IoT_Engineer", 
        "Supervisor": "Supervisor", 
        "Secretary": "Secretary"
    }
)

# Lu·ªìng K·ªπ thu·∫≠t: Coder -> Tester
workflow.add_edge("Coder", "Tester")
workflow.add_conditional_edges(
    "Tester", 
    lambda x: x.get("next_step", "Supervisor"), 
    {"Coder": "Coder", "Supervisor": "Supervisor"}
)

# Lu·ªìng V·∫≠t l√Ω & T√†i ch√≠nh c·ªë ƒë·ªãnh: Hardware -> Procurement -> Investment -> Supervisor/Secretary
workflow.add_edge("Hardware", "Procurement")
workflow.add_edge("Procurement", "Investment")
workflow.add_conditional_edges(
    "Investment",
    lambda x: "Secretary" if x.get("next_step") == "FINISH" else "Supervisor",
    {"Secretary": "Secretary", "Supervisor": "Supervisor"}
)

# --- 4.7 K·∫øt th√∫c h·ªá th·ªëng ---
workflow.add_edge("Secretary", END)

# --- 4.8 BI√äN D·ªäCH H·ªÜ TH·ªêNG ---
ai_app = workflow.compile() 
app = ai_app
db = None # Placeholder cho ƒë·ªëi t∆∞·ª£ng Database c·ªßa ng√†i

# ============================================================================
# 5. H√ÄM V·∫¨N H√ÄNH CH√çNH (ƒê·∫∂T ·ªû ƒê√ÇY)
# ============================================================================
async def run_ai_corporation(user_input, thread_id="1"):
    """
    ƒêi·ªÉm k√≠ch ho·∫°t h·ªá th·ªëng: Qu·∫£n l√Ω phi√™n l√†m vi·ªác v√† x·ª≠ l√Ω l·ªói t·∫ßng cao nh·∫•t.
    """
    config = {"configurable": {"thread_id": thread_id}, "recursion_limit": 50}
    
    # Kh·ªüi t·∫°o tr·∫°ng th√°i ban ƒë·∫ßu
    initial_state = {
                "messages": [HumanMessage(content=user_input)],
                "next_step": "Supervisor",
                "current_agent": "User", # Th√™m d√≤ng n√†y ƒë·ªÉ tr√°nh l·ªói NoneType
                "error_log": [],
                "task_type": "general"
            }

    print(colored(f"\nüöÄ PROJECT START: {user_input[:50]}...", "blue", attrs=["bold"]))

    try:
        # Ch·∫°y Graph (Gi·∫£ s·ª≠ b·∫°n ƒë√£ compile graph th√†nh app)
        async for event in app.astream(initial_state, config):
            for node, values in event.items():
                if node != "__metadata__":
                    print(colored(f"üìç Node [{node}] has completed.", "dark_grey"))
        
        print(colored("\n‚úÖ PROJECT FINISHED SUCCESSFULLY", "green", attrs=["bold"]))

    except Exception as e:
        # N·∫øu Graph s·∫≠p, k√≠ch ho·∫°t Fallback ngay l·∫≠p t·ª©c
        return ultimate_fallback(initial_state, [str(e)])
    
# ============================================================================
# 6. CH·∫†Y H·ªÜ TH·ªêNG (ASYNC ENGINE)
# ============================================================================

async def main_loop():
    print(colored("\n" + "="*50, "cyan"))
    print(colored("üöÄ AI CORPORATION - H·ªÜ TH·ªêNG ƒêI·ªÄU H√ÄNH T·ª∞ ƒê·ªòNG", "cyan", attrs=["bold"]))
    print(colored("Ch·∫ø ƒë·ªô: Parallel Coding & AST Testing [ON]", "green"))
    print(colored("="*50 + "\n", "cyan"))
    
    while True:
        try:
            user_input = input(colored("CEO (Y√™u c·∫ßu): ", "white", attrs=["bold"]))
            if user_input.lower() in ['q', 'exit']: 
                auto_backup_brain() # T·ª± ƒë·ªông sao l∆∞u tr∆∞·ªõc khi t·∫Øt m√°y
                break
            
            initial_state = {
                "messages": [HumanMessage(content=user_input)],
                "next_step": "Supervisor",
                "current_agent": "User", 
                "error_log": [],
                "task_type": "general"
            }
            
            # K√≠ch ho·∫°t Graph ch·∫°y (S·ª≠ d·ª•ng astream cho c√°c h√†m async)
            print(colored("\n--- ƒêANG X·ª¨ L√ù ---", "white", attrs=["bold"]))
            config = {"configurable": {"thread_id": "ceo_session"}, "recursion_limit": 150}
            async for event in app.astream(initial_state, config=config):
                for node, values in event.items():
                    if node != "__end__":
                        print(colored(f"  [‚ûî] {node} ƒë√£ ho√†n th√†nh nhi·ªám v·ª•.", "dark_grey"))
                        # N·∫øu mu·ªën in n·ªôi dung tin nh·∫Øn cu·ªëi c√πng c·ªßa t·ª´ng b∆∞·ªõc:
                        # print(values["messages"][-1].content)

            print(colored("\n‚úÖ ƒê√É HO√ÄN T·∫§T QUY TR√åNH.", "green", attrs=["bold"]))

        except Exception as e:
            print(colored(f"‚ùå L·ªñI H·ªÜ TH·ªêNG: {e}", "red"))


# ============================================================================
# 7. KH·ªûI CH·∫†Y TH·ª∞C T·∫æ
# ============================================================================
if __name__ == "__main__":
    try:
        # Ch·∫°y v√≤ng l·∫∑p ch√≠nh th√¥ng qua asyncio
        asyncio.run(main_loop())
    except KeyboardInterrupt:
        print("\nüëã ƒê√£ tho√°t h·ªá th·ªëng.")